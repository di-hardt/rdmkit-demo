{"0": {
      "doc": "About",
      "title": "Who is the RDMkit for?",
      "content": "The ELIXIR Research Data Management Kit (RDMkit) has been designed to guide life scientists in their efforts to better manage their research data following the FAIR Principles. It is based on the various steps of the data lifecycle, although not all the steps will be relevant to everyone. The contents are generated and maintained by the ELIXIR community coming together as part of the ELIXIR-CONVERGE project. RDMkit is recommended in the Horizon Europe Program Guide as the \"resource for Data Management guidelines and good practices for the Life Sciences.\" . ",
      "url": "/pages/bedroesb/rdmkit/about.html#who-is-the-rdmkit-for",
      "relUrl": "/about.html#who-is-the-rdmkit-for"
    },"1": {
      "doc": "About",
      "title": "Why are the FAIR principles needed?",
      "content": "At the heart of FAIR Science lies good data management practice. This is increasingly important as life science research becomes data-intensive and traditional ‘wet labs’ make space for ‘dry (computer) labs’. With that in mind, the FAIR Principles were designed to improve the infrastructure supporting the reuse of research data. The intention of these principles is to improve the Findability, Accessibility, Interoperability and Reuse of digital assets. The increasing volume, complexity and speed of data creation made scientists rely on computational support exponentially. Therefore, the FAIR Principles place specific emphasis on enhancing the ability of machines to automatically find and use data, as well as supporting its reuse by other scientists, which facilitates knowledge discovery and improves research transparency. ",
      "url": "/pages/bedroesb/rdmkit/about.html#why-are-the-fair-principles-needed",
      "relUrl": "/about.html#why-are-the-fair-principles-needed"
    },"2": {
      "doc": "About",
      "title": "What does the RDMkit aim to achieve?",
      "content": "More than ever, scientists need relevant tools and guidance to better manage their data and hence contribute to making FAIR Science a reality as well as help researchers be more productive for themselves and their collaborators. For researchers - RDMkit is a one stop shop of information, advice and signposting to research data management know-how, tools, examples and best practice. Struggling with writing your project data management plan? Then RDMkit can help. For data managers in laboratories, facilities or universities - RDMkit is a resource for your researchers, a complement to your own resources and a guide to the specific challenges of RDM for life science researchers. For funding agencies and policy makers - RDMkit is the resource researchers can turn to at the proposal stage of the research, throughout their projects and when publishing their results. Remember that the cost of not having FAIR research data has been estimated at €10.2bn annually in Europe. By using RDMkit and better managing data, the outcomes of your investments will be contributing to strengthening regional, national and European economies. ",
      "url": "/pages/bedroesb/rdmkit/about.html#what-does-the-rdmkit-aim-to-achieve",
      "relUrl": "/about.html#what-does-the-rdmkit-aim-to-achieve"
    },"3": {
      "doc": "About",
      "title": "Funding acknowledgement",
      "content": "RDMkit was developed with funding from the European Union’s Horizon 2020 Research and Innovation programme under grant agreement No 871075, as part of the ELIXIR-CONVERGE project. We are grateful to all who contributed their time to producing contents for RDMkit. ",
      "url": "/pages/bedroesb/rdmkit/about.html#funding-acknowledgement",
      "relUrl": "/about.html#funding-acknowledgement"
    },"4": {
      "doc": "About",
      "title": "How to cite the RDMkit",
      "content": "ELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075). URL: https://rdmkit.elixir-europe.org . The ELIXIR Research Data Management Kit makes all of their materials publicly available under Open Source Initiative licenses. The process documents and data are made available under a CC-BY license. Software are made available under an MIT license. For full details on licensing, please visit https://github.com/elixir-europe/rdmkit/blob/master/LICENSE . ",
      "url": "/pages/bedroesb/rdmkit/about.html#how-to-cite-the-rdmkit",
      "relUrl": "/about.html#how-to-cite-the-rdmkit"
    },"5": {
      "doc": "About",
      "title": "About",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/about.html",
      "relUrl": "/about.html"
    },"6": {
      "doc": "Accessibility",
      "title": "How to report problems",
      "content": "If you have any difficulties or annoyances using the site, or feel that we are not meeting accessibility requirements, then please contact us (rdm-editors@elixir-europe.org). We will do our best to make the site easier to use. ",
      "url": "/pages/bedroesb/rdmkit/accessibility.html#how-to-report-problems",
      "relUrl": "/accessibility.html#how-to-report-problems"
    },"7": {
      "doc": "Accessibility",
      "title": "How we assess the accessibility of the site",
      "content": "The website is a community effort, so unfortunately we do not have the resources to hire an accessibility expert. However, our community includes professional web developers, who assess the accessibility of the site using: . The W3C Web Accessibility Initiative checklist The British government accessibility checklist The Web Content Accessibility Guidelines AccessibilityTest.org, which runs several online accessibility checkers at once. The website was last tested on 17 May 2021. ",
      "url": "/pages/bedroesb/rdmkit/accessibility.html#how-we-assess-the-accessibility-of-the-site",
      "relUrl": "/accessibility.html#how-we-assess-the-accessibility-of-the-site"
    },"8": {
      "doc": "Accessibility",
      "title": "Level of compliance",
      "content": "As far as we can ascertain, this website is compliant with the Web Content Accessibility Guidelines version 2.1 AA standard. If you feel there are issues to address, however, then please email rdm-editors@elixir-europe.org. This statement was prepared on 30 March 2021. ",
      "url": "/pages/bedroesb/rdmkit/accessibility.html#level-of-compliance",
      "relUrl": "/accessibility.html#level-of-compliance"
    },"9": {
      "doc": "Accessibility",
      "title": "Accessibility",
      "content": "We appreciate people have different preferences and abilities when using websites. We have tried to make the site easy to use and understand, no matter what technology or browsing method you are using. For example, you should be able to: . | change colours, contrast levels and fonts | zoom in up to 300% without the text spilling off the screen | navigate the website using just a keyboard | listen to most of the website using a screen reader. | . We aim to conform to the to the ‘AA’ standard of the Web Content Accessibility Guidelines (WCAG) 2.1. This is not just a legal requirement in the European Union, but a best practice that will enable as many people as possible to use the site. ",
      "url": "/pages/bedroesb/rdmkit/accessibility.html",
      "relUrl": "/accessibility.html"
    },"10": {
      "doc": "All tools and resources",
      "title": "All tools and resources",
      "content": "This is the main tool and resource list of our website. This is a curated list which means that not all tools or resources that exist for a certain topic are listed here. This is mainly because we do not intend to be a registry. In most cases you will only find back the tools or resources that are mentioned in the different pages. Most pages will show a filtered list of this main table at the end of the page. We link to tools and resources to related information in ELIXIR registries: related policies and standards in FAIRsharing, scientific and technical descriptions of the resource in bio.tools, and related training in TeSS. If you see a missing link with one of the registries or a mistake, please open an issue or check our how to add a tool or resource guide. Skip tool table . | Tool or resource | Description | Related pages | Registry | . | Access to Biological Collection Data Schema (ABCD) | A standard schema for primary biodiversity data | Microbial biotechnology | bio.tools | . | Ada Discovery Analytics (Ada) | Ada is a performant and highly configurable system for secured integration, visualization, and collaborative analysis of heterogeneous data sets, primarily targeting clinical and experimental sources. | Data analysis TransMed | | . | Addgene | A searchable repository with a focus on plasmids | Microbial biotechnology | FAIRsharing | . | Amazon Web Services | Amazon Web Services | Data storage Data analysis Data transfer | TeSS | . | Amnesia | Amnesia is a GDPR compliant high accuracy data anonymization tool | Sensitive data | | . | AOP4EUpest | AOP4EUpest web server is devoted to the identification of pesticides involved in an Adverse Outcome Pathway via text mining approaches. | Toxicology data | bio.tools | . | APID Interactomes | APID (Agile Protein Interactomes DataServer) is a server that provides a comprehensive collection of protein interactomes for more than 400 organisms based in the integration of known experimentally validated protein-protein physical interactions (PPIs) | Intrinsically disordered proteins | bio.tools | . | Argos | Plan and follow your data. Bring your Data Management Plans closer to where data are generated, analysed and stored. | Data management plan Researcher Data steward research | | . | ArrayExpress | A repository of array based genomics data | Microbial biotechnology | bio.tools FAIRsharing TeSS | . | Arvados | With Arvados, bioinformaticians run and scale compute-intensive workflows, developers create biomedical applications, and IT administrators manage large compute and storage resources. | Data steward infrastructure Data steward policy Researcher Data analysis | | . | Aspera Fasp | With fast file transfer and streaming solutions built on the award-winning IBM FASP protocol, IBM Aspera software moves data of any size across any distance | Data transfer Data steward infrastructure | | . | ATCC | Biological materials resource including cell-lines, strains and genomics tools | Microbial biotechnology | bio.tools | . | Atlas | Free, publicly available web-based, open-source software application developed by the OHDSI community to support the design and execution of observational analyses to generate real world evidence from patient level observational data. | Data steward research Researcher TransMed | bio.tools FAIRsharing TeSS | . | BacDive | A searchable database for bacteria specific information | Microbial biotechnology | bio.tools | . | Bacillus Genetic Stock Center (BGSC) | A repository specific to Bacillus strains | Microbial biotechnology | | . | BASE | A search engine for academic web resources | Existing data | bio.tools TeSS | . | BBMRI-ERIC's ELSI Knowledge Base | The ELSI Knowledge Base is an open-access resource platform that aims at providing practical know-how for responsible research. | Data protection Sensitive data Data steward policy Data steward research Human data | | . | Beacon | The Beacon protocol defines an open standard for genomics data discovery. | Researcher Data steward research Data steward infrastructure Human data | bio.tools TeSS | . | Benchling | R&amp;D Platform for Life Sciences | Microbial biotechnology | | . | BIAFLOWS | BIAFLOWS is an open-soure web framework to reproducibly deploy and benchmark bioimage analysis workflows | Data analysis | bio.tools | . | BigNASim | Repository for Nucleic Acids MD simulations | Biomolecular simulation data | bio.tools | . | BIII | The BioImage Informatics Index is a registry of software tools, image databases for benchmarking, and training materials for bioimage analysis | Data steward infrastructure Data analysis | bio.tools | . | BindingDB | Public, web-accessible database of measured binding affinities | Biomolecular simulation data | bio.tools FAIRsharing | . | Bio-Formats | Bio-Formats is a software tool for reading and writing image data using standardized, open formats | OMERO | bio.tools TeSS | . | Bioactive Conformational Ensemble | Platform designed to efficiently generate bioactive conformers and speed up the drug discovery process. | Biomolecular simulation data | bio.tools | . | Bioconda | Bioconda is a bioinformatics channel for the Conda package manager | Data steward infrastructure Data analysis | bio.tools TeSS | . | Biodiversity Information Standards (TDWG) | Biodiversity Information Standards (TDWG), historically the Taxonomic Databases Working Group, work to develop biodiversity information standards | Microbial biotechnology | | . | BioExcel COVID-19 | Platform designed to provide web-access to atomistic-MD trajectories for macromolecules involved in the COVID-19 disease. | Biomolecular simulation data | | . | BioImageArchive | The BioImage Archive stores and distributes biological images that are useful to life-science researchers. | Data publication | FAIRsharing | . | BioModels | A repository of mathematical models for application in biological sciences | Microbial biotechnology | bio.tools FAIRsharing TeSS | . | BIONDA | BIONDA is a free and open-access biomarker database, which employs various text mining methods to extract structured information on biomarkers from abstracts of scientific publications | Data storage Researcher Human data Proteomics | bio.tools | . | BioSamples | BioSamples stores and supplies descriptions and metadata about biological samples used in research and development by academia and industry. | Documentation and metadata Plant sciences Plant Genomics | bio.tools FAIRsharing TeSS | . | BioStudies | A database hosting datasets from biological studies. Useful for storing or accessing data that is not compliant for mainstream repositories. | Microbial biotechnology Documentation and metadata Plant sciences | bio.tools FAIRsharing TeSS | . | Bitbucket | Git based code hosting and collaboration tool, built for teams. | Data organisation Data steward research Data steward infrastructure | | . | BMRB | Biological Magnetic Resonance Data Bank | Intrinsically disordered proteins Researcher | bio.tools | . | BoostDM | BoostDM is a method to score all possible point mutations (single base substitutions) in cancer genes for their potential to be involved in tumorigenesis. | Data analysis Human data | bio.tools | . | Box | Cloud storage and file sharing service | Data storage Data steward infrastructure Data transfer | TeSS | . | BrAPI | Specification for a standard API for plant data: plant material, plant phenotyping data | Data steward infrastructure Plant sciences | TeSS | . | BRENDA | Database of enzyme and enzyme-ligand information, across all taxonomic groups, manually extracted from primary literature and extended by text mining procedures | Microbial biotechnology | bio.tools FAIRsharing TeSS | . | Bulk Rename Utility | File renaming software for Windows | Data organisation Data steward research Researcher | | . | CalibraCurve | A highly useful and flexible tool for calibration of targeted MS?based measurements. CalibraCurve enables an automated batch-mode determination of dynamic linear ranges and quantification limits for both targeted proteomics and similar assays. The software uses a variety of measures to assess the accuracy of the calibration and provides intuitive visualizations. | Data analysis Proteomics | bio.tools | . | Cancer Genome Interpreter | Cancer Genome Interpreter (CGI) is designed to support the identification of tumor alterations that drive the disease and detect those that may be therapeutically actionable. | Data analysis Human data | bio.tools | . | CAS Registry | The CAS Registry (Chemical Abstracts Service Registry) includes more than 188 million unique chemicals. CAS Registry Numbers are broadly used as a unique identifier for chemical substances. The Registry is maintained by CAS, a subdivision of the American Chemical Society. | Toxicology data | FAIRsharing | . | Castor | Castor is an EDC system for researchers and institutions. With Castor, you can create and customize your own database in no time. Without any prior technical knowledge, you can build a study in just a few clicks using our intuitive Form Builder. Simply define your data points and start collecting high quality data, all you need is a web browser. | Identifiers Data steward infrastructure Data steward research | bio.tools | . | CATH | A hierarchical domain classification of protein structures in the Protein Data Bank. | | bio.tools FAIRsharing TeSS | . | CellRepo | A version management tool for modifying strains | Microbial biotechnology | | . | Cellular Microscopy Phenotype Ontology (CMPO) | An ontology for expressing cellular (or multi-cellular) terms with applications in microscopy | Microbial biotechnology | TeSS | . | CERNBox | CERNBox cloud data storage, sharing and synchronization | Data storage | | . | ChEBI | Dictionary of molecular entities focused on 'small' chemical compounds | Microbial biotechnology | bio.tools FAIRsharing TeSS | . | ChEMBL | Database of bioactive drug-like small molecules, it contains 2-D structures, calculated properties and abstracted bioactivities. | Data analysis Researcher Toxicology data | bio.tools FAIRsharing TeSS | . | ChIPSummitDB | ChIPSummitDB is a database of transcription factor binding sites and the distances of the binding sites relative to the peak summits. | Human data | bio.tools | . | Choose a license | Choose an open source license | Licensing Researcher Data steward research Data steward policy | TeSS | . | ClinicalTrials.gov | ClinicalTrials.gov is a resource depending on the National Library of medicine which makes available private and public-funded clinical trials. | Toxicology data | FAIRsharing | . | Common Workflow Language (CWL) | An open standard for describing workflows that are build from command line tools | Data steward infrastructure Researcher Data analysis | FAIRsharing TeSS | . | Comptox | The CompTox Chemicals Dashboard provides toxicological information for over 800.000 chemical compounds. It is a part of a suite of databases and web applications developed by the US Environmental Protection Agency's Chemical Safety for Sustainability Research Program. These databases and apps support EPA's computational toxicology research efforts to develop innovative methods to change how chemicals are currently evaluated for potential health risks. | Toxicology data | bio.tools FAIRsharing | . | COmputational Modeling in BIology NEtwork (COMBINE) | An initiative to bring together various formats and standard for computational models in biology | Microbial biotechnology | | . | Conda | Open source package management system | Data steward infrastructure Data analysis | TeSS | . | Consent Clauses for Genomic Research | A resource for researchers when drafting consent forms so they can use language matching cutting-edge GA4GH international standards | Human data | | . | COPO | Portal for scientists to broker more easily rich metadata alongside data to public repos. | Documentation and metadata Researcher Plant sciences | bio.tools FAIRsharing | . | COVID-19 Molecular Structure and Therapeutics Hub | COVID-19 Molecular Structure and Therapeutics Hub | Biomolecular simulation data | | . | Create a Codebook | Examples and tools to create a codebook by the Data Documentation Initiative (DDI) | Documentation and metadata Researcher Data steward research | | . | Creative Commons License Chooser | It helps you choose the right Creative Commons license for your needs. | Licensing Researcher Data steward research Data steward policy | | . | Crop Ontology | The Crop Ontology compiles concepts to curate phenotyping assays on crop plants, including anatomy, structure and phenotype. | Researcher Data steward research Data steward infrastructure Plant sciences | FAIRsharing TeSS | . | CS3 | Cloud Storage Services for Synchronization and Sharing (CS3) | Data storage | | . | CTD | A database that aims to advance understanding about how environmental exposures affect human health. | Toxicology data | bio.tools | . | cURL | command line tool and library for transferring data with URLs | Data transfer Data steward infrastructure | | . | DAISY | Data Information System to keep sensitive data inventory and meet GDPR accountability requirement. | Data steward infrastructure Data steward policy Human data Data protection TransMed | bio.tools | . | Data Catalog | Unique collection of project-level metadata from large research initiatives in a diverse range of fields, including clinical, molecular and observational studies. Its aim is to improve the findability of these projects following FAIR data principles. | Documentation and metadata TransMed | TeSS | . | Data Curation Centre Metadata list | List of metadata standards | Documentation and metadata Researcher Data steward research | | . | Data INRAE | Dataverse for life sciences and agronomic related data | Plant sciences Plant Genomics Researcher Data steward research | FAIRsharing | . | Data Stewardship Wizard | Publicly available online tool for composing smart data management plans | Data management plan Researcher Data steward research Data steward infrastructure NeLS TSD | bio.tools TeSS | . | Data Use Ontology | DUO allows to semantically tag datasets with restriction about their usage. | Data steward research Researcher Human data | FAIRsharing TeSS | . | data.world Data License list | Overview of typical licenses used for data resources | Licensing Biomolecular simulation data | | . | DataCite | A search engine for the complete collection of publicly available DataCite DOIs | Existing data | FAIRsharing | . | DATAVERSE | Open source research data respository software. | Data storage Researcher Data steward research Data steward infrastructure IFB | TeSS | . | DAWID | The Data Agreement Wizard is a tool developed by ELIXIR-Luxembourg to facilitate data sharing agreements. | Data protection Data steward policy Human data | | . | dbGAP | The database of Genotypes and Phenotypes (dbGaP) archives and distributes data from studies investigating the interaction of genotype and phenotype in Humans | Data publication Researcher Data steward infrastructure Human data | bio.tools FAIRsharing | . | DisGeNET | A discovery platform containing collections of genes and variants associated to human diseases. | Data analysis Human data Researcher Toxicology data | bio.tools FAIRsharing | . | DisProt | A database of intrinsically disordered proteins | Intrinsically disordered proteins Researcher | bio.tools | . | DMP Canvas Generator | Questionnaire, which generates a pre-filled a DMP | Data management plan Researcher Data steward research | | . | DMPlanner | Semi-automatically generated, searchable catalogue of resources that are relevant to data management plans. | Data management plan Researcher Data steward research | | . | DMPonline | A free tool to write, share and export a data management plan. Built-in data management plan templates for many major funders. | Data management plan Researcher Data steward research | TeSS | . | DMPTool | Build your Data Management Plan | Data management plan Researcher Data steward research | | . | DNA Data Bank of Japan (DDBJ) | A database of DNA sequences | Microbial biotechnology | bio.tools | . | Docker | Docker is a software for the execution of applications in virtualized environments called containers. It is linked to DockerHub, a library for sharing container images | Data steward infrastructure Data analysis | FAIRsharing TeSS | . | DropBox | Cloud storage and file sharing service | Data storage Data steward infrastructure Data transfer | | . | Drug Matrix | A toxicogenomic resource that provides access to the gene expression profiles of over 600 different compounds in several cell types from rats and primary rat hepatocytes. | Toxicology data | | . | Dryad | Open-source, community-led data curation, publishing, and preservation platform for CC0 publicly available research data | Data publication Biomolecular simulation data | FAIRsharing | . | Dynameomics | Database of folding / unfolding pathway of representatives from all known protein folds by MD simulation | Biomolecular simulation data | | . | e!DAL | Electronic data archive library is a framework for publishing and sharing research data | Data storage Data steward infrastructure | bio.tools | . | e!DAL-PGP | Plant Genomics and Phenomics Research Data Repository | Plant sciences Plant Genomics Researcher Data steward research Data steward infrastructure | FAIRsharing | . | EasyDMP | DMP creation, versioning and sharing | Data management plan Researcher Data steward research | | . | ECOTOX | The ECOTOXicology Knowledgebase (ECOTOX) is a comprehensive, publicly available Knowledgebase providing single chemical environmental toxicity data on aquatic life, terrestrial plants, and wildlife. | Toxicology data | | . | ECPGR | Hub for the identification of plant genetic resources in Europe | Plant sciences Researcher Data steward research | | . | EDKB | Endocrine Disruptor Knowledge Base is a platform designed to foster the development of computational predictive toxicology. This platform allows direct access to ten libraries containing the following resources: a biological activity database, QSAR training sets, in vitro and in vivo experimental data for more than 3,000 chemicals, literature citations, chemical-structure search capabilities. | Toxicology data | | . | ELIXIR Core Data Resources | Set of European data resources of fundamental importance to the wider life-science community and the long-term preservation of biological data | Existing data COVID-19 Data Portal | FAIRsharing TeSS | . | ELIXIR Deposition Databases for Biomolecular Data | List of discipline-specific deposition databases recommended by ELIXIR. | Data publication Researcher Data steward research Data steward infrastructure COVID-19 Data Portal NeLS IFB CSC | FAIRsharing | . | ELIXIR-AAI | The ELIXIR Authentication and Authorisation Infrastructure (AAI) | Sensitive data NeLS TSD TransMed | TeSS | . | EMBL-EBI Ontology Lookup Service | EMBL-EBI’s web portal for finding ontologies | Documentation and metadata Data steward research Researcher | TeSS | . | EMBL-EBI's data submission wizard | EMBL-EBI's wizard for finding the right EMBL-EBI repository for your data. | Data publication Researcher Data steward research | | . | EMPIAR | Electron Microscopy Public Image Archive is a public resource for raw, 2D electron microscopy images. You can browse, upload and download the raw images used to build a 3D structure | Data publication OMERO | | . | Ensembl | Genome browser for vertebrate genomes that supports research in comparative genomics, evolution, sequence variation and transcriptional regulation. | | bio.tools FAIRsharing TeSS | . | Ensembl Genomes | Comparative analysis, data mining and visualisation for the genomes of non-vertebrate species | | bio.tools FAIRsharing TeSS | . | Ensembl Plants | Open-access database of full genomes of plant species. | Plant Genomics | FAIRsharing TeSS | . | EU General Data Protection Regulation | Regulation (eu) 2016/679 of the european parliament and of the council on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation). | Data protection Data steward policy Human data TSD | | . | EUDAT licence selector wizard | EUDAT's wizard for finding the right licence for your data or code. | Licensing Researcher Data steward research Data steward policy | | . | EudraVigilance | The European database of suspected adverse drug reaction reports is a public resource aimed to provide access to reported suspected side-effects of drugs. Side-effects are defined according to the MedDRA ontology. | Toxicology data | | . | EUPID | EUPID provides a method for identity management, pseudonymisation and record linkage to bridge the gap between multiple contexts. | Data steward infrastructure Data steward policy Human data | | . | EURISCO | European Search Catalogue for Plant Genetic Resources | Plant sciences Researcher Data steward research | bio.tools | . | Europe PMC | Europe PMC is a repository, providing access to worldwide life sciences articles, books, patents and clinical guidelines. | Researcher | bio.tools FAIRsharing TeSS | . | European Nucleotide Archive (ENA) | A record of sequence information scaling from raw sequcning reads to assemblies and functional annotation | Microbial biotechnology Plant Genomics | bio.tools FAIRsharing TeSS | . | European Variation Archive (EVA) | Open-access database of all types of genetic variation data from all species. | Plant Genomics | bio.tools FAIRsharing | . | Evidence and Conclusion Ontology (ECO) | Controlled vocabulary that describes types of evidence and assertion methods | Existing data Documentation and metadata | FAIRsharing | . | FAERS | The FDA Adverse Event Reporting System (FAERS) is an american resource that contains adverse event reports, medication error reports and product quality complaints submitted by healthcare professionals, consumers, and manufacturers. MedDRA ontology is used for coding adverse effects. Note that reports available in FAERS do not require a causal relationship between a product and an adverse event and further evaluations are conducted by FDA to monitor the safety of products. | Toxicology data | bio.tools | . | FAIDARE | FAIDARE is a tool allowing to search data across dinstinct databases that implemented BrAPI. | Researcher Data steward research Plant sciences IFB | bio.tools | . | FAIR Cookbook | FAIR Cookbook is an online resource for the Life Sciences with recipes that help you to make and keep data Findable, Accessible, Interoperable and Reusable (FAIR) | Compliance monitoring &amp; measurement Data steward research TransMed | | . | FAIR Evaluation Services | Resources and guidelines to assess the FAIRness of digital resources. | Compliance monitoring &amp; measurement Data steward research Data steward policy | | . | FAIRassist.org | Help you discover resources to measure and improve FAIRness. | Compliance monitoring &amp; measurement Data steward research Data steward policy | | . | FAIRDOM-SEEK | Data, model and SOPs management for projects, from preliminary data to publication, support for running SBML models etc. | Data storage Data steward infrastructure NeLS Microbial biotechnology IFB | bio.tools TeSS | . | FAIRDOMHub | Data, model and SOPs management for projects, from preliminary data to publication, support for running SBML models etc. (public SEEK instance) | Data storage Researcher NeLS Documentation and metadata Microbial biotechnology | FAIRsharing | . | fairsharing | A curated, informative and educational resource on data and metadata standards, inter-related to databases and data policies. | Documentation and metadata Data publication Data steward policy Data steward research Researcher Microbial biotechnology Existing data | FAIRsharing TeSS | . | FigShare | Data publishing platform | Data publication Biomolecular simulation data | FAIRsharing TeSS | . | FileZilla | A free FTP solution | Data transfer Data steward infrastructure | | . | Free-IPA | FreeIPA is an integrated Identity and Authentication solution for Linux/UNIX networked environments. | Data steward infrastructure TransMed | | . | Freegenes | Repository of IP-free synthetic biological parts | Microbial biotechnology | | . | GA4GH data security toolkit | Principled and practical framework for the responsible sharing of genomic and health-related data. | Data publication Data steward policy Data steward research Data steward infrastructure Human data | | . | GA4GH regular and ethical toolkit | Framework for Responsible Sharing of Genomic and Health-Related Data | Data protection Sensitive data Data steward policy Data steward research Data steward infrastructure Human data | | . | Galaxy | Open, web-based platform for data intensive biomedical research. Whether on the free public server or your own instance, you can perform, reproduce, and share complete analyses. Different instances available | NeLS Marine Metagenomics Data analysis Researcher Data steward infrastructure IFB | bio.tools TeSS | . | GenBank | A database of genetic sequence information. GenBank may also refer to the data format used for storing information around genetic sequence data. | Microbial biotechnology | bio.tools FAIRsharing TeSS | . | Gene Expression Omnibus (GEO) | A repository of MIAME-compliant genomics data from arrays and high-throughput sequencing | Microbial biotechnology | | . | GENEID | Geneid is an ab initio gene finding program used to predict genes along DNA sequences in a large set of organisms. | Data analysis Researcher | bio.tools | . | GEO | A public repository that stores microarray, next-generation sequencing, and other forms of high-throughput functional genomics data submitted by the research community. | Toxicology data | TeSS | . | GHS Classification | GHS (Globally Harmonized System of Classification and Labelling of Chemicals) classification was developed by the United Nations in an attempt to align standards and chemical regulations in different countries. GHS includes criteria for the classification of health, physical and environmental hazards, and what information should be included on labels of hazardous chemicals and safety data sheets. | Toxicology data | | . | Git | Distributed version control system designed to handle everything from small to very large projects | Data organisation Data steward research Data steward infrastructure | TeSS | . | GitHub | Versioning system, used for sharing code, as well as for sharing of small data | Data publication Data organisation Data steward infrastructure Data steward research | FAIRsharing TeSS | . | GitLab | GitLab is an open source end-to-end software development platform with built-in version control, issue tracking, code review, CI/CD, and more. Self-host GitLab on your own servers, in a container, or on a cloud provider. | Data organisation Data publication Data steward infrastructure Data steward research | TeSS | . | Globus | Globus lets you efficiently, securely, and reliably transfer data directly between systems separated by an office wall or an ocean. Focus on your research and offload your data transfer headaches to Globus | Data transfer Data steward infrastructure | | . | GnpIS | A multispecies integrative information system dedicated to plant and fungi pests. It allows researchers to access genetic, phenotypic and genomic data. It is used by both large international projects and the French National Research Institute for Agriculture, Food and Environment. | Plant sciences | bio.tools FAIRsharing | . | Google Dataset Search | Search engine for datasets | Existing data | TeSS | . | Google Drive | Cloud Storage for Work and Home | Data storage Data transfer | | . | GPCRmd | Repository of GPCR protein simulations | Biomolecular simulation data | bio.tools | . | GRAPE 2.0 | The GRAPE pipeline provides an extensive pipeline for RNA-Seq analyses. It allows the creation of an automated and integrated workflow to manage, analyse and visualize RNA-Seq data. | Data analysis | bio.tools | . | Harvard Medical School – ELN Comparison Grid | ELN Comparison Grid by Hardvard Medical School | Documentation and metadata Identifiers Researcher Data steward research | | . | Haz-Map | Haz-Map is an occupational health database that makes available information about the adverse effects of exposures to chemical and biological agents at the workplace. These associations have been established using current scientific evidence. | Toxicology data | | . | How to License Research Data - DCC | Guidelines about how to license research data from Digital Curation Centre | Licensing Researcher Data steward research Data steward policy | | . | Human Protein Atlas | The Human Protein Atlas contains information for a large majority of all human protein-coding genes regarding the expression and localization of the corresponding proteins based on both RNA and protein data. | Proteomics | FAIRsharing TeSS | . | HumanMine | HumanMine integrates many types of human data and provides a powerful query engine, export for results, analysis for lists of data and FAIR access via web services. | Data organisation Data steward research Researcher Human data Data analysis | bio.tools FAIRsharing TeSS | . | iCloud | Data sharing | Data storage Data analysis Data transfer | | . | Identifiers.org | The Identifiers.org Resolution Service provides consistent access to life science data using Compact Identifiers. Compact Identifiers consist of an assigned unique prefix and a local provider designated accession number (prefix:accession). | Identifiers Data steward infrastructure Data steward research | bio.tools FAIRsharing | . | Intrinsically disordered proteins ontology (IDPO) | Intrinsically disordered proteins ontology | Intrinsically disordered proteins Documentation and metadata | | . | iGEM Parts Registry | A collection of standard biological parts to which all entrants in the iGEM competition must submit their parts | Microbial biotechnology | | . | Image Data Resource (IDR) | A repository of image datasets from scientific publications | Microbial biotechnology Data publication Documentation and metadata Data transfer OMERO | bio.tools FAIRsharing | . | Informed Consent Ontology | The Informed Consent Ontology (ICO) is an ontology for the informed consent and informed consent process in the medical field. | Data steward infrastructure Data steward policy Human data | FAIRsharing | . | International Compilation of Human Research Standards | The International Compilation of Human Research Standards enumerates over 1,000 laws, regulations, and guidelines (collectively referred to as standards) that govern human subject protections in 133 countries, as well as standards from a number of international and regional organizations | Human data | | . | International Nucleotide Sequence Database Collaboration (INSDC) | A collaborative database of genetic sequence datasets from DDBJ, EMBL-EBI and NCBI | Microbial biotechnology | bio.tools | . | International Society for the Advancement of Cytometry (ISAC) | Data standards and formats for reporting flow cytometry data | Microbial biotechnology | | . | International Union of Biochemistry and Molecular Biology (IUBMB) | Resource for naming standards in biochemistry and molecular biology | Microbial biotechnology | | . | InterPro | Functional analysis of protein sequences by classifying them into families and predicting the presence of domains and important sites | | bio.tools FAIRsharing TeSS | . | IntoGen | IntoGen collects and analyses somatic mutations in thousands of tumor genomes to identify cancer driver genes. | Data analysis Human data | bio.tools | . | IRIS | The Integrated Risk Information System (IRIS) resource evaluates information on health that might arise after exposure to environmental contaminants. | Toxicology data | bio.tools TeSS | . | iRODS | Integrated Rule-Oriented Data System (iRODS) is open source data management software for a cancer genome analysis workflow. | Data storage Data steward infrastructure TransMed | bio.tools | . | ISA-tools | Open source framework and tools helping to manage a diverse set of life science, environmental and biomedical experiments using the Investigation Study Assay (ISA) standard | Data steward infrastructure Data steward research Microbial biotechnology | FAIRsharing | . | ISA4J | Open source software library that can be used to generate a ISA-TAB export from in-house data sets. These comprises e.g. local database or local file system based experimental. | Plant sciences | bio.tools | . | ISO/IEC 27001 | International information security standard | Data protection Data steward policy Human data | | . | ITER | The International Toxicity Estimates for Risk (ITER) is a free online resource of human health risk values and cancer classifications for over 680 chemicals of environmental concern. ITER shows tabular risk data from several sources in a tabular format for easy comparison, a synopsis explaining discrepancies in data, and a link to each organization for further details. | Toxicology data | TeSS | . | IUPAC-IUBMB Joint Commission on Biochemical Nomenclature (JCBN) | A collaborative resource from IUPAC and IUBMB for naming standards in biochemistry | Microbial biotechnology | | . | JBEI-ICE | A registry platform for biological parts | Microbial biotechnology | | . | Jupyter | Jupyter notebooks allow to share code, documentation | Data steward infrastructure Data analysis | TeSS | . | Keycloak | Keycloak is an open source identity and data access management solution. | Data steward infrastructure TransMed | TeSS | . | LimTox | The LiMTox system is a text mining approach that tries to extract associations between compounds and a particular toxicological endpoint at various levels of granularity and evidence types, all inspired by the content of toxicology reports. It integrates direct ranking of associations between compounds and hepatotoxicity through combination of heterogeneous complementary strategies from term co-mention, rules, and patterns to machine learning-based text classification. It also provides indirect associations to hepatotoxicity through the extraction of relations reflecting the effect of compounds at the level of metabolism and liver enzymes. | Toxicology data | bio.tools | . | Linked Open Vocabularies (LOV) | Web portal for finding ontologies | Documentation and metadata Data steward research Researcher | | . | List of Prokaryotic names with Standing in Nomenclature (LPSN) | A database of prokaryote specific biodiversity information | Microbial biotechnology | | . | LUMI | EuroHPC world-class supercomputer | Data analysis Researcher Data steward infrastructure CSC | bio.tools | . | maDMP - Research Bridge | Machine-Actionable Data Management Plan | Webinar (2016) on making a good data management plan. | Data management plan Data steward infrastructure | | . | MarDB | MarDB includes all non-complete marine microbial genomes regardless of level of completeness. Each entry contains 120 metadata fields including information about sampling environment or host, organism and taxonomy, phenotype, pathogenicity, assembly and annotation. | Marine Metagenomics | bio.tools | . | MarFun | MarFun is a manually curated marine fungi genome database. | Marine Metagenomics | | . | Marine metagenomics portal | High-quality curated and freely accessible microbial genomics and metagenomics resources for the marine scientific community | Marine Metagenomics | bio.tools | . | MarRef | MarRef is a manually curated marine microbial reference genome database that equenced genomes. Each entry contains 120 metadata fields including information about sampling environment or host, organism and taxonomy, phenotype, pathogenicity, assembly and annotation information | Marine Metagenomics | bio.tools | . | Multi-Crop Passport Descriptor (MCPD) | The Multi-Crop Passport Descriptor is the metadata standard for plant genetic resources maintained ex situ by genbanks. | Documentation and metadata Researcher Data steward infrastructure Data steward policy Plant sciences Plant sciences | FAIRsharing | . | MemProtMD | Database of over 5000 intrinsic membrane protein structures | Biomolecular simulation data | | . | Mendeley data | Multidisciplinary, free-to-use open repository specialized for research data | Data publication Biomolecular simulation data | FAIRsharing | . | MetabolomeXchange | A repository of genomics data relating to the study of the metabolome | Microbial biotechnology | bio.tools | . | MIADE | Minimum Information About Disorder Experiments (MIADE) standard | Documentation and metadata Researcher Data steward research Intrinsically disordered proteins | | . | MIAPPE | Minimum Information About a Plant Phenotyping Experiment | Documentation and metadata Researcher Data steward research Plant sciences Plant Genomics | FAIRsharing TeSS | . | Microsoft Azure | Cloud storage and file sharing service from Microsoft | Data storage Data steward infrastructure Data transfer | | . | Microsoft OneDrive | Cloud storage and file sharing service from Microsoft | Data storage Data steward infrastructure | | . | MIGS/MIMS | Minimum Information about a (Meta)Genome Sequence | Documentation and metadata Researcher Data steward research Marine metagenomics Microbial biotechnology | FAIRsharing | . | MINT | MINT, the Molecular INTeraction database, focuses on experimentally verified protein-protein interactions mined from the scientific literature by expert curators | | bio.tools FAIRsharing | . | MIxS | Minimum Information about any (x) Sequence | Documentation and metadata Researcher Data steward research Marine metagenomics Plant Genomics | FAIRsharing TeSS | . | MobiDB | A database of protein disorder and mobility annotations | Intrinsically disordered proteins Researcher | bio.tools FAIRsharing | . | MoDEL | Database of Protein Molecular Dynamics simulations representing different structural clusters of the PDB | Biomolecular simulation data | bio.tools FAIRsharing TeSS | . | MoDEL Covid19 | Database of COVID-19 related atomistic Molecular Dynamic Trajectories | Biomolecular simulation data | | . | MoDEL-CNS | Repository for Central Nervous System-related mainly membrane protein MD simulations | Biomolecular simulation data | | . | ModelArchive | Repository for molecular models - providing doi of models | Biomolecular simulation data | FAIRsharing | . | Molgenis | Molgenis is a modular web application for scientific data. Molgenis provides researchers with user friendly and scalable software infrastructures to capture, exchange, and exploit the large amounts of data that is being produced by scientific organisations all around the world. | Identifiers Data steward infrastructure Data steward research | bio.tools | . | MolMeDB | Database about interactions of molecules with membranes | Biomolecular simulation data | bio.tools FAIRsharing | . | MONARC | A risk assessment tool that can be used to do Data Protection Impact Assessments | Data protection Data steward policy Human data TransMed | FAIRsharing | . | MRI2DICOM | a Magnetic Resonance Imaging (MRI) converter from ParaVision® (Bruker, Inc. Billerica, MA) file format to DICOM standard | Researcher Data steward research XNAT-PIC | | . | National Center for Biotechnology Information (NCBI) | Online database hosting a vast amount of biotechnological information including nucleic acids, proteins, genomes and publications. Also boasts integrated tools for analysis. | Microbial biotechnology | | . | NBP | The National Biomonitoring Program (NBP) is a public resource that offers an assessment of nutritional status and the exposure of the U.S. population to environmental chemicals and toxic substances. | Toxicology data | | . | NCBI Taxonomy | NCBI's taxonomy browser is a database of biodiversity information | Microbial biotechnology | FAIRsharing | . | NCIMB | Hosts information relating to strains, cultures and more | Microbial biotechnology | | . | Nettskjema | Form and survey tool, also for sensitive data | Sensitive data TSD | | . | NextCloud | As fully on-premises solution, Nextcloud Hub provides the benefits of online collaboration without the compliance and security risks. | Data storage Data steward infrastructure Data transfer | | . | Nextflow | Nextflow is a framework for data analysis workflow execution | Data steward infrastructure Data analysis | bio.tools TeSS | . | NMRlipids | Repository for lipid MD simulations to validate force fields with NMR data | Biomolecular simulation data | | . | NPDS | The National Poison Data System (NPDS) is a resource that provides poisson exposure occurring in the US and some freely associated states. | Toxicology data | | . | OHDSI | Multi-stakeholder, interdisciplinary collaborative to bring out the value of health data through large-scale analytics. All our solutions are open-source. | Researcher Data steward research Data analysis Data storage TransMed Toxicology data | bio.tools | . | OMERO | OMERO is an open-source client-server platform for managing, visualizing and analyzing microscopy images and associated metadata | Documentation and metadata Data steward research Data steward infrastructure Data storage OMERO | bio.tools TeSS | . | OmicsDI | Omics Discovery Index (OmicsDI) provides a knowledge discovery framework across heterogeneous omics data (genomics, proteomics, transcriptomics and metabolomics) | Existing data Proteomics | bio.tools FAIRsharing TeSS | . | OMOP-CDM | OMOP is a common data model for the harmonisation for of observational health data. | TransMed | | . | Ontobee | A web portal to search and visualise ontologies | Documentation and metadata Data steward research Researcher | FAIRsharing | . | ONTOMATON | OntoMaton facilitates ontology search and tagging functionalities within Google Spreadsheets. | Researcher Data steward research Data steward infrastructure Documentation and metadata Identifiers | | . | Open Definition Conformant Licenses | Licenses that are conformant with the principles laid out in the Open Definition. | Licensing Researcher Data steward research Data steward policy | | . | OpenAIRE Explore | Explore Open Access research outcomes from OpenAIRE network | Existing data | | . | OpenEBench | ELIXIR benchmarking platform to support community-led scientific benchmarking efforts and the technical monitoring of bioinformatics reosurces | Data analysis Data steward research Data steward infrastructure | bio.tools | . | OpenRefine | Data curation tool for working with messy data | Data quality | TeSS | . | OpenScienceFramework | free and open source project management tool that supports the entire research lifecycle: planning, execution, reporting, archiving, and discovery | Data publication Biomolecular simulation data | FAIRsharing | . | OpenStack | OpenStack is an open source cloud computing infrastructure software project and is one of the three most active open source projects in the world | Data storage Data analysis TransMed IFB | TeSS | . | Orphadata | The Orphadata platform provides the scientific community with comprehensive, high-quality datasets related to rare diseases and orphan drugs, in a reusable and computable format | | bio.tools FAIRsharing | . | OSF | OSF (Open Science Framework) is a free, open platform to support your research and enable collaboration. | Data storage Researcher Data steward research | | . | OTP | One Touch Pipeline (OTP) is a data management platform for running bioinformatics pipelines in a high-throughput setting, and for organising the resulting data and metadata. | Human data Documentation and metadata Data management plan Data analysis | bio.tools | . | OwnCloud | Cloud storage and file sharing service | Data storage Data steward infrastructure Data transfer Data analysis | | . | PAA | PAA is an R/Bioconductor tool for protein microarray data analysis aimed at biomarker discovery. | Data analysis Researcher Human data Proteomics | bio.tools | . | PANGAEA | Data Publisher for Earth and Environmental Science | Data publication | bio.tools FAIRsharing | . | PCDDB | The Protein Circular Dichroism Data Bank | Intrinsically disordered proteins Researcher | bio.tools | . | PDB | The Protein Data Bank (PDB) | Researcher Intrinsically disordered proteins | bio.tools TeSS | . | PDB-Dev | Prototype archiving system for structural models obtained using integrative or hybrid modeling | Biomolecular simulation data | | . | PharmGKB | A resource that curates knowledge about the impact of genetic variation on drug response. | Toxicology data | bio.tools | . | Pharos | Pharos provides hazard, use, and exposure information on 140,872 chemicals and 180 different kinds of building products. | Toxicology data | bio.tools | . | PIA - Protein Inference Algorithms | PIA is a toolbox for mass spectrometrey based protein inference and identification analysis. | Data analysis Researcher Proteomics | bio.tools | . | PLAZA | Access point for plant comparative genomics, centralizing genomic data produced by different genome sequencing initiatives. | Plant sciences Plant Genomics Researcher | FAIRsharing TeSS | . | PMut | Platform for the study of the impact of pathological mutations in protein stuctures. | Data analysis Human data | bio.tools | . | PRIDE | PRoteomics IDEntifications (PRIDE) Archive database | Proteomics | bio.tools FAIRsharing TeSS | . | Privacy Impact Assessment Tool | Privacy Impact Assessment Tool is a software, that allows you to carry out Privacy Impact Assessment (PIA) independently. | Data protection Data steward policy Human data | | . | ProteomeXchange | ProteomeXchange provides globally coordinated standard data submission and dissemination pipelines | Proteomics | bio.tools FAIRsharing TeSS | . | Proteomics Standards Initiative | The HUPO Proteomics Standards Initiative defines community standards for data representation in proteomics and interactomics to facilitate data comparison, exchange and verification. | Proteomics | | . | protocols.io | A secure platform for developing and sharing reproducible methods. | Microbial biotechnology | | . | R Markdown | R Markdown documents are fully reproducible. Use a productive notebook interface to weave together narrative text and code to produce elegantly formatted output. Use multiple languages including R, Python, and SQL. | Data analysis Researcher | TeSS | . | RD-Connect Genome Phenome Analysis Platform | The RD-Connect GPAP is an online tool for diagnosis and gene discovery in rare disease research. | Researcher Human data | TeSS | . | RDA Standards | Directory of standard metadata, divided into different research areas | Documentation and metadata Researcher Data steward research | | . | re3data | Registry of Research Data Repositories | Existing data | TeSS | . | REACH registered substances | Portal with public data submitted to ECHA in REACH registration dossiers by substance manufacturers, importers, or their representatives, as laid out by the REACH Regulation (see Understanding REACH regulation). | Toxicology data | | . | REDCap | REDCap is a secure web application for building and managing online surveys and databases. While REDCap can be used to collect virtually any type of data in any environment, it is specifically geared to support online and offline data capture for research studies and operations. | Identifiers Data steward infrastructure Data steward research Data quality | bio.tools | . | REDIportal | Database of A-to-I (deamination of adenosines to inosines) events that enables to search RNA editing sites by genomic region, gene name and other relevant features as the tissue of origin. | Epitranscriptome data | bio.tools | . | REDItools | Python scripts to detect RNA editing events in RNAseq experiments | Epitranscriptome data | bio.tools | . | REDItools2 | REDItools2 is the optimized, parallel multi-node version of REDItools. | Epitranscriptome data | | . | REMS | REMS (Resource Entitlement Management System), developed by CSC, is a tool that can be used to manage researchers’ access rights to datasets. | Data steward infrastructure TransMed | bio.tools | . | Renamer4Mac | File renaming software for Mac | Data organisation Data steward research Researcher | | . | Repository Finder | Repository Finder can help you find an appropriate repository to deposit your research data. The tool is hosted by DataCite and queries the re3data registry of research data repositories. | Data publication Researcher Data steward research | | . | Research Data Management Platform (RDMP) | Data management platform for automated loading, storage, linkage and provision of data sets | Data storage Data steward infrastructure | bio.tools | . | Research Management Plan | Machine actionable DMPs. | Data management plan Researcher Data steward research | TeSS | . | Research Object Crate (RO-Crate) | RO-Crate is a lightweight approach to packaging research data with their metadata, using schema.org. An RO-Crate is a structured archive of all the items that contributed to the research outcome, including their identifiers, provenance, relations and annotations. | Documentation and metadata Data storage Data organisation Data steward research Researcher Microbial biotechnology | FAIRsharing | . | Reva | Reva connects cloud storages and application providers | Data analysis Data transfer | bio.tools | . | Rightfield | RightField is an open-source tool for adding ontology term selection to Excel spreadsheets | Researcher Documentation and metadata Data steward research Microbial biotechnology Identifiers | bio.tools | . | Rstudio | Rstudio notebooks allow to share code, documentation | Data analysis Data steward infrastructure Researcher | bio.tools TeSS | . | Rucio | Rucio - Scientific Data Management | Data storage Data analysis Data transfer | | . | RxNorm | RxNorm is a normalized naming system for medications that is maintained by the National Library of Medicine. Rxnorm provides unique identifiers and allows unambiguous communication of drug-related information across the American health computer systems. | Toxicology data | bio.tools FAIRsharing | . | salDB | SalDB is a salmon specific database of genome sequenced prokaryotes representing the microbiota of fishes found in the taxonomic family of Salmonidae. | Marine Metagenomics | | . | SASBDB | Small Angle Scattering Biological Data Bank | Intrinsically disordered proteins Researcher | | . | SBOL Visual | A standard library of visual glyphs used to represent SBOL designs and interactions. | Microbial biotechnology | | . | SBOLDesigner | A CAD tool to create SBOL designs through the use of SBOL Visual glyphs. | Microbial biotechnology | | . | Schemapedia | Web portal for finding ontologies | Documentation and metadata Data steward research Researcher | | . | ScienceMesh | ScienceMesh - frictionless scientific collaboration and access to research services | Data storage Data analysis Data transfer | | . | Scientific Data's Recommended Repositories | List of respositories recommended by Scientific Data, contains both discipline-specific and general repositories. | Data publication Researcher Data steward research Data steward infrastructure | | . | SeaFile | SeaFile File Synchronization and Share Solution | Data storage Data transfer | | . | semares | All-in-one platform for life science data management, semantic data integration, data analysis and visualization | Researcher Data steward research Documentation and metadata Data analysis Data steward infrastructure Data storage | | . | ShortBOL | A scripting language for creating Synthetic Biology Open Language (SBOL) in a more abstract way. | Microbial biotechnology | | . | SIFTS | Structure integration with function, taxonomy and sequence | Researcher Intrinsically disordered proteins | | . | Silva | SILVA provides comprehensive, quality checked and regularly updated datasets of aligned small (16S/18S, SSU) and large subunit (23S/28S, LSU) ribosomal RNA (rRNA) sequences for all three domains of life (Bacteria, Archaea and Eukarya). | | bio.tools FAIRsharing TeSS | . | Singularity | Singularity is a container platform. | Data steward infrastructure Data analysis TSD | TeSS | . | SMASCH | SMASCH (Smart Scheduling) system, is a web-based tooldesigned for longitudinal clinical studies requiring recurrent follow-upvisits of the participants. SMASCH controls and simplifies the scheduling of big database of patients. Smasch is also used to organize the daily plannings (delegation of tasks) for the different medical professionals such as doctors, nurses and neuropsychologists. | Data organisation TransMed | | . | Snakemake | Snakemake is a framework for data analysis workflow execution | Data steward infrastructure Data analysis | bio.tools TeSS | . | Standards for Reporting Enzyme Data (STRENDA) | Resource of standards for reporting enzyme data | Microbial biotechnology | | . | STRING | Known and predicted protein-protein interactions. | Proteomics | bio.tools FAIRsharing TeSS | . | SynBioHub | A searchable design repository for biological constructs | Microbial biotechnology | bio.tools FAIRsharing | . | Synthetic Biology Open Language (SBOL) | An open standard for the representation of in silico biological designs and their place in the Design-Build-Test-Learn cycle of synthetic biology. | Microbial biotechnology | bio.tools | . | Systems Biology Markup Language (SBML) | An open format for computational models of biological processes | Microbial biotechnology | bio.tools | . | T3DB | The Toxin and Toxin Target Database is a bioinformatics resource that combines exhaustive toxin data with toxin target information. Currently it presents more than 42,000 toxin-target associations extracted from other databases, government documents, books and scientific literature. Each toxin record includes data on chemical properties and descriptors, toxicity values and medical information. | Toxicology data | bio.tools | . | Talend | Talend is an open source data integration platform. | Data steward research Researcher TransMed | | . | TG-GATES | A toxicogenomics database that stores gene expression data and biochemistry, hematology, and histopathology findings derived from in vivo (rat) and in vitro (primary rat hepatocytes, primary human hepatocytes) exposure to 170 compounds at multiple dosages and time points. | Toxicology data | bio.tools | . | The Environment Ontology (EnvO) | An ontology for expressing environmental terms | Microbial biotechnology | | . | The European Genome-phenome Archive (EGA) | EGA is a service for permanent archiving and sharing of all types of personally identifiable genetic and phenotypic data resulting from biomedical research projects | Data publication Human data Data steward policy CSC TSD | bio.tools FAIRsharing | . | The Genomic Standards Consortium (GSC) | Minimum Information about any (x) Sequence | Documentation and metadata Researcher Data steward infrastructure Data steward policy Human data | FAIRsharing | . | The Open Biological and Biomedical Ontology (OBO) Foundry | Collaborative effort to develob interoperable ontologies for the biological sciences | Documentation and metadata Data steward research Researcher | FAIRsharing | . | Tox21_Toolbox | The Toxicology in the 21st Century program, or Tox21, is a unique collaboration between several federal agencies to develop new ways to rapidly test whether substances adversely affect human health. The Tox21 Toolbox contains data-analysis tools for accessing and visualizing Tox21 quantitative high-throughput screening (qHTS) 10K library data, as well as integrating with other publicly available data. | Toxicology data | | . | ToxCast_data | The Toxicology in the 21st Century program, or Tox21, is a unique collaboration between several federal agencies to develop new ways to rapidly test whether substances adversely affect human health. This portal contains diverse downloadable results of the ToxCast project. | Toxicology data | | . | TOXNET | The Toxicology Data Network (TOXNET) was a portal that allowed access to several relevant sources in the toxicological field. Nowadays, these sources have been integrated into other NLM resources. | Toxicology data | | . | tranSMART | Knowledge management and high-content analysis platform enabling analysis of integrated data for the purposes of hypothesis generation, hypothesis validation, and cohort discovery in translational research. | Researcher Data steward research Data analysis Data storage TransMed | bio.tools | . | Tryggve ELSI Checklist | A list of Ethical, Legal, and Societal Implications (ELSI) to consider for research projects on human subjects | Sensitive data Data steward policy Data steward research Human data NeLS CSC TSD | | . | TSD | Norwegian Services for sensitive data | Sensitive data TSD Data storage | TeSS | . | TXG-MAPr | A tool that contains weighted gene co-expression networks obtained from the Primary Human Hepatocytes, rat kidney, and liver TG-GATEs dataset. | Data analysis Researcher Toxicology data | bio.tools | . | UMLS | The Unified Medical Language System (UMLS) is a set of tools that establishes a mapping structure among different vocabularies in the biomedical sciences field to enable interoperativity between computer systems. | Toxicology data | | . | UniChem | UniChem is a very simple, large-scale non-redundant database of pointers between chemical structures and EMBL-EBI chemistry resources. Primarily, this service has been designed to maintain cross references between EBI chemistry resources. These include primary chemistry resources (ChEMBL and ChEBI), and other resources where the main focus is not small molecules, but which may nevertheless contain some small molecule information (eg: Gene Expression Atlas, PDBe). | Toxicology data | | . | UniProt | Comprehensive resource for protein sequence and annotation data | Documentation and metadata Researcher Intrinsically disordered proteins Microbial biotechnology Proteomics | bio.tools FAIRsharing TeSS | . | University of Cambridge - Electronic Research Notebook Products | List of Electronic Research Notebook Products by University of Cambridge | Documentation and metadata Identifiers Researcher Data steward research | | . | VisBOL | A JavaScript library for the visualisation of SBOL. | Microbial biotechnology | | . | Wellcome Open Research - Data Guidelines | Wellcome Open Research requires that the source data underlying the results are made available as soon as an article is published. This page provides information about data you need to include, where your data can be stored, and how your data should be presented. | Data publication Researcher Data steward research | | . | WinSCP | WinSCP is a popular SFTP client and FTP client for Microsoft Windows! Copy file between a local computer and remote servers using FTP, FTPS, SCP, SFTP, WebDAV or S3 file transfer protocols. | Data transfer Data steward infrastructure | | . | XNAT | Open source imaging informatics platform. It facilitates common management, productivity, and quality assurance tasks for imaging and associated data. | Researcher Data analysis TransMed XNAT-PIC | | . | XNAT-PIC Pipelines | Analysing of single or multiple subjects within the same project in XNAT | Researcher Data steward research Data analysis XNAT-PIC | | . | XNAT-PIC Uploader | Import tool for multimodal DICOM image datasets to XNAT | Researcher Data steward research XNAT-PIC | | . | Zenodo | Generalist research data repository built and developed by OpenAIRE and CERN | Data publication Biomolecular simulation data | FAIRsharing TeSS | . | RDM Guide | RDM Guide describes Belgian data management guidelines, resources, tools and services available for researchers in Life Sciences. | Researcher Data steward research | | . | Galaxy Belgium | Galaxy Belgium is a Galaxy instance managed by the Belgian ELIXIR node, funded by the Flemish government, which utilizing infrastructure provided by the Flemish Supercomputer Center (VSC). Galaxy | Researcher Data analysis | | . | ENA upload tool | The program submits experimental data and respective metadata to the European Nucleotide Archive (ENA). | Data steward infrastructure Data steward research Researcher | | . | DMPonline.be | This instance of DMPonline is provided by the DMPbelgium Consortium. We can help you write and maintain data management plans for your research. | Data steward research Researcher Data management plan | | . | PIPPA | PIPPA, the PSB Interface for Plant Phenotype Analysis, is the central web interface and database that provides the tools for the management of the plant imaging robots on the one hand, and the analysis of images and data on the other hand. | Plant sciences Data steward research Researcher Data steward infrastructure | | . | Belnet | Belnet is the privileged partner of higher education, research and administration for connectivity. We provide high-bandwidth internet access and related services for our specific target groups. | Data steward research Researcher Data steward infrastructure Data transfer | | . | Flemish Supercomputing Center (VSC) | VSC is the Flanders' most highly integrated high-performance research computing environment, providing world-class services to government, industry, and researchers. | Data steward research Data steward infrastructure Data analysis Data storage | | . | Red Española de Supercomputación | | | | . | MareNostrum | | | | . | Chipster | Chipster is a user-friendly analysis software for high-throughput data such as RNA-seq and single cell RNA-seq. It contains analysis tools and a large reference genome collection. | CSC Researcher Data steward infrastructure Data analysis | | . | DMPTuuli | Data management planning tool (Finland) | CSC Researcher Data steward research Data management plan | | . | DMP OPIDoR | Online questionnaire for the development of data management plans - repository of DMPs | IFB Researcher Data steward research Data management plan | | . | Feide | Feide is the national solution for secure login and data exchange in education and research. Feide can be linked with ELIXIR-AAI through eduGAIN. | Marine Metagenomics | | . | DS Wizard ELIXIR-Norway | DS Wizard is a simple but powerful solution for researchers to help them understand what is needed for a good, FAIR-oriented Data Stewardship, and to help them build their own Data Management Plans. The template we provide in this instance provides additional guidance on resources, laws and regulations in Norway. | Data management plan | | . | Meta-pipe | META-pipe is a pipeline for annotation and analysis of marine metagenomics samples, which provides insight into phylogenetic diversity, metabolic and functional potential of environmental communities. | Data analysis Marine Metagenomics | | . | MarDB | MarDB includes all non-complete marine microbial genomes regardless of level of completeness. Each entry contains 120 metadata fields including information about sampling environment or host, organism and taxonomy, phenotype, pathogenicity, assembly and annotation. | Data analysis Marine Metagenomics | | . | MarFun | MarFun is a manually curated marine fungi genome database. | Data analysis Marine Metagenomics | | . | Norwegian COVID-19 Data Portal | The Norwegian COVID-19 Data Portal aims to bundle the Norwegian research efforts and offers guidelines, tools, databases and services to support Norwegian COVID-19 researchers. | Human data Sensitive data Existing data Data publication | | . | Norwegian Federated EGA | Federated instance collects metadata of -omics data collections stored in national or regional archives and makes them available for search through the main EGA portal. With this solution, sensitive data will not physically leave the country, but will reside on TSD. | Human data Sensitive data Existing data Data publication | | . | usegalaxy.no | | Human data Data analysis Sensitive data Existing data Data publication | | . | NeLS | Norwegian e-Infrastructure for Life Sciences enables Norwegian life scientists and their international collaborators to store, share, archive, and analyse their omics-scale data | NeLS Marine Metagenomics | | . | NIRD | The National Infrastructure for Research Data (NIRD) infrastructure offers storage services, archiving services, and processing capacity for computing on the stored data. It offers services and capacities to any scientific discipline that requires access to advanced, large-scale, or high-end resources for storing, processing, publishing research data or searching digital databases and collections. This service is owned and operated by [UNINETT Sigma2](https://www.sigma2.no). | Data transfer Data storage NeLS | | . | saga | saga is the national hpc resource. It has a computational capacity of approximately 140 million CPU hours a year and a life expectancy of four year, until 2023. The Norwegian academic high-performance computing and storage infrastructure is maintained by [Sigma2 NRIS](https://sigma2.no/nris), which is a joint collaboration between UiO, UiB, NTNU, UiT, and [UNINETT Sigma2](https://www.sigma2.no/). | Data analysis | | . | RETTE | System for Risk and compliance. Processing of personal data in research and student projects at UiB. | Human data Data protection Sensitive data Data steward policy Data steward research | | . | DataverseNO | | Data publication | | . | FAIRDOM-SEEK | A data Management Platform for organising, sharing and publishing research datasets, models, protocols, samples, publications and other research outcomes. | Data storage Documentation and metadata | | . View national resources 26 . ",
      "url": "/pages/bedroesb/rdmkit/all_tools_and_resources.html",
      "relUrl": "/all_tools_and_resources.html"
    },"11": {
      "doc": "All training resources",
      "title": "All training resources",
      "content": "Overview of all training resources mentioned in RDMkit pages. This overview is automatically generated. It is recommended to add your training materials and events into the ELIXIR training registry TeSS. Skip tool table . | Name | Related page | Registry | . | Training in TeSS about Analysing | Analysing | TeSS | . | Training in TeSS about Belgium | Belgium | TeSS | . | ELIXIR Belgium community in Zenodo | Belgium | Zenodo | . | ELIXIR Belgium YouTube | Belgium | | . | Training in TeSS about Biomolecular simulation data | Biomolecular simulation data | TeSS | . | BioExcel Knowledge Resource Center | Biomolecular simulation data | | . | Training in TeSS about Collecting | Collecting | TeSS | . | Training in TeSS about CSC | CSC | TeSS | . | CSC - Bioscience webpages | CSC | | . | CSC - Training and events webpages | CSC | | . | CSC - Learning Materials for Bioscientists | CSC | | . | CSC - Data management youtube channel | CSC | | . | CSC - Research data management services for life science research (youtube video) | CSC | | . | Data analysis with Chipster - Course packages | CSC | | . | Tutorials and lecture playlists on different topics (youtube) | CSC | | . | Training in TeSS about Data analysis | Data analysis | TeSS | . | Training in TeSS about Data management plan | Data management plan | TeSS | . | Training in TeSS about Data protection | Data protection | TeSS | . | TeSS - ELIXIR’s training portal | Data steward infrastructure | TeSS | . | RDNL - Essentials for Data Support | Data steward infrastructure | | . | Mantra - RDM training | Data steward infrastructure | | . | GO FAIR resources | Data steward infrastructure | | . | Data Carpentry lessons | Data steward infrastructure | | . | RDNL &amp; DCC - Delivering RDM Services | Data steward infrastructure | | . | NPOS/ELIXIR data steward competency framework | Data steward infrastructure | Zenodo | . | ELIXIR Data Management Network | Data steward infrastructure | | . | TeSS - ELIXIR’s training portal | Data steward policy | TeSS | . | RDNL - Essentials for Data Support | Data steward policy | | . | Mantra - RDM training | Data steward policy | | . | FAIR guiding principles | Data steward policy | | . | Data Carpentry lessons | Data steward policy | | . | RDNL &amp; DCC - Delivering RDM Services | Data steward policy | | . | NPOS/ELIXIR data steward competency framework | Data steward policy | Zenodo | . | ELIXIR Data Management Network | Data steward policy | | . | Science Europe - Practical Guide to the International Alignment of RDM | Data steward policy | | . | TeSS - ELIXIR’s training portal | Data steward research | TeSS | . | RDNL - Essentials for Data Support | Data steward research | | . | Mantra - RDM training | Data steward research | | . | GO FAIR starter kit | Data steward research | | . | Data Carpentry lessons | Data steward research | | . | RDNL &amp; DCC - Delivering RDM Services | Data steward research | | . | NPOS/ELIXIR data steward competency framework | Data steward research | Zenodo | . | ELIXIR Data Management Network | Data steward research | | . | Training in TeSS about Data transfer | Data transfer | TeSS | . | Training in TeSS about Human data | Human data | TeSS | . | A FAIR guide for data providers to maximise sharing of human genomic data | Human data | | . | Toward better governance of human genomic data | Human data | | . | OMOP Common Data Model and the OHDSI analytics for observational analytics of real world healthcare data courses in EHDEN academy | Human data | | . | Training in TeSS about IFB | IFB | TeSS | . | Data management training at the IFB | IFB | | . | Inist and the network of regional scientific information units (Urfist) | IFB | | . | Documentation for the IFB core cluster | IFB | | . | Documentation for the Biosphere cloud federation | IFB | | . | Training in TeSS about Marine metagenomics | Marine metagenomics | TeSS | . | Training in TeSS about Marine Metagenomics | Marine Metagenomics | TeSS | . | ELIXIR Norways training pages | Marine Metagenomics | | . | Workshop materials on ELIXIR-SI eLearning Platform (EeLP) | Marine Metagenomics | | . | Training in TeSS about NeLS | NeLS | TeSS | . | General information about the OME ecosystem | OMERO | | . | Collection of presentations given over the years | OMERO | | . | Technical documentation for developers, system administrators | OMERO | | . | Collection of workflow describing to how use the system, with links to scripts and notebooks | OMERO | | . | YouTube channel with tutorials and presentations | OMERO | | . | Imaging Forum - discussions about imaging related topics | OMERO | | . | Training in TeSS about Planning | Planning | TeSS | . | Training in TeSS about Plant sciences | Plant sciences | TeSS | . | Training in TeSS about Preserving | Preserving | TeSS | . | Training in TeSS about Processing | Processing | TeSS | . | Training in TeSS about Proteomics | Proteomics | TeSS | . | Training in TeSS about Researcher | Researcher | TeSS | . | Training in TeSS about Reusing | Reusing | TeSS | . | Training in TeSS about Sensitive data | Sensitive data | TeSS | . | Training in TeSS about Sharing | Sharing | TeSS | . | Training in TeSS about Data storage | Data storage | TeSS | . | Documentation for the HPC cluster | TSD | | . | Courses on the usage of TSD from the University of Oslo | TSD | | . | Recording of a previous course on Nettskjema and TSD | TSD | | . | EOSC-Life website | XNAT-PIC | | . | Euro-Bioimaging website | XNAT-PIC | | . | Data Management - Biological and Preclinical Imaging Perspective | XNAT-PIC | | . | XNAT-PIC - expanding XNAT for image archiving and processing to Preclinical Imaging Centers | XNAT-PIC | | . ",
      "url": "/pages/bedroesb/rdmkit/all_training_resources.html",
      "relUrl": "/all_training_resources.html"
    },"12": {
      "doc": "Analysing",
      "title": "What is data analysis?",
      "content": "Data analysis consists in exploring the collected data to begin understanding the messages contained in a dataset and/or in applying mathematical formula (or models) to identify relationships between variables. The steps of the workflow in the analysis phase will often be repeated several times to explore the data as well as to optimize the workflow itself. According to the different types of data (quantitative or qualitative) the data analysis methods will differ. Data analysis follows the (often automated, batch) data processing stage. ",
      "url": "/pages/bedroesb/rdmkit/analysing.html#what-is-data-analysis",
      "relUrl": "/analysing.html#what-is-data-analysis"
    },"13": {
      "doc": "Analysing",
      "title": "Why is data analysis important?",
      "content": "Since data analysis is the stage where new knowledge and information are generated, it can be considered as central in the research process. Because of the relevance of the data analysis stage in research findings, it is essential that the analysis workflow applied to a dataset complies with the FAIR principles. Moreover, it is extremily important that the analysis workflow is reproducible by other researchers and scientists. With many disciplines becoming data-oriented, more data intensive projects will arise which will require experts from dedicated fields. ",
      "url": "/pages/bedroesb/rdmkit/analysing.html#why-is-data-analysis-important",
      "relUrl": "/analysing.html#why-is-data-analysis-important"
    },"14": {
      "doc": "Analysing",
      "title": "What should be considered for data analysis?",
      "content": "Because of the diversity of domains and techologies in Life Sciences, data can be either “small” or “big data”. As a consequence, the methods and technical solutions used for data analysis might differ. The characteristics of “big data” are often summarized by a growing list of “V” properties: Volume, Velocity, Variety, Variability, Veracity, Visualization and Value. The data analysis stage relies on the previous stages (collection, processing) that will lay the foundations for the generation of new knowledge by providing accurate and trustworthy data. The location of your data is important because of the need of proximity with computing resources. This can impact data transfer across the different infrastructures. It is worthwhile to compare the cost of the transfer of massive amounts of data compared to the transfer of virtual images of machines for the analysis. For the analysis of data, you will first have to consider the computing environment and choose between several computing infrastructure types, e.g. cluster, cloud. You will also need to select the appropriate work environment according to your needs and expertise (command line, web portal). You will have to select the tools best suited for the analysis of your data. It is important to document the exact steps used for data analysis. This includes the version of the software used, as well as the parameters used, as well as the computing environment. Manual “manipulation” of the data may complicate this documentation process. In the case of collaborative data analysis, you will have to ensure access to the data and tools for all collaborators. This can be achieved by setting up virtual research environments. Consider publishing your analysis workflow according to the FAIR principles as well as your datasets. ",
      "url": "/pages/bedroesb/rdmkit/analysing.html#what-should-be-considered-for-data-analysis",
      "relUrl": "/analysing.html#what-should-be-considered-for-data-analysis"
    },"15": {
      "doc": "Analysing",
      "title": "Analysing",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/analysing.html",
      "relUrl": "/analysing.html"
    },"16": {
      "doc": "Belgium",
      "title": "Introduction",
      "content": "An overview of data management services provided by ELIXIR Belgium can be found on the ELIXIR Belgium website. Details about national guidelines, tools and resources can be found at RDM Guide. ",
      "url": "/pages/bedroesb/rdmkit/be_resources.html#introduction",
      "relUrl": "/be_resources.html#introduction"
    },"17": {
      "doc": "Belgium",
      "title": "Funders",
      "content": ". Research Foundation - Flanders (FWO). The Belgian Science Policy Office (BELSPO). Kom op tegen Kanker. Special Research Fund (BOF) from Univeristies. ",
      "url": "/pages/bedroesb/rdmkit/be_resources.html#funders",
      "relUrl": "/be_resources.html#funders"
    },"18": {
      "doc": "Belgium",
      "title": "Belgium",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/be_resources.html",
      "relUrl": "/be_resources.html"
    },"19": {
      "doc": "Biomolecular simulation data",
      "title": "Introduction",
      "content": "Biomolecular simulations are important technique for our understanding and design of biological molecules and their interactions. Simulation methods are demonstrating rapidly growing impact in areas as diverse as biocatalysis, drug delivery, biomaterials, biotechnology, and drug or protein design. Simulations offer the potential of uniquely detailed, atomic‐level insight into mechanisms, dynamics, and processes, as well as increasingly accurate predictions of molecular properties. Yet the field only relatively recently started to store and share (bio)simulation data to be reused for new, unexpected projects, and started discussions about their biomolecular simulation data FAIRification (i.e. to make them Findable, Accessible, Interoperable and Reusable). Here we show several current possibilities moving in this direction, but we should stress that these guidelines are not carved to stone and the biomolecular simulation community still needs to address challenges to FAIRify their data. ",
      "url": "/pages/bedroesb/rdmkit/biomolecular_simulation_data.html#introduction",
      "relUrl": "/biomolecular_simulation_data.html#introduction"
    },"20": {
      "doc": "Biomolecular simulation data",
      "title": "Storing and sharing the data from biomolecular simulations",
      "content": "Description . The biomolecular simulation data comes in several forms and multiple formats, which unfortunately are not completely interoperable. Different methods also require slightly different metadata description. Considerations . What type of data do you have? . Molecular dynamics data - by far the most typical and largest biomolecular simulation data. Each molecular dynamics simulation is driven by the used engine, force-field, and multiple other and often hidden simulation parameters to produce trajectories that are further analysed. Molecular docking data - docking provides the structures of the complex (e.g. ligand-protein, protein-protein, protein-nucleic acid, etc) and its score/energy. Virtual screening data - virtual screening is used for selection of active compounds from the pool of others and is usually in the form of ID and its score/energy. Free energies and other analysis data - data calculatable from the analysis of the simulations. Where should you store this data? . Since there is no common community repository that would be able to gather the often spatious simulation data, the field did not systematically store them. Recently, there’s multiple possibilities where the data can be stored. The repositories can be divided in two main branches: . Generic: Repositories that can be used to store any kind of data. Specific: Repositories designed to store specific data (e.g. MD data). Are you looking for a long-term or short-term storage? Repositories have different options (and sometimes prices) for the storage time of your data. Do you need a static reference for your data? A code (identifier) that can uniquely identify and refer to your data? . What data should you store? What type of data should you store from the whole bunch of data generated in our project. Again, the type of data might vary depending on the biomolecular simulation field. Consider what is essential (absolutely needed to reproduce the simulated experiment) versus what can be extracted from this data (analyses). How do you want your data to be shared? . You should consider the terms in which other scientists can use your data for other projects, access, modify, or redistribute them. Solutions . Deposit your data to a suitable repository for sharing. There’s a long (and incomplete) list of repositories available for data sharing. Repositories are divided into two main categories, general-purpose and discipline-specific, and both categories are utilised in the domain of biomolecular modeling and simulation. For a general introduction to repositories, you are advised to read the data publication page. General-purpose repositories such as Zenodo, FigShare, Mendeley Data, DataDryad, and OpenScienceFramework can be used. Discipline-specific repositories can be used when the repository supports the type of data to be shared e.g. molecular dynamics data. Repositories for various data types and models are listed below: . Molecular Dynamics repositories . GPCRmd - for GPCR protein simulations, with submission process. MoDEL - (https://bio.tools/model) specific database for protein MD simulations. BigNASim - (https://bio.tools/bignasim) specific database for Nucleic Acids MD simulations, with submission process. MoDEL-CNS - specific database for Central Nervous System-related, mainly membrane protein, MD simulations. NMRlipids - project to validate lipid force fields with NMR data with submission process MolSSI - BioExcel COVID-19 therapeutics hub - database with COVID-19 related simulations, with submission process. Molecular Dynamics databases - allow access to precalculated data . BioExcel-CV19 - database and associated web server to offer in a graphical way analyses on top of COVID-19 related MD trajectories stored in the MolSSI-BioExcel COVID-19 therapeutics hub. Dynameomics - database of folding/unfolding pathways MemprotMD - database of automatically generated membrane proteins from PDB inserted into simulated lipid bilayers . Docking respositories . MolSSI - BioExcel COVID-19 therapeutics hub - database with COVID-19 related simulations, with submission process. PDB-Dev - prototype archiving system for structural models using integrative or hybrid modeling, with submission process. Model Archive - theoretical models of macromolecular structures, with submission process. Virtual Screening repositories: . Bioactive Conformational Ensemble - small molecule conformations, with submission process. BindingDB - database of measured binding affinities, focusing chiefly on the interactions of protein considered to be drug-targets with small, drug-like molecules, with submission process. Repositories for the analyzed data from simulations: . MolMeDB - for molecule-membrane interactions and free energy profiles, with submission process. ChannelsDB - resource of channels, pores and tunnels found in biomacromolecules, with submission process. Based on the type of data to be shared, pay attention to what should be included and the data and metadata that will be deposited to repositories. Below listed are some suggested examples of types of essential and optional data describing the biomolecular simulation data: . Molecular Dynamics: . Essentials: . Metadata (Temperature, pressure, program, version, …) Complete set of input files that were used in the simulations Trajectory(ies) Topology(ies) . Optionals: . Analysis data (Free energy, snapshots, clusterization) . Docking poses: . Essentials: . The complete set of molecules tested as well as the scoring functions used and the high-ranking, final poses (3D-structures) Metadata (Identifiers (SMILES, InChI-Key), target (PDBID), energies/scores, program, version, box definition) . Optionals: . Complete ensemble of poses . Virtual Screening: . Essentials: . List of molecules sorted Metadata (identifiers of ligands and decoy molecules, target, program+version, type of VS (QSAR, ML, Docking,…)) . Optionals: . Details of the method, scores, … . Free energies and other analyses: . Essentials: . Metadata (model, method, program, version, force field(s), etc.) Values (Free energy values, channels, etc.) . Optionals: . Link to Trajectory (Dynamic PDB?) . Associate a license with the data and/or source code e.g. models. Licenses mainly differ on openness vs restrictiveness, and it is crucial to understand the differences among licenses before sharing your research outputs. The RDMkit licensing page lists resources that can help you understand licensing and choose an appropriate license. Related problems . File formats Biomolecular simulation field has a tendency to produce a multitude of input/output formats, each of them mainly related to one software package. That makes interoperability and reproducibility really difficult. You can share your data but this data will only be useful if the scientist interested in it has access to the tool that has generated it. The field is working on possible standards (e.g. TNG trajectory). Metadata standards There is no existing standard defining the type and format of the metadata needed to describe a particular project and its associated data. How to store the program, version, parameters used, input files, etc. is still an open question, which has been addressed in many ways and using many formats (json, xml, txt, etc.). Again, different initiatives exist trying to address this issue (see further references). Data size Data generated in the biomolecular simulation field is growing at an alarming pace. Making this data available to the scientific community sometimes means transferring them to a long-term storage, and even this a priori straightforward process can be cumbersome because of the large data size. ",
      "url": "/pages/bedroesb/rdmkit/biomolecular_simulation_data.html#storing-and-sharing-the-data-from-biomolecular-simulations",
      "relUrl": "/biomolecular_simulation_data.html#storing-and-sharing-the-data-from-biomolecular-simulations"
    },"21": {
      "doc": "Biomolecular simulation data",
      "title": "Biomolecular simulation data",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/biomolecular_simulation_data.html",
      "relUrl": "/biomolecular_simulation_data.html"
    },"22": {
      "doc": "Collecting",
      "title": "What is data collection?",
      "content": "Data collection is the process where information is gathered about specific variables of interest either using instrumentation or other methods (e.g. questionnaires, patient records). While data collection methods depend on the field and research subject, it is important to ensure data quality. You can also reuse existing data in your project. This can either be individual earlier collected datasets or reference data from curated resources like ELIXIR Core Data Resources or consensus data like reference genomes. For more information see Reuse in the data life cycle. ",
      "url": "/pages/bedroesb/rdmkit/collecting.html#what-is-data-collection",
      "relUrl": "/collecting.html#what-is-data-collection"
    },"23": {
      "doc": "Collecting",
      "title": "Why is data collection important?",
      "content": "Apart from being the source of information to build your findings on, the collection phase lays the foundation for the quality of both the data and its documentation. It is important that the decisions made regarding quality measures are implemented, and that the collect procedures are appropriately recorded. ",
      "url": "/pages/bedroesb/rdmkit/collecting.html#why-is-data-collection-important",
      "relUrl": "/collecting.html#why-is-data-collection-important"
    },"24": {
      "doc": "Collecting",
      "title": "What should be considered for data collection?",
      "content": "Appropriate tools or integration of multiple tools (also called tool assembly or ecosystem) can help you with data management and documentation during data collection. Suitable tools for data management and documentation during data collection are Electronic Lab Notebooks (ELNs), Electronic Data Capture (EDC) systems, Laboratory Information Management Systems (LIMS). Moreover, online platforms for collaborative research and file sharing services could also be used as ELN or data management systems. Independently of the tools you will use, consider the following, while collecting data . How to capture provenance - e.g. of samples, researchers and instruments Ensure data quality - data can either be generated by yourself, or by another infrastructure or facility with this specialization Reusing data instead of generating new data Experimental design - including a collection plan (e.g. repetitions, controls, randomization) in advance Instrument calibration If you work with sensitive or confidential data, take care of data protection and security issues If you work with human-related data, think about permissions, consent How to store the data Where to store the data Identify suitable metadata standards . ",
      "url": "/pages/bedroesb/rdmkit/collecting.html#what-should-be-considered-for-data-collection",
      "relUrl": "/collecting.html#what-should-be-considered-for-data-collection"
    },"25": {
      "doc": "Collecting",
      "title": "Collecting",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/collecting.html",
      "relUrl": "/collecting.html"
    },"26": {
      "doc": "Compliance monitoring & measurement",
      "title": "How can you measure and document data management capabilities?",
      "content": "Description . Being able to reliably measure and document capabilities in data management, data protection and information security is important for research institutions. By knowing their capabilities institutions can spot areas of improvement and direct human and IT resources accordingly. Also, having capabilities documented or formalised by certifications saves a good deal of effort during data management planning. Considerations . Are you being asked to describe information security and data protection arrangements for a project DMP and you find yourself repeating similar descriptions across DMPs of projects? . Contact your institution’s Data Protection Officer (DPO) and Chief Information Security Officer (CISO). They may be able to provide you with a standard description of data protection and information security measures for institutional data platforms. Inquire whether the platforms you will use for your project’s data management have an information security or data privacy certification. Are you providing a data service, such as data hosting, curation or archival and want to document and assess your service’s capabilities? . Consider measuring the FAIR maturity of your services and the FAIRness of your data assets using community adopted standard metrics. Solutions . FAIR data . GO-FAIR Initiative provides a framework for designing metrics for the evaluation of FAIRness. RDA developed a first set of guidelines and a checklist related to the implementation of the FAIR indicators. The FAIRplus project with its FAIR Cookbook provides services, tools, and indicators necessary for the assessment or the evaluation of data against the FAIR Principles: . FAIR Evaluators are an automated approach to evaluate FAIRness of data services. FAIRassist.org aims to collect and describe existing resources for the assessment and/or evaluation of digital objects against the FAIR principles. Information Security, Data Protection, Accountability . 21 CFR part 11 is a standard, which outlines criteria for electronic records in an IT system to be as valid as signed paper records. It is widely adopted in lab information systems and applications used in clinical trials and medical research. ISO 27001 is an international standard for the management of information security. It is adopted by some universities and research institutes to certify their data centres. ISO/IEC 27018 is a standard aimed to be a code of practice for protection of personally identifiable information (PII) in public clouds. ",
      "url": "/pages/bedroesb/rdmkit/compliance_monitoring.html#how-can-you-measure-and-document-data-management-capabilities",
      "relUrl": "/compliance_monitoring.html#how-can-you-measure-and-document-data-management-capabilities"
    },"27": {
      "doc": "Compliance monitoring & measurement",
      "title": "How can you ethically access genetic resources of another country?",
      "content": "Description . If during your research project you need to access or transport genetic resources and/or associated traditional knowledge from any country, you should comply to all relevant (inter)national legislation. One important legislation in this case is the Nagoya Protocol. The Nagoya Protocol specifies the Access and Benefit-Sharing (ABS) principles, established by the Convention on Biological Diversity (CBD), for countries providing and using genetic resources in a legally binding way. Article 3 of CBD clarifies, that states have sovereign rights over their own (biological and genetic) resources. Negotiations concluded in 2014 with the Nagoya Protocol on ABS. Since then, working with genetic resources and associated data of another country requires more preparatory measures. The aim of the Nagoya protocol is to ensure fair and equitable sharing of benefits arising from utilisation of genetic resources and from traditional knowledge associated with genetic resources. Many contries, as well as the EU, are parties of the Nagoya Protocol and information on this can be found at the ABS Clearing House. By enactment of EU Regulation No. 511/2014 the obligations were implemented in the EU on 12.10.2014. Here you can find a short video about ABS – Simply Explained. Genetic resources are defined as “all genetic material of actual or potential value. Essentially, the term encompasses all living organisms (plants, animals and microbes) that carry genetic material potentially useful to humans. Genetic resources can be taken from the wild, domesticated or cultivated. They are sourced from: natural environments (in situ) or human-made collections (ex situ) (e.g. botanical gardens, gene banks, seed banks and microbial culture collections).”. The definition of “traditional knowledge associated with genetic resources” is left to the Parties of the Protocol instead. However, in the context of the Nagoya Protocol, “the term is used in relation to the knowledge, innovations and practices of indigenous and local communities that result from the close interaction of such communities with their natural environment, and specifically to knowledge that may provide lead information for scientific discoveries on the genetic or biochemical properties of genetic resources. It is characteristic of traditional knowledge that it is not known outside the community holding such knowledge.”. Considerations . Since ABS regulations and Nagoya Protocol put high demands on documentation, this legal aspect is time consuming and therefore needs to be taken into account when planning the research project. ABS is not relevant for all genetic resources. It applies only to resources that have been accessed from a provider country after October 12, 2014. Some genetic resources are explicitly excluded, like for example human genomes, some crops and some viruses. Moreover, there are countries who are party of the Nagoya Protocol, but have no ABS legislation in place. If ABS is relevant to the project it should be part of the Data Management Plan. You must comply with the Nagoya Protocol and other national legislation before accessing the genetic resources. When negotiating the Mutually Agreed Terms (MAT), it is very important to think about the future reusability of the data generated based on the genetic resources. When sharing this data, it is important to include the necessary metadata regarding ABS and to clarify the legal basis, in order to make the data reusable to others again. Solutions . In the planning stage of your research project, allow extra time to familiarise yourself with the legal requirements. In order to determine if the Nagoya Protocol applies to your research, take a look at: . The European documents Sharing nature’s genetic resources – ABS and Access and Benefit Sharing. The dedicated websites Nagoya Protocol or ABS Clearing-House. Look for “Nagoya Protocol checklists for researchers” available in your institution to determine if the Nagoya Protocol applies to your research. Ask help to legal experts and get in contact with the corresponding office in your country or the legal team in your institution. If ABS principles and Nagoya Protocol apply to your project, make sure to: . Investigate the conditions for accessing the genetic resources and/or the associated traditional knowledge in the country of origin. Make a Prior Informed Consent (PIC) with the country that will provide the genetic resources and/or the associated traditional knowledge, to clarify the goal of your research and how you will use the requested resources. Negotiate a Mutually Agreed Terms (MAT) to establish how to share the resulting benefits. The benefits for the provider of the genetic resources and/or the associated traditional knowledge can be monetary, transfer of knowledge and technology, training, etc. ",
      "url": "/pages/bedroesb/rdmkit/compliance_monitoring.html#how-can-you-ethically-access-genetic-resources-of-another-country",
      "relUrl": "/compliance_monitoring.html#how-can-you-ethically-access-genetic-resources-of-another-country"
    },"28": {
      "doc": "Compliance monitoring & measurement",
      "title": "Compliance monitoring & measurement",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/compliance_monitoring.html",
      "relUrl": "/compliance_monitoring.html"
    },"29": {
      "doc": "Contact",
      "title": "Contact",
      "content": "For questions and general enquiries please contact us at rdm-editors@elixir-europe.org. ",
      "url": "/pages/bedroesb/rdmkit/contact.html",
      "relUrl": "/contact.html"
    },"30": {
      "doc": "Contributors",
      "title": "Contributors",
      "content": "This project would not be possible without the many amazing community contributors. RDMkit is an open community project, and you are welcome to join us! . Alan R Williams . Alessandro Paglialonga . Alex Henderson . Alexander Botzki . Alexia Cardona . Ana Portugal Melo . Anastasia Chasapi . Anastasis Oulas . Anika Frericks-Zipper . Anil Wipat . Anne-Françoise Adam-Blondon . INRAE / ELIXIR-FR . Editor Bert Droesbeke . VIB-UGent / ELIXIR-BE . Bradley Brown . Carlos Vega . LCSB . Lead Carole Goble . The University of Manchester . Celia van Gelder . Chris Child . Christian Atallah . Christophe Trefois . LCSB / ELIXIR-LU . Cyril Pommier . Editor Daniel Faria . University of Lisbon / ELIXIR-PT . Daniel Wibberg . Dario Longo . David Markham . Dominik Martinat . Elin Kronander . NBIS / ELIXIR-SE . Erik Hjerde . Ernesto Picardi . Erwan Le Floch . Espen Åberg . Evangelos Pafilis . Editor Federico Bianchini . Fedrico Bianchini . Ferran Sanz . Universitat Pompeu Fabra . Flavio Licciulli . Editor Flora D'Anna . VIB-UGent / ELIXIR-BE . Lead Frederik Coppens . VIB-UGent / ELIXIR-BE . Frederik Delaere . Graham Hughes . Gregoire Rossier . Guy Cochrane . Hinri Kerstens . Lead Inge Jonassen . University of Bergen . Irina Balaur . LCSB . Ivan Mičetić . Janet Piñero Gonzalez . Universitat Pompeu Fabra . Jean-Marie Burel . Euro-BioImaging | University of Dundee . Juan Manuel Ramírez-Anguita . Universitat Pompeu Fabra . Karel Berka . Kees van Bochove . The Hyve . Editor Korbinian Bösl . University of Bergen / ELIXIR-NO . Laura Portell . Editor Laura Portell Silva . Barcelona Supercomputing Center / ELIXIR-ES . M-Christine Jacquemot-Perbal . Manuel Pastor . Universitat Pompeu Fabra . Marco Carraro . Marco Roos . Marcus Lundberg . Marianna Ventouratou . Marie-Christine Jacquemot . Marko Vidak . Markus Englund . NBIS / ELIXIR-SE . Editor Martin Cook . ELIXIR Hub . Martin Eisenacher . Michael R. Crusoe . Michael Turewicz . Miguel Angel Mayer . Universitat Pompeu Fabra . Mijke Jetten . Minna Ahokas . CSC - IT Center for Science / ELIXIR-FI . Muhammad Shoaib . LCSB . Editor Munazah Andrabi . Nadia Tonello . Nadim Rahman . Nazeefa Fatima . Nick Juty . Editor Niclas Jareborg . NBIS / ELIXIR-SE . Nicola Soranzo . Earlham Institute . Nils P Willassen . Nils Peder Willassen . Nirupama Benis . Olivier Collin . Ott Oopkaup . Paulette Lieby . Pedro Fernandes . Peter McQuilton . Editor Pinar Alper . LCSB / ELIXIR-LU . Priit Adler . Rafael Andrade Buono . Editor Rob Hooft . Robert Andrews . Sam Holt . Sara Zullino . Euro-BioImaging | University of Torino . Sebastian Beier . Shuxin Zhang . Siiri Fuchs . CSC - IT Center for Science / ELIXIR-FI . Silvio Aime . Sirarat Sarntivijai . Soumyabrata Ghosh . LCSB . Stian Soiland-Reyes . Thanasis Vergoulis . Tina Visnovska . Editor Ulrike Wittig . HITS / ELIXIR-DE . Venkata Satagopam . LCSB / ELIXIR-LU . Vera Ortseifen . Vicky Sundesha . Victoria Dominguez D. Angel . Walter Dastrù . Wei Gu . LCSB / ELIXIR-LU . Wolmar Nyberg Åkerström . NBIS / ELIXIR-SE . Xinhui Wang . LCSB . Yvonne Kallberg . NBIS / ELIXIR-SE . ",
      "url": "/pages/bedroesb/rdmkit/contributors.html",
      "relUrl": "/contributors.html"
    },"31": {
      "doc": "Copyright guidelines",
      "title": "Quoting from other resources",
      "content": "If you do need to quote from another resource, like a website or publication: . Enclose the quote within quotation marks Keep the exact words Link to the resource Give clear credit by naming the author (if known) and the resource Give the full citation for a publication. Please do not copy the text and tweak the wording slightly. If you are not giving the exact quote, then you must express the concept in your own words. Avoid using long quotations from other resources. ",
      "url": "/pages/bedroesb/rdmkit/copyright.html#quoting-from-other-resources",
      "relUrl": "/copyright.html#quoting-from-other-resources"
    },"32": {
      "doc": "Copyright guidelines",
      "title": "Images",
      "content": "It is your responsibility to make sure you have permission to use the images you put on the website. The great majority of images on the web are copyrighted. This includes images on social media, even if they have been reposted or shared. Please make sure you have permission to use them before including them on the website, ideally in writing. If you are not sure whether you can use them, assume that you do not have permission, and contact the copyright owner. If you are using an image with a Creative Commons license, remember you still need to attribute the creator in accordance with the license. If you are using a stock image you have bought, or a royalty-free image, check the image license to make sure that you can use it on the RDMkit website. If you are using images provided by your institute or organisation, please also check that you can use them on this site. You are welcome to create the images yourself, but please follow the site style guide, and bear in mind the image may be modified to better suite the style of the site. The content of the site is published under the Creative Commons 4 license. If you think an image would be helpful to explain your content but you cannot find the right one, then create a new issue to suggest a new image. ",
      "url": "/pages/bedroesb/rdmkit/copyright.html#images",
      "relUrl": "/copyright.html#images"
    },"33": {
      "doc": "Copyright guidelines",
      "title": "Copyright guidelines",
      "content": "This website aims to be unique and authoritative, but do feel free to include quotes and images from other resources if they add value to the page. Please follow the guidelines below if you do so and respect copyright. ",
      "url": "/pages/bedroesb/rdmkit/copyright.html",
      "relUrl": "/copyright.html"
    },"34": {
      "doc": "COVID-19 Data Portal",
      "title": "What is the European COVID-19 Data Portal?",
      "content": "The European COVID-19 Data Platform was launched to facilitate the urgent need to share and analyse COVID-19 data and thus accelerate research that will provide responses and build solutions, such as vaccines, treatments and public health interventions. The Platform comprises three core components, the SARS-CoV-2 Data Hubs, the Federated European Genome-phenome Archive and the COVID-19 Data Portal. The COVID-19 Data Portal brings together and continuously updates relevant COVID-19 datasets from a breadth of analytical platforms. Data are submitted using the SARS-CoV-2 Data Hubs functions or via other major centres of biomedical data. The data available from the COVID-19 Data Portal cover raw and assembled viral and human sequences, protein structures, proteomics, gene and protein expression data, compound screening, metabolomics and imaging data; COVID-19-relevant literature publications and pre-prints are also integrated. The aim is to have a wide variety of open data from across the globe systematised and easily accessible to researchers following FAIR principles (Findable, Accessible, Interoperable and Reusable). The European COVID-19 Data Platform enables national data producers to share biomolecular data with the international scientific community, making these data available for reuse. Ultimately it aims to allow for rapid analysis and dissemination to inform research, public health and health communities and in an evidenced-based manner. ",
      "url": "/pages/bedroesb/rdmkit/covid19_data_portal.html#what-is-the-european-covid-19-data-portal",
      "relUrl": "/covid19_data_portal.html#what-is-the-european-covid-19-data-portal"
    },"35": {
      "doc": "COVID-19 Data Portal",
      "title": "How the portal is useful for researchers and how it is supposed to fit into their processes?",
      "content": "Researchers benefit the COVID-19 Data Portal in a host of ways: . Discoverability of COVID-19-relevant biomolecular data and related literature Support for the management, sharing, analysis and publication of newly-generated data Access to integrated COVID-19 data across multiple assay platforms Reusable data Flexible access via web, API and FTP interfaces Access to data exploration and analysis tools . ",
      "url": "/pages/bedroesb/rdmkit/covid19_data_portal.html#how-the-portal-is-useful-for-researchers-and-how-it-is-supposed-to-fit-into-their-processes",
      "relUrl": "/covid19_data_portal.html#how-the-portal-is-useful-for-researchers-and-how-it-is-supposed-to-fit-into-their-processes"
    },"36": {
      "doc": "COVID-19 Data Portal",
      "title": "What are the components for the COVID-19 Data Portal?",
      "content": "The COVID-19 Data Portal data flow schematic, showing collation, indexing, integration and user-access functions. The following reusable components will be of value to data stewards: . Discovery API with documented COVID-19 index points across data and literature resources covered by the COVID-19 Data Portal. Options for membership of the network of national COVID-19 Data Portals, currently comprising some 8 partners. Toolkit providing elements to support nation COVID-19 Data Portal construction. Open source web application to establish national COVID-19 Data Portal, courtesy of ELIXIR Sweden. Support via ecovid19@ebi.ac.uk . ",
      "url": "/pages/bedroesb/rdmkit/covid19_data_portal.html#what-are-the-components-for-the-covid-19-data-portal",
      "relUrl": "/covid19_data_portal.html#what-are-the-components-for-the-covid-19-data-portal"
    },"37": {
      "doc": "COVID-19 Data Portal",
      "title": "What country specific instances are there and how new instances are deployed?",
      "content": "The European COVID-19 Data Portal includes a federation of national data portals, hosted in those nations. As of early 2021, this includes Italy, Japan, Norway, Poland, Slovenia, Spain, Sweden and Turkey. The national COVID-19 Data Portals provide information, guidelines, tools and services to support each nation’s researchers in creating and sharing research data on COVID-19. The purpose of the national Portals is to provide an entry and orientation point into national activities including support, data management tools, projects and funding. For centralised data, such as viral sequences, national Portals are closely linked into the central COVID-19 Data Portal to allow users smooth access across the full selection of functions available to them. ",
      "url": "/pages/bedroesb/rdmkit/covid19_data_portal.html#what-country-specific-instances-are-there-and-how-new-instances-are-deployed",
      "relUrl": "/covid19_data_portal.html#what-country-specific-instances-are-there-and-how-new-instances-are-deployed"
    },"38": {
      "doc": "COVID-19 Data Portal",
      "title": "COVID-19 Data Portal",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/covid19_data_portal.html",
      "relUrl": "/covid19_data_portal.html"
    },"39": {
      "doc": "CSC",
      "title": "What is the CSC data management tool assembly?",
      "content": "CSC – IT Center for Science and ELIXIR Finland provide services, tools and software for managing research data throughout the project life cycle. Services cover computing environments, analysis programs, tools for storing and sharing data during the project as well as opening and discovering research data. Furthermore, ELIXIR-FI provides flexible infrastructure for bioinformatics data analysis. Services are actively developed, and hence, please visit CSC web pages for the latest updates. ",
      "url": "/pages/bedroesb/rdmkit/csc_assembly.html#what-is-the-csc-data-management-tool-assembly",
      "relUrl": "/csc_assembly.html#what-is-the-csc-data-management-tool-assembly"
    },"40": {
      "doc": "CSC",
      "title": "Who can use the CSC data management tool assembly?",
      "content": "CSC and ELIXIR-FI services are accessible for researchers in Finland and to foreign collaborators of Finland-based research groups. Most of CSC’s services are free-of-charge for academic research, education and training purposes in Finnish higher education institutions and in state research institutes. Researchers can start using services by registering an account and get bioinformatics user support from our service desk. ",
      "url": "/pages/bedroesb/rdmkit/csc_assembly.html#who-can-use-the-csc-data-management-tool-assembly",
      "relUrl": "/csc_assembly.html#who-can-use-the-csc-data-management-tool-assembly"
    },"41": {
      "doc": "CSC",
      "title": "How can you access the CSC data management tool assembly?",
      "content": "You can access all CSC services through several secure authentication methods. Start by registering an account at CSC with your home organization HAKA or VIRTU login or by contacting our service desk. Afterwards you can also use ELIXIR login. Find more information from CSC accounts and support web pages how to get access to different services. ",
      "url": "/pages/bedroesb/rdmkit/csc_assembly.html#how-can-you-access-the-csc-data-management-tool-assembly",
      "relUrl": "/csc_assembly.html#how-can-you-access-the-csc-data-management-tool-assembly"
    },"42": {
      "doc": "CSC",
      "title": "For what can you use the CSC data management tool assembly?",
      "content": "Figure 1. The CSC - IT Center for Science data management tool assembly. Data management planning . You can get support for data management planning through DMPTuuli, a Finnish instance of DMPonline which includes guidance and templates provided by different organisations and funders. DMPs created in DMPTuuli are not yet machine actionable or linked to the CSC data management tool assembly services. However, a DMP is a valuable document when contacting the CSC research data management services. Data collection . When you start collecting data and need a storing environment where you can, for example, host cumulating or changing data, Allas Object Storage is the recommended option. Indeed, Allas is CSC’s general purpose research data storage server, which can be accessed on the CSC servers as well as from anywhere on the internet. Allas can be used both for static research data that needs to be available for analysis and to collect cumulating or changing data. For example, if you work with sequence data, the sequencing provider can transfer the data directly to Allas under your project. Data processing and analysis . For processing, analysing and storing data during the research project, CSC offers several computing platforms. These include both environments for non-sensitive and sensitive data. Depending on your needs, you can choose from a wide variety of computing resources: use Chipster software for high-throughput data such as RNA-seq and single cell RNA-seq, build your own custom virtual machine, or utilise the full power of our world-class supercomputers. Supercomputers Puhti and Mahti can be used for larger scale analysis and simulations. They will soon be accompanied with the world-class supercomputer LUMI. Pouta and Rahti cloud computing services offer more flexibility, allowing the user to manage the infrastructure. CSC’s computers have a wide range of preinstalled scientific software and databases with usage instructions. This summer, CSC will be releasing beta versions of new services for sensitive data management: Sensitive Data Desktop (SD Desktop) and Sensitive Data Connect (SD Connect). Sensitive Data Submit (SD Submit) will be available later this year. The new Sensitive Data Services are designed to facilitate collaborative research across Finland and between Finnish academics and their collaborators. SD Desktop is a service that allows a user and their authorized colleagues to access a private computing environment workspace via a web browser and analyze the data within a secure cloud. SD Connect allows you to collect, organize and share your encrypted sensitive data in a secure manner via web browser. Data sharing and publishing . It is recommended to publish data in data specific repositories. You can find many options from ELIXIR Deposition Databases for Biomolecular data web page. Furthermore, CSC and ELIXIR-FI will offer Federated EGA for sensitive human biomedical data that is linked to the central European Genome-phenome Archive and the SD Submit at the end of 2021. SD Submit allows you to publish sensitive data securely in a national repository. The service will give you the tools to describe your dataset (adding the appropriate metadata) and assign a permanent identifier (DOI). After publication, you will remain the data controller and decide according to specific policies, who can access the sensitive data for reuse. According to the GDPR, your data will remain within the Finnish borders and, at the same time, they will be accessible and discoverable according to FAIR data principles. In addition to the above mentioned services, you can use national Fairdata.fi services. Fairdata IDA storage service enables saving, organising and sharing data within the project group and storing the data in an immutable state. After freezing your data in IDA, you can use Qvain, the research dataset description tool, to describe your data and thus create core metadata for your dataset, and publish it. Publishing means that your dataset will be published in Etsin, the research data finder, where you can discover and download any files you have associated with the dataset. Any published dataset is also made available to the Research.fi portal automatically by Fairdata services. ",
      "url": "/pages/bedroesb/rdmkit/csc_assembly.html#for-what-can-you-use-the-csc-data-management-tool-assembly",
      "relUrl": "/csc_assembly.html#for-what-can-you-use-the-csc-data-management-tool-assembly"
    },"43": {
      "doc": "CSC",
      "title": "CSC",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/csc_assembly.html",
      "relUrl": "/csc_assembly.html"
    },"44": {
      "doc": "Data analysis",
      "title": "What are the best practices for data analysis?",
      "content": "Description . When carrying out your analysis, you should also keep in mind that all your data analysis has to be reproducible. This will complement your research data management approach since your data will be FAIR compliant but also your tools and analysis environments. In other words, you should be able to tell what data and what code or tools were used to generate your results. This will help to tackle reproducibility problems but also will improve the impact of your research through collaborations with scientists who will reproduce your in silico experiments. Considerations . There are many ways that will bring reproducibility to your data analysis. You can act at several levels: . By providing your code. By providing your execution environment. By providing your workflows. By providing your data analysis execution. Solutions . Make your code available. If you have to develop some software for your data analysis, it is always a good idea to publish your code. The git versioning system offers both a way to release your code but offers also a versioning system. You can also use Git to interact with your software users. Be sure to specify a license for your code (see the licensing section). Use package and environment management system. By using package and environment management systems like Conda and its bioinformatics specialized channel Bioconda, researchers that have got access to your code will be able to easily install specific versions of tools, even older ones, in an isolated environment. They will be able to compile/run your code in an equivalent computational environment, including any dependencies such as the correct version of R or particular libraries and command-line tools your code use. You can also share and preserve your setup by specifying in a environment file which tools you installed. Use container environments. As an alternative to package management systems you can consider container environments like Docker or Singularity. Use workflow management systems. Scientific Workflow management systems will help you organize and automate how computational tools are to be executed. Compared to composing tools using a standalone script, workflow systems also help document the different computational analyses applied to your data, and can help with scalability, such as cloud execution. Reproducibility is also enhanced by the use of workflows, as they typically have bindings for specifying software packages or containers for the tools you use from the workflow, allowing others to re-run your workflow without needing to pre-install every piece of software it needs. It is a flourishing field and many other workflow management systems are available, some of which are general-purpose (e.g. any command line tool), while others are domain-specific and have tighter tool integration. Among the many workflow management systems available, one can mention . Workflow platforms that manage your data and provide an interface (web, GUI, APIs) to run complex pipelines and review their results. For instance: Galaxy and Arvados (CWL-based, open source). Workflow runners that take a workflow written in a proprietary or standardized format (such as the CWL standard) and execute it locally or on a remote compute infrastructure. For instance, toil-cwl-runner, the reference CWL runner (cwltool), Nextflow, Snakemake, Cromwell. Use notebooks. Using notebooks, you will be able to create reproducible documents mixing text and code; which can help explain your analysis choices; but also be used as an exploratory method to examine data in detail. Notebooks can be used in conjunction with the other solutions mentioned above, as typically the notebook can be converted to a script. Some of the most well-known notebooks systems are: Jupyter, with built-in support for code in Python, R and Julia, and many other kernels; RStudio based on R. See the table below for additional tools. ",
      "url": "/pages/bedroesb/rdmkit/data_analysis.html#what-are-the-best-practices-for-data-analysis",
      "relUrl": "/data_analysis.html#what-are-the-best-practices-for-data-analysis"
    },"45": {
      "doc": "Data analysis",
      "title": "How can you use package and environment management systems?",
      "content": "Description . By using package and environment management systems like Conda and its bioinformatics specialized channel Bioconda, you will be able to easily install specific versions of tools, even older ones, in an isolated environment. You can also share and preserve your setup by specifying in a environment file which tools you installed. Considerations . Conda works by making a nested folder containing the traditional UNIX directory structure bin/ lib/ but installed from Conda’s repositories instead of from a Linux distribution. As such Conda enables consistent installation of computational tools independent of your distribution or operating system version. Conda is available for Linux, macOS and Windows, giving consistent experience across operating systems (although not all software is available for all OSes). Package management systems work particularly well for installing free and Open Source software, but can also be useful for creating an isolated environment for installing commercial software packages; for instance if they requires an older Python version than you have pre-installed. Conda is one example of a generic package management, but individual programming languages typically have their environment management and package repositories. You may want to consider submitting a release of your own code, or at least the general bits of it, to the package repositories for your programming language. Solutions . MacOS-specific package management systems: Homebrew, Macports. Windows-specific package management systems: Chocolatey and Windows Package Manager winget. Linux distributions also have their own package management systems (rpm/yum/dnf, deb/apt) that have a wide variety of tools available, but at the cost of less flexibility in terms of the tool versions, to ensure they exist co-installed. Language-specific virtual environments and repositories: rvm and RubyGems for Ruby, pip and venv for Python, npm for NodeJS/Javascript, renv and CRAN for R, Apache Maven or Gradle for Java etc. Tips and tricks to navigate the landscape of software package management solutions: . If you need multiple tools/programming languages, but your machines have different OS types or versions, list packages in a Conda environment.yml. If you need conflicting versions of some tools/libraries for different operations, make separate Conda environments. If you need a few open source libraries for my Python script, none which require complilation, make a requirements.txt and reference pip packages. ",
      "url": "/pages/bedroesb/rdmkit/data_analysis.html#how-can-you-use-package-and-environment-management-systems",
      "relUrl": "/data_analysis.html#how-can-you-use-package-and-environment-management-systems"
    },"46": {
      "doc": "Data analysis",
      "title": "How can you use container environments?",
      "content": "Description . Container environments like Docker or Singularity allow you to easily install specific versions of tools, even older ones, in an isolated environment. Considerations . In short containers works almost like a virtual machine (VMs), in that it re-creates a whole Linux distibution with separation of processes, files and network. Containers are more lightweight than VMs since they don’t virtualize hardware. This allows a container to run with a fixed version of the distribution independent of the host, and have just the right, minimal dependencies installed. The container isolation also adds a level of isolation, which although not as secure as VMs, can reduce the attack vectors. For instance if the database container was compromised by unwelcome visitors, they would not have access to modify the web server configuration, and the container would not be able to expose additional services to the Internet. A big advantage of containers is that there are large registries of community-provided container images. Note that modifying things inside a container is harder than in a usual machine, as changes from the image are lost when a container is recreated. Typically containers run just one tool or applications, and for service deployment this is useful for instance to run mySQL database in a separate container from a NodeJS application. Solutions . Docker is the most well-known container runtime, followed by Singularity. These require (and could be used to access) system administrator privileges to be set up. uDocker and Podman are also user space alternatives that have compatible command line usage. Large registries of community-provided container images are Docker Hub and RedHat Quay.io. These are often ready-to-go, not requiring any additional configuration or installations, allowing your application to quickly have access to open source server solutions. Biocontainers have a large selection of bioinformatics tools. To customize a Docker image, it is possible to use techniques such as volumes to store data and Dockerfile. This is useful for installing your own application inside a new container image, based on a suitable base image where you can do your apt install and software setup in a reproducible fashion - and share your own application as an image on Docker Hub. Container linkage can be done by container composition using tools like Docker Compose. More advanced container deployment solutions like Kubernetes and Computational Workflow Management systems can also manage cloud instances and handle analytical usage. Tips and tricks to navigate the landscape of container solutions: . If you just need to run a database server, describe how to run it as a Docker/Singularity container. If you need several servers running, connected together, set up containers in Docker Compose. If you need to install many things, some of which are not available as packages, make a new Dockerfile recipe to build container image. If you need to use multiple tools in a pipeline, find Conda or container images, compose them in a Computational Workflow. If you need to run tools in a cloud instance, but it has nothing preinstalled, use Conda or containers to ensure installion on cloud VM matches your local machine. If you just need a particular open source tool installed, e.g. ImageMagick, check the document how to install: For Ubuntu 20.04, try apt install imagemagick. ",
      "url": "/pages/bedroesb/rdmkit/data_analysis.html#how-can-you-use-container-environments",
      "relUrl": "/data_analysis.html#how-can-you-use-container-environments"
    },"47": {
      "doc": "Data analysis",
      "title": "Data analysis",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/data_analysis.html",
      "relUrl": "/data_analysis.html"
    },"48": {
      "doc": "Data management plan",
      "title": "What template should you use to draft your Data Management Plan (DMP)?",
      "content": "Description . A number of DMP templates are currently available, originating from different funding agencies or institutions. Moreover, there are ongoing efforts to develop templates for machine-actionable DMPs. Considerations . Each funding agency could require or recommend a specific DMP template. Your institution could require and recommend a DMP template. Template could be presented as list of questions in text format or in a machine-actionable format. Solutions . Consult the documentation of your funding agency or institution, or contact them to figure out if they require or recommend a DMP template. A core DMP template has been provided by Science Europe. From the Horizon Europe Programme Guide and the Horizon Europe Annotated Model Grant Agreement you can read DMP guidelines and access the Horizon Europe DMP template. Consider adopting the DMP Common Standard model from the Research Data Alliance if you want to produce a machine-actionable DMP template. ",
      "url": "/pages/bedroesb/rdmkit/data_management_plan.html#what-template-should-you-use-to-draft-your-data-management-plan-dmp",
      "relUrl": "/data_management_plan.html#what-template-should-you-use-to-draft-your-data-management-plan-dmp"
    },"49": {
      "doc": "Data management plan",
      "title": "What tool should you use to write your DMP?",
      "content": "Description . DMPs can be written offline by using the downloaded template in a text document format. However, a number of web-based DMP tools are currently available that greatly facilitate the process, as they usually contain several DMP templates and provide guidance in interpreting and answering the questions. Some of the tools also allow collaboration on a DMP and track the progress as it is a living document. Considerations . Check what DMP tool is recommended or provided by your funding agency. Check what DMP tool is recommended or provided by your institute. Make sure that the tool you choose includes the DMP template that you need. If you want to produce a machine-actionable DMP, you need to make sure the tool you choose allows exporting the DMP in a machine-actionable format (e.g. JSON) rather than only as a PDF document. Solutions . Use the tool suggested by your funding agency or institution. Choose one of the following online DMP tools (ordered alphabetically): . Data Stewardship Wizard (DSW): publicly available open-source tool to collaboratively compose data management plans through smart and customisable questionnaires with FAIRness evaluation. DMP Canvas Generator: this tool, mainly for researchers in Switzerland, is based on a questionnaire following the structure of the SNSF (Swiss National Science Foundation) instructions for DMP submission. Each Swiss High School can develop a specific template/canvas. DMPonline: tool widely used in Europe and many universities or institutes provide a DMPonline instance to researchers. DMPTool: widely used tool and many universities or institutes provide a DMPTool instance to researchers. EasyDMP: tool provided by the pan-European network EUDAT. Additional tools for creating a DMP are listed in the table below. ",
      "url": "/pages/bedroesb/rdmkit/data_management_plan.html#what-tool-should-you-use-to-write-your-dmp",
      "relUrl": "/data_management_plan.html#what-tool-should-you-use-to-write-your-dmp"
    },"50": {
      "doc": "Data management plan",
      "title": "What should you write in a DMP?",
      "content": "Description . A DMP should address a broad range of data management aspects, regardless of template. It is important to be aware of the current best practices in DMPs before starting one. Considerations . Common topics of a DMP are: . General information about the project. Description of the datasets that will be used and generated. Description of metadata, ontologies and data documentation. Storage solutions, data security and preservation strategy that will be adopted during and after the project. How, when and where data will be shared and published. Costs and resources needed for data management. Ethical and legal issues, such as privacy, intellectual property and licences. Solutions . This website includes best practices and guidelines about the different aspects of research data management that should be covered in a DMP. Core requirements for DMP have been described by Science Europe. Consider the DMP Common Standard from the Research Data Alliance as a reference data model for organising the different topics. ",
      "url": "/pages/bedroesb/rdmkit/data_management_plan.html#what-should-you-write-in-a-dmp",
      "relUrl": "/data_management_plan.html#what-should-you-write-in-a-dmp"
    },"51": {
      "doc": "Data management plan",
      "title": "Data management plan",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/data_management_plan.html",
      "relUrl": "/data_management_plan.html"
    },"52": {
      "doc": "Data organisation",
      "title": "What is the best way to name a file?",
      "content": "Description . Brief and descriptive file names are important in keeping your data files organised. A file name is the principal identifier for a file and a good name gives information what the file contains and helps in sorting them, but only if you have been consistent with the naming. Considerations . Best practice is to develop a file naming convention with elements that are important to your project already when the project starts. When working in collaboration with others, it is important to follow the same file naming convention. Solutions . Tips for naming files . Balance with the amount of elements: too many makes it difficult to understand vs too few makes it general. Order the elements from general to specific. Use meaningful abbreviations. Use underscore (_), hypen (- ) or capitalized letters to separate elements in the name. Don’t use spaces or special characters: ?!&amp; , * % # ; * ( ) @$ ^ ~ ‘ { } [ ] &lt; &gt;. Use date format ISO8601: YYYYMMDD, and time if needed HHMMSS. Include a unique identifier (see: Identifiers) Include a version number if appropriate: minimum two digits (V02) and extend it, if needed for minor corrections (V02-03). The leading zeros, will ensure the files are sorted correctly. Write your file naming convention down and explain abbreviations in your data documentation. If you need to rename a lot files in order to organize your project data and manage the files easier, it is possible use applications e.g. Bulk Rename Utility (Windows, free), Renamer4Mac (Mac). Example elements to include in the name . Date of creation Project number / Experiment / Acronym Type of data (Sample ID, Analysis, Conditions, Modifications etc.) Location / Coordinates Name / Initials of the creator Version number Reserve the last 3-letters for file format (e.g.xls, .rtf, .mov, .tif, .doc) . Examples of good file names . Honeybee project, experiment 2 done in Helsinki, data file created on the second of December 2020 . File name: 20201202_HB_EXP2_HEL_DATA_V03.xls Explanation: Time_ProjectAbbreviation_ExperimentNumber_Location_TypeOfData_VersionNumber . Cropped image of an ant head taken on the third of December 2020 by Meg Megson . File name: 20201203_MM_HEAD_CROPPED_V1.psd Explanation: Time_CreatorData_TypeModification_Version . ",
      "url": "/pages/bedroesb/rdmkit/data_organisation.html#what-is-the-best-way-to-name-a-file",
      "relUrl": "/data_organisation.html#what-is-the-best-way-to-name-a-file"
    },"53": {
      "doc": "Data organisation",
      "title": "How do you manage file versioning?",
      "content": "Description . File versioning is a way to keep track of changes made to files and datasets. While the implementation of a good file naming convention will indicate that different versions exist, this will not explain the difference between two (or more) versions. File versioning will enable transparency on which actions and changes were made when, and it will be easier to backtrack and find something that was present in a previous version, but which has later been deleted or changed. Considerations . Do you need to collaborate on files, perhaps at the same time? Is there a need to be able to backtrack and restore a previous version? Will there be many changes made? . Solutions . Smaller demands of versioning can be managed manually e.g. by keeping a log where the changes for each respective file is documented, version by version. For automatic management of versioning, conflict resolution and back-tracing capabilities, use a proper version control software such as Git, hosted by e.g. GitHub and BitBucket. Use a Cloud Storage service (see Data storage page) that provides automatic file versioning. It can be very handy for spreadsheets, text files and slides. ",
      "url": "/pages/bedroesb/rdmkit/data_organisation.html#how-do-you-manage-file-versioning",
      "relUrl": "/data_organisation.html#how-do-you-manage-file-versioning"
    },"54": {
      "doc": "Data organisation",
      "title": "How do you organise files in a folder structure?",
      "content": "Description . A carefully planned folder structure, with intelligible folder names and an intuitive design, is the foundation for good data organisation. The folder structure gives an overview of which information can be found where, enabling present as well as future stakeholders to understand what files have been produced in the project. Considerations . The decisions on how to organise the files should be made during planning and design of the project, so that the strategy can be implemented already from the start. Consider to consistently apply the same strategy in every project within the research group. Solutions . Folders should: . follow a structure with folders and subfolders that correspond to the project design and workflow have a self-explanatory name that is only as long as is necessary have a unique name – avoid assigning the same name to a folder and a subfolder . The top folder should have a README.txt file describing the folder structure and what files are contained within the folders. This file should also contain explanation of the file naming convention. See also A Quick Guide to Organizing Computational Biology Projects. An example: . project/ code/ code needed to go from input files to final results data/ raw and primary data (never edit!) raw_external/ raw_internal/ meta/ doc/ documentation of the study intermediate/ output files from intermediate analysis steps logs/ logs from the different analysis steps notebooks/ notebooks that document your day-to-day work results/ output from workflows and analyses figures/ reports/ tables/ scratch/ temporary files that can safely be deleted or lost README.txt file and folder description . ",
      "url": "/pages/bedroesb/rdmkit/data_organisation.html#how-do-you-organise-files-in-a-folder-structure",
      "relUrl": "/data_organisation.html#how-do-you-organise-files-in-a-folder-structure"
    },"55": {
      "doc": "Data organisation",
      "title": "Data organisation",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/data_organisation.html",
      "relUrl": "/data_organisation.html"
    },"56": {
      "doc": "Data protection",
      "title": "How do you protect research data under GDPR?",
      "content": "Description . Where scientific research involves the processing of data concerning people in the EU, it is subject to the General Data Protection Regulation (GDPR). The GDPR applies a “special regime” to research, providing derogations from some obligations given appropriate criteria are met and safeguards are in place. The criteria is to follow standards in research method and ethics, as well as to aim societal benefit rather than serving private interests in research. The safeguards are a multitude and include: . data collection with informed consent under ethical oversight and accountability, ensuring lawful processing and exchange of human-subject information, putting in place organisational and technical data protection measures such as encryption and pseudonymisation. The practical impact of the GDPR on research is, then, establishing these safeguards within projects. Considerations . Seek expert help for the interpretation of GDPR legal requirements to practicable measures. Research institutes appoint Data Protection Officers (DPO). Before starting a project you should contact your DPO to be informed of GDPR compliance requirements for your institution. Each EU country has its own national implementation of the GDPR. If your project involves a multi-national consortium, the requirements of all participating countries need to be met and you should inform the project coordinator of any country-specific requirements. Legal offices in research institutes provide model agreements, which cater for various research scenarios and consortia setups. You should inform your local legal office of your project’s setup and identify the necessary agreements to be signed. Assess your project under the GDPR. Determine your GDPR role. Are you a data controller, who determines the purposes and means of the processing, or, are you a data processor, who acts under instructions from the controller? If you are a controller, you need to check whether your processing poses high privacy risks for data subjects, and if so, perform a Data Protection Impact Assessment (DPIA). The GDPR lists certain data e.g. race, ethnicity, health, genetic, biometric data as special category, requiring it’s heightened protection. Your research will be considered high risk processing if it involves special category data or if it includes some specified types of processing. A DPIA is often a pre-requisite for ethics applications. Your DPO or local ethics advisory board can help determine whether your project requires a DPIA. Performing the DPIA while writing the DMP will allow you to reuse information and save time. An outcome of the DPIA will be a listing of risks and corresponding mitigations. Mitigations identify the data protection measures you’ll adopt, both technical organisational. Apply technical and organisational measures for data protection. These include: . Institutional policies and codes of conduct, Staff training, User authentication, authorisation, data level access control, Data privacy measures such as pseudonymisation, anonymisation and encryption, Arrangements that will enable data subjects to exercise their rights. Record your data processing. To meet GDPR’s accountability requirement you should maintain records on the following: . Project stakeholders and their GDPR roles (controller, processor), Purpose of your data processing, Description of data subjects and the data, Description of data recipients, particularly those outside the EU, Logs of data transfers to recipients and the safeguards put in place for transfers, such as data sharing agreements, Time limits for keeping different categories of personal data, Description of organizational and technical data protection measures. Solution . EU General Data Protection Regulation. European Data Protection Supervisor’s “Preliminary opinion on Data Protection and Scientific Research” BBMRI-ERIC’s Ethical Legal Societal Issues (ELSI) Knowledge Base contains a glossary, agreement templates and guidance. Data Information System DAISY is software tool from ELIXIR that allows the record keeping of data processing activities in research projects. DAWID is a software tool from ELIXIR that allows generation of tailor-made data sharing agreements Tryggve ELSI Checklist is a list of Ethical, Legal, and Societal Implications (ELSI) to consider for research projects on human subjects. ",
      "url": "/pages/bedroesb/rdmkit/data_protection.html#how-do-you-protect-research-data-under-gdpr",
      "relUrl": "/data_protection.html#how-do-you-protect-research-data-under-gdpr"
    },"57": {
      "doc": "Data protection",
      "title": "Data protection",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/data_protection.html",
      "relUrl": "/data_protection.html"
    },"58": {
      "doc": "Data publication",
      "title": "Can you really deposit your data in a public repository?",
      "content": "Description . Sometimes it is difficult to determine if publishing data you have at hand is the right thing to do. Some reasons for hesitations might be that you have not used the data in a publication yet and don’t want to be scooped, that the data contains personal information about patients or that the data was collected or produced in a collaboration. Considerations . Publishing data does not necessarily mean open access nor public. Data can be published with closed or restricted access. Data doesn’t have to be published immidiately while you are still working on the project. Data can be made available during the revision of the paper or after the publication of the paper. Make sure to have the rights or permissions to publish the data. Is the data commercially-sensitive? Does the data contain confidential/restricted information? Who controls the data? . Solutions . If ethical, legal or contractual issues apply to your data (e.g. personal or sensitive data, confidential or third-party data, data with copyright, data with potential economic or commercial value, intellectual property or IP data, etc) ask help to the Legal Team, Tech Transfer Office or Data Protection Officer of your institute. Decide what is the right type of access for your data, for instance: . Open access. Registered access or with authentication procedure. Controlled access or via Data Access Committees (DACs). Decide what licence should be applied to your metadata and data. Certain repositories offer solutions for depositing data that need to be under restricted access. This allows for data to be findable even when it can not be published openly. One example is the The European Genome-phenome Archive (EGA) that can be used to deposit potentially identifiable genetic and phenotypic human data. Many repositories provide the option to put an embargo on a deposited dataset. This might be useful if you prefer to use the data in a publication before making it available for others to use. Establish an agreement outlining the controllership of the data and each collaborators’ rights and responsibilities. Even if the data cannot be published, it is good practice to publish the metadata of your datasets. ",
      "url": "/pages/bedroesb/rdmkit/data_publication.html#can-you-really-deposit-your-data-in-a-public-repository",
      "relUrl": "/data_publication.html#can-you-really-deposit-your-data-in-a-public-repository"
    },"59": {
      "doc": "Data publication",
      "title": "Which repository should you use to publish your data?",
      "content": "Description . Once you have completed your experiments and have performed quality control of your data it is good scientific practice to share your data in a public repository. Publishing your data is often required by funders and publishers. The most suitable repository will depend on the data type and your discipline. Considerations . What type of data are you planning to publish? Does the repository need to provide solutions for restricted access for sensitive data? Do you have the rights to publish the data via the repository? How sustainable is the repository, will the data remain public over time? How FAIR is the repository? Does the funding agency or the scientific journal pose specific requirements regarding data sharing? What are the repository’s policies concerning licences and data reuse? . Solutions . Based on the possible ethical, legal and contractual implications of your data, decides: . The right type of access for your data. The licence that should be applied to your metadata and data. Check if/what discipline-specific repositories can apply the necessary access conditions and licences to your (meta)data. Discipline-specific repositories: if a discipline-specific repository, recognised by the community, exists this should be your first choice since discipline-specific repositories often increases the FAIRness of the data. The EMBL-EBI’s data submission wizard will help you choose a suitable repository based on your data type. Lists of discipline-specific, community-recognised repositories can be found in the following links: . ELIXIR Deposition Databases. Scientific Data journal’s recommended repositories. General-purpose and institutional repositories: For other cases, a repository that accepts data of different types and disciplines should be considered. It could be a general-purpose repository or a centralised repository provided by your institution or university. re3data.org or Repository Finder gathers information about existing repositories and allows you to filter them based on access and licence types. re3data.org and FAIRsharing websites gather features of repositories, which you can filter by discipline, data type, taxonomy and many other features. ",
      "url": "/pages/bedroesb/rdmkit/data_publication.html#which-repository-should-you-use-to-publish-your-data",
      "relUrl": "/data_publication.html#which-repository-should-you-use-to-publish-your-data"
    },"60": {
      "doc": "Data publication",
      "title": "How do you prepare your data for publication in data repositories?",
      "content": "Description . Once you have decided where to publish your data, you will have to make your (meta)data ready for repository submission. For this reason it is recommended to become aware of repository’s requirements before start collecting the data. Considerations . What file formats should be used for the data? How is the data uploaded? What metadata do you need to provide? Under which licence should the data be published? . Solutions . Learn the following information about the chosen repositories: . Required metadata schemes. Required ontologies or controlled vocabularies. Accepted file formats for data and metadata. Costs for sharing and storing data. Repositories generally have information about data formats, metadata requirements and how data can be uploaded under a section called “submit”, “submit data”, “for submitters” or something similar. Read this section in detail. To ascertain re-usability data should be released with a clear and accessible data usage licence. We suggest making your data available under licences that permit free reuse of data, e.g. a Creative Commons licence, such as CC0 or CC-BY. Note that every repository can have one default licence for all datasets. For instance, sequence data submitted to for example ENA are implicitly free to reuse by others as specified in the INCD Standards and policies. See the corresponding page for more detailed information about metadata, licences and data transfer. ",
      "url": "/pages/bedroesb/rdmkit/data_publication.html#how-do-you-prepare-your-data-for-publication-in-data-repositories",
      "relUrl": "/data_publication.html#how-do-you-prepare-your-data-for-publication-in-data-repositories"
    },"61": {
      "doc": "Data publication",
      "title": "Data publication",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/data_publication.html",
      "relUrl": "/data_publication.html"
    },"62": {
      "doc": "Data quality",
      "title": "How do you ensure the quality of research data?",
      "content": "Description . Data quality is a term that can be understood in many ways. In enterprise context, it often refers to master data management as defined by the ISO 8000 standards. In science, the quality of data is closely linked to the suitability of the data for (re)use for a particular purpose and it is a key attribute of research data. Data quality affects the reliability of research results and it is a key factor increasing the reusability of data for secondary research. Data quality control can take place at any stage during the research data lifecycle. That said, you should ensure that the necessary procedures are defined during data management planning. Considerations . What is your data collection mechanism? Quality control is most typically performed during data collection. The elements of data collection in your research will determine the quality measures you can take. Examples of such measures are: . setup data management working group (DMWG) that includes people who generate data, analyse data and data managers; for data collection: DMWG to plan and define data dictionary (including validation rules) before collecting data; for metadata collection: DMWG to plan and define metadata data templates; use electronic data capture systems; automated quality monitoring through tools, pipelines, dashboards; training of study participants and researchers, surveyors or other staff involved; adopting standards; instrument calibrations; repeated samples; post collection data curation; data peer-review. Are there standards or established working practices for quality in your field of study? Certain areas such as clinical studies, or those involving Next Generation Sequencing have commonly working methods to ensure data quality. There are many frameworks proposed in the literature to define and evaluate overall data quality, such as: . the four data quality dimensions (Accuracy, Relevancy, Representation, Accessibility) by Wang; the five C’s of Sherman (Clean, Consistent, Conformed, Current and Comprehensive), and the three categories from Kahn (Conformance, Completeness and Plausibility), for electronic health data. Kahn also proposes two different modes to evaluate these components: . verification (focusing on the intrinsic consistency, such as adherence to a format or specified value range); validation (focusing on the alignment of values with respect to external benchmarks). For health data, a nice example of working out what data quality means can be found in the OHDSI community. The context in this case is observational healthcare data represented in the OMOP Common Data Model. Solutions . Electronic data capturing system: REDCap allows you to design electronic data capture forms and allows you to monitor the quality of data collected via those forms. An example of data dictionary illustrating the elements and factors that should be defined for the variable needed by data collection. The World Bank provides quality assurance guidance for survey design and execution. The U.S. National Institute’s of Health’s provides introductory training material on data quality. Bio.tools’ listing for computational tools and pipelines for data quality control in life sciences. Data integration tools that include pre-defined building blocks to monitor and check data quality, such as Pentaho Community Edition (CE), Talend Open Studio. Data curation tools such as OpenRefine that help you to identify quality issues, correct (curate) them, carry out transformations in the collected data with easy-to-use graphic interface and visualisation. It also documents all the steps during the curation for reproducibility and backtracking. For health data. The Book of OHDSI has several chapters on methods for assessing the data quality of observational health datasets, split out by data quality, clinical validity, software validity and method validity. The OHDSI DataQualityDashboard is a software framework for asesssing the quality and suitability of routinely generated healthcare data that is represented in the OMOP Common Data Model. Frameworks proposed in the literature, to define and evaluate overall data quality, could be used to create computational representations of the data quality of a dataset, such as the one visualized in the OHDSI Data Quality Dashboard, which leverages the Kahn framework referenced above (adapted from original thehyve.nl blogpost). ",
      "url": "/pages/bedroesb/rdmkit/data_quality.html#how-do-you-ensure-the-quality-of-research-data",
      "relUrl": "/data_quality.html#how-do-you-ensure-the-quality-of-research-data"
    },"63": {
      "doc": "Data quality",
      "title": "Data quality",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/data_quality.html",
      "relUrl": "/data_quality.html"
    },"64": {
      "doc": "Data steward infrastructure",
      "title": "Description",
      "content": "As a infrastructure data steward, I focus on liaising with the people involved in the IT infrastructure, technicians, application managers and other service providers inside and outside my research institute. My task is to translate the requirements of policies and science into suitable IT solutions and tools as well as provide advice. I implement IT infrastructure solutions, give access to data and software, and I may also perform hands-on work in a research project. ",
      "url": "/pages/bedroesb/rdmkit/data_steward_infrastructure.html#description",
      "relUrl": "/data_steward_infrastructure.html#description"
    },"65": {
      "doc": "Data steward infrastructure",
      "title": "Focus",
      "content": ". Identify the requirements of an adequate data infrastructure and tool landscape that fits with research data management (RDM) policies Ensure the compliance of the data infrastructure and tool landscape with codes of conduct and regulations Align the data infrastructure and tool landscape to the FAIR (Findable, Accessible, Interoperable, Reusable) data principles and the principles of Open Science, and facilitate and support FAIR data Identify the requirements of and provide access to data infrastructure for RDM for researchers Make an inventory for data infrastructure and tools that fit with the researchers RDM needs Liaison and align the data infrastructure and tools management in and outside the organisation Facilitate the availability of local data-infrastructure and tools for FAIR and long term archiving of data . ",
      "url": "/pages/bedroesb/rdmkit/data_steward_infrastructure.html#focus",
      "relUrl": "/data_steward_infrastructure.html#focus"
    },"66": {
      "doc": "Data steward infrastructure",
      "title": "Learning path",
      "content": "Institutes across Europe have started hiring professional data stewards. A infrastructure oriented data steward is expected to be competent in the following areas: . Advise and assist researchers on short and long term actions for data infrastructure and tools Continuously monitor data infrastructure and tools available inside and outside the institute, in close collaboration with the responsible (IT) department Assess technical knowledge status of researchers and relevant stakeholders (including other data stewards), and if needed give training in technical RDM skills Communicate with researchers, technical staff and support staff about data infrastructure and tools Knowledge of encryption of data, data protection and data security protocols Translate ethical and technical requirements for data infrastructure and tools to technological measures, while understanding the research requirements and limitations Translate the FAIR data principles into data infrastructure and tool requirements Advise researchers and stakeholders (including other data stewards) on archiving solutions, including (meta)data standards . ",
      "url": "/pages/bedroesb/rdmkit/data_steward_infrastructure.html#learning-path",
      "relUrl": "/data_steward_infrastructure.html#learning-path"
    },"67": {
      "doc": "Data steward infrastructure",
      "title": "Data steward infrastructure",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/data_steward_infrastructure.html",
      "relUrl": "/data_steward_infrastructure.html"
    },"68": {
      "doc": "Data steward policy",
      "title": "Description",
      "content": "As a policy data steward, I focus on policy development and the implementation of research data management practices in my organisation. I work in close collaboration with directors, policy makers and funders, and I help establish suitable and sufficient data stewardship services and infrastructure. It is my job to coordinate and align efforts on the quality, security and management of my organisation’s data, thus effectively supporting the research process. I have a good knowledge of local, national and international procedures and regulations, such as my institution’s research code, national codes of conduct or the EU privacy legislation. I translate these general concepts into practical guidelines for my organisation, and coordinate their implementation and monitoring. ",
      "url": "/pages/bedroesb/rdmkit/data_steward_policy.html#description",
      "relUrl": "/data_steward_policy.html#description"
    },"69": {
      "doc": "Data steward policy",
      "title": "Focus",
      "content": ". Advise, develop and monitor a research data management (RDM) policy institutionally or nationally Ensure compliance of the RDM policy to codes of conduct and regulations, including ethical and legal compliance Align the RDM policy to the FAIR (Findable, Accessible, Interoperable, Reusable) data principles and the principles of Open Science Organise RDM support into a set of services Identify the requirements of adequate data-infrastructure for RDM to comply with the institute’s RDM policy and alignment to (inter)national data-infrastructure and tools Determine the adequate level of knowledge and skills on RDM Maintain a network of aligned RDM expertise inside and outside the organisation Identify the requirements of adequate support and data infrastructure for FAIR and long-term archiving of data . ",
      "url": "/pages/bedroesb/rdmkit/data_steward_policy.html#focus",
      "relUrl": "/data_steward_policy.html#focus"
    },"70": {
      "doc": "Data steward policy",
      "title": "Learning path",
      "content": "Institutes across Europe have started hiring professional data stewards. A policy oriented data steward is expected to be competent in the following areas: . Develop, implement, monitor and evaluate policies regarding research data that are endorsed by researchers and aligned with internal and external stakeholders and effectuate change management Communicate about the FAIR data and Open Science principles to researchers, support staff and relevant stakeholders, thus creating awareness within institutes and organisations Give advice on RDM and formats for data management plans (DMP) to a broad audience Communicate about the RDM policy, explain implications and create awareness Translate RDM policy and legislation and codes of conduct concerning research data into practical implications and guidelines for that researchers can understand Formulate requirements for RDM support (staff and services) Assess and analyse needs from researchers regarding support on RDM for researchers and define new requirements Assess RDM knowledge and skills and identify gaps among researchers and relevant stakeholders . ",
      "url": "/pages/bedroesb/rdmkit/data_steward_policy.html#learning-path",
      "relUrl": "/data_steward_policy.html#learning-path"
    },"71": {
      "doc": "Data steward policy",
      "title": "Data steward policy",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/data_steward_policy.html",
      "relUrl": "/data_steward_policy.html"
    },"72": {
      "doc": "Data steward research",
      "title": "Description",
      "content": "As a research data steward, I support and work in close collaboration with the main data producers and users in academia: the researchers, ranging from undergraduate students to full professors. I advise researchers, make sure data is handled in a manner compliant with the institute’s policy and may also perform hands-on work in a project. My work focuses on implementing the institute’s data guidelines and translating them into domain and project specific procedures, for example by managing a database or reviewing data management plans. My responsibilities and tasks focus on translating the researcher needs on data into infrastructural and service requirements. ",
      "url": "/pages/bedroesb/rdmkit/data_steward_research.html#description",
      "relUrl": "/data_steward_research.html#description"
    },"73": {
      "doc": "Data steward research",
      "title": "Focus",
      "content": ". Develop and implement data management plans for projects and data collections and align Data Managements Plans (DMP) with the FAIR (Findable, Accessible, Interoperable, Reusable) data principles and the principles of Open Science Advise projects and data collections on compliance with codes of conduct, regulations and field specific legal and ethical standards Provide adequate research data management (RDM) support to researchers. This involves, for example, supporting researchers in improving the reproducibility of their computational analyses or directing researchers to appropriate data management and archival solutions Monitor a project’s needs regarding data-infrastructure and tools for RDM Determine the adequate level of knowledge and skills of researchers on RDM Identify the requirements of adequate support and data infrastructure for FAIR and long-term archiving of data of a project . ",
      "url": "/pages/bedroesb/rdmkit/data_steward_research.html#focus",
      "relUrl": "/data_steward_research.html#focus"
    },"74": {
      "doc": "Data steward research",
      "title": "Learning path",
      "content": "Institutes across Europe have started hiring professional data stewards. A research oriented data steward is expected to be competent in the following areas: . Create awareness and communicate about RDM and the FAIR data principles and translate RDM policies into guidelines for researchers Transform discipline specific research data into FAIR data with help of available services and tools Advise and assist researchers on short and long term actions for RDM Assess RDM knowledge and skills, identify gaps among researchers and take action when needed Understand the purpose and use of a DMP in a project and have the skills to utilise the available tools and templates to produce a DMP Assist researchers in developing a DMP, review DMPs, and support researchers in putting DMPs into action Liaise with the surrounding environment (department, project, national stakeholders and international network) and continuously follow the field to gain knowledge of relevant facilities, tools and emerging standards available for RDM . ",
      "url": "/pages/bedroesb/rdmkit/data_steward_research.html#learning-path",
      "relUrl": "/data_steward_research.html#learning-path"
    },"75": {
      "doc": "Data steward research",
      "title": "Data steward research",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/data_steward_research.html",
      "relUrl": "/data_steward_research.html"
    },"76": {
      "doc": "Data transfer",
      "title": "How do you transfer large data files?",
      "content": "Description . Often, research in Life Sciences generates massive amounts of digital data, such as output files of ‘omics’ techniques (genomics, transcriptomics, metabolomics, proteomics, etc). Large data files cannot be sent by email because they exceed the file size limit of most common email servers. So, how can large data files be transferred from a local computer to a distant one? . Considerations . There are many aspects to consider when dealing with data transfer. The size or volume of the data and the capacity or bandwidth of the network that links your local computer with the distant computer are crucial aspects. Data size and bandwidth are tightly linked since transferring large volumes of data on a low bandwidth network will be so time consuming that it could be simpler to send the data on a hard drive through carrier services. Data security. If you intend to transfer sensitive data to another location, you have to check the regulation and security measures on the remote site. You can interact with the IT departments at both locations in order to establish your strategy. Do not forget to check the Human Data pages of the RDMkit. If you have the technical skills and knowledge, consider using appropriate File Transfer Protocols. Consider using Cloud Storage Services (see Data Storage page), that provide data sharing solutions, or specialised data transfer services available in your institute or country. Consider pros and cons of transferring data by shipping hard disks through carrier services (time, costs, security). This is not a recommended method, unless good internet connection is not available. Since data transfer involves so many technical aspects, it is a good idea to interact with your technical/IT team in order to avoid any problem if you want to transfer large amounts of data. Solutions . Try to optimise and ease your data transfer by archiving your data in a single file. This can be done with two tools available on most systems. tar (tape archive) will create an archive, a single file containing several files or directories. gzip: since tar does not compress the archive created, a compression tool such as gzip is often used to reduce the size of the archive. Ask the IT team of your institution or organisation about available services for data transfer. Usually, for small data volume or limited number of files universities and professional organisations can provide: . Secure server- or cloud-based applications where you should store work-related data files, synchronize files from different computers and share files by sending a link for access or download. This solution is ideal in case of a small number of files, since files need to be downloaded one by one and this can be inconvenient. Examples of these kinds of applications are NextCloud, Box, OwnCloud (see Data storage page). Access to Office 365 (Software as a Service, or SaaS) which include cloud storage on OneDrive and SharePoint for collaborations and files sharing. You can “transfer” your data with these services by generating and sending a link for access or download of specific files. Usually, universities and institutions strongly discourage the use of personal accounts on Google Drive, Amazon Drive, Dropbox and similar, to share and transfer work related data, and especially sensitive or personal data. Moreover, it is not allowed to store human data in clouds which are not hosted in the EU. Institutions and professional organisations could also make use of Infrastructure as a Service (IaaS), such as Microsoft Windows Azure, Amazon Web Services (Amazon Simple Storage Service or S3), Oracle Cloud Infrastructure or Google Cloud Platform. A useful comparison of cloud-computing software and providers is on Wikipedia. Cloud-computing infrastructures, services and platforms offer a variety of file hosting services; a comparison of file hosting services is available on Wikipedia. If you are considering transferring data from or to cloud-based services (Microsoft Azure or Amazon S3) by shipping hard disks through carrier services, it is useful to know that services such as Amazon Snowball and Azure Data Box Disk will help you with the shipping of hard disks or appliances through carrier services. Countries could provide national file sender services (browser based or other) which could be useful for one time transfer of data files, limited in number and volume (for instance, up to 100 GB or 250 GB), from person to person. Importantly, an academic account is usually needed to use these kinds of services, so ask the IT team in your institute for more information. If you have the technical skills and the knowledge, you can use the most common data transfer protocols. These protocols are useful for data volume bigger than 50GB or for hundreds of data files. Applications suitable for small to mid size data available on any operating system and that can be used either through command-line (directly or with tools like cURL) or through a graphical interface, are: . FTP (File Transfer Protocol) will transfer files between a client and an FTP server, which will require an account in order to transfer the files. Be sure to use a secure version of this protocol, such as FTPS or SFTP (SSH File Transfer Protocol). HTTP (HyperText Transfer Protocol) Rsync (remote synchronization): can be used to transfer files between two computers and to keep the files synchronized between these two computers. SCP (secure copy): SCP will securely transfer files between a client and a server. It will require an account on the server and can use SSH key based authentication. For massive amounts of data, additional protocols have been developed, parallelizing the flow of data. These transfer solutions require commercial licences for your site and as such they are available mostly on large computational centres. Aspera Fasp GridFTP and Globus . Data Transfer Protocols with a graphical user interface are: . FileZilla WinSCP . When using data transfer protocol, make sure to check the transfer. During the transfer some data might become corrupted, thus it is important to check if the files you transfered have conserved their integrity. This can be done with hash algorithms. A checkshum file is calculated for each file before transfer and compared to a checksum calculated on the transferred files. If the checksums are the same, the files are not corrupted. md5 SHA . ",
      "url": "/pages/bedroesb/rdmkit/data_transfer.html#how-do-you-transfer-large-data-files",
      "relUrl": "/data_transfer.html#how-do-you-transfer-large-data-files"
    },"77": {
      "doc": "Data transfer",
      "title": "Data transfer",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/data_transfer.html",
      "relUrl": "/data_transfer.html"
    },"78": {
      "doc": "Editorial board",
      "title": "Meet the editorial board members",
      "content": "Bert Droesbeke . VIB-UGent / ELIXIR-BE . Carole Goble . The University of Manchester . Daniel Faria . University of Lisbon / ELIXIR-PT . Federico Bianchini . Flora D'Anna . VIB-UGent / ELIXIR-BE . Frederik Coppens . VIB-UGent / ELIXIR-BE . Korbinian Bösl . University of Bergen / ELIXIR-NO . Laura Portell Silva . Barcelona Supercomputing Center / ELIXIR-ES . Martin Cook . ELIXIR Hub . Munazah Andrabi . Niclas Jareborg . NBIS / ELIXIR-SE . Pinar Alper . LCSB / ELIXIR-LU . Rob Hooft . Ulrike Wittig . HITS / ELIXIR-DE . Previous Next ",
      "url": "/pages/bedroesb/rdmkit/editorial_board.html#meet-the-editorial-board-members",
      "relUrl": "/editorial_board.html#meet-the-editorial-board-members"
    },"79": {
      "doc": "Editorial board",
      "title": "Join as editorial board member",
      "content": "Being on the editorial board of RDMkit entails shaping how RDM best practices, written by experts, are communicated to the readers who are interested in applying those best practices in their everyday work. As an editor you contribute to the vision of how this information is presented to the life science community at large. Advantages of being an RDMkit editor . You will be up-to-date with numerous RDM topics while reviewing proposed content. This work helps to increase your visibility as expert in RDM. You will be able to suggest thematic gaps that should be added to the RDMkit, of which you can take ownership as part of the team. You will expand your professional network. You will make an impact in improving RDM in Life Sciences by developing effective ways to communicate best practices to stakeholders. Responsibilities . Careful consideration and examination of topics proposed by contributors, to make sure it is relevant for the RDMkit. Work together with contributors how and where their specific contribution fits best in RDMkit. Provide timely feedback for improving the quality of the content, according to the RDMkit style guide. Formatting content according to RDMkit templates. Join the biweekly editorial meeting. Manage a Github repository. Call for joining us . RDMkit is receiving lot of attention from policy makers, funders and RDM infrastructure providers, therefore editorial board members should be professionals and experts in RDM, with an interest for effective digital communication, who can help in keeping high quality content. We want to keep the editorial board manageable in numbers, therefore we are looking for complementary expertise to the current members. Requirements . The applicant must have experience in RDM. The applicant must be interested in effective digital communication of best practices and guidelines. The applicant should be willing to spend some time on evaluation of content. The applicant should be open to discuss topics with other editorial board members and to accept feedback. The applicant should have basic knowledge of Github. Application . To become part of the editorial team, please provide: . A short motivation letter (a couple of paragraphs) describing why you would like to join the RDMkit editorial board. A summary of your experience in RDM. List of the RDMkit sections that you are most interested in (or feel are missing). Please, send your application to rdm-editors@elixir-europe.org. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board.html#join-as-editorial-board-member",
      "relUrl": "/editorial_board.html#join-as-editorial-board-member"
    },"80": {
      "doc": "Editorial board",
      "title": "Editorial board",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/editorial_board.html",
      "relUrl": "/editorial_board.html"
    },"81": {
      "doc": "Editorial board guide",
      "title": "All you need to know about this GitHub repository",
      "content": "General rules . As an editor, try to work as much as possible on a different branch than the master branch. This can be on the elixir-europe rdm-toolkit repo or your own fork. Open a pull request if you want to merge your changes with the master branch. This way it is possible for other editors to give feedback on your changes. Typos or other small fixes can of course be done immediately on the master branch. The google doc way of contributing . This process is sketched below. Overview of the file structure in GitHub . The content of the website is built up using markdown files found in the pages directory. These markdown files are divided over subdirectories (your_role, your_domain, your_tasks…) for sorting reasons only. The metadata/frontmatter of the markdown file . In order to render the markdown file to the website, it needs a specific frontmatter/metadata section in the top part of the file. This latter can look like: . --- title: Title of the page --- . Mandatory metadata/frontmatter: . title: Specify here the page title. This will be the H1 title (replacing the top level title using the # in markdown ) . Optional metadata/frontmatter: . summary: By using this attribute it is possible to specify a summary which will be displayed under the page title. description: Short sentence about the page starting with a lowercase. This sentence is visualized when pages are automatically listed as Related page. contributors: List here all the contributors that helped in establishing the page, preferibly with their full name. Make sure that the person names that are listed can be found in the CONTRIBUTORS.yaml file in the _data directory if you want to link the GitHub ID and other contact information. Multiple contributors will be put in a list like this: [example1, example2]. search: By setting this field to “exclude”, the page will not end up in the search results of the search bar. Default: true. hide_sidebar: When true, the sidebar will be hidden. Default: false. custom-editme: This attribute can be used to specify an alternative file/link when clicked on the edit-me button. keywords: List here all the keywords that can be used to find the page using the search box in the right-upper corner of the page. Multiple keywords are put in a list like this: [example1, example2]. sidebar: Specify here an alternative sidebar. Default: main. toc: When set to false, the table of contents at the beginning of the page will not be generated. affiliations: List here all affiliations that made this page possible. This is especially used for tool assembly pages. Countries use the ISO 3166-1-alpha-2 notation, other affiliations must be present in the affiliations.yaml in the _data directory in order to work. audience: List here the audiences of this page. This is especially used for tool assembly pages. Countries use the ISO 3166-1-alpha-2 notation, other affiliations must be present in the affiliations.yaml in the _data directory in order to work. “ALL” can be used if the tools and infrastructure this page talks about are accessible for everyone in the world. page_id: Unique identifier of a page. It is usually a shortened version of the page name or title, with small letters and spaces, or an acronym, with capital and small letters. Used to list Related pages. related_pages: List here the page_id of RDMkit pages that you want to display as Related pages, grouped by section (Your tasks, Your domain, Tool assembly). If you want pages from the specific section (Your tasks, Your domain, Tool assembly) to be shown here as Related pages, list their page_id. If you want to list multiple related pages, make sure to put them in a list like this: [page_id1, page_id2]. The specific sections allowed in each page are specified in each page template. Please, do not add extra sections in the metadata of the page. related_pages: your_tasks: [page_id1, page_id2] your_domain: [page_id1, page_id2] tool_assembly: [page_id1, page_id2] . training: List here training material relevant for the page. We recommend to add your training material in TeSS. However, you can also list here training material that is not yet present in TeSS. Each training item will be automatically added as an entry to the table in the All training resources page. training: - name: Training in TeSS registry: TeSS registry_url: https://tess.elixir-europe.org url: https://tess.elixir-europe.org/search?q=data%20analysis - name: Training in TeSS registry: TeSS registry_url: https://tess.elixir-europe.org url: https://tess.elixir-europe.org/search?q=data%20analysis . faircookbook: List here all the links towards FAIR Cookbook recipes. faircookbook: - name: Data licenses url: https://fairplus.github.io/the-fair-cookbook/content/recipes/reusability/ATI_licensing_data.html . datatable: Use this attribute to activate pagination, sorting and searching in tables. Markdown file naming . Markdown files should be named without capitals and without spaces (replace them with underscores). Make sure that the markdown file has a unique name. If the markdown file is named example.md, the page will be found at https://rdmkit.elixir-europe.org/example. By default a page will not show up in the sidebar. In order to do so you will have to add the link towards the page to the .yaml file in the _data/sidebars directory or link towards it from another page. More info about this can be found on the find your page back section. GitHub checks . With each PR or merge to the master, some checks are done using GitHub actions. One of them checks wether the website builds correctly. The other checks for changes in the tool/resource Excel table. When each of them fails, the PR will not be able to be merged. Click on the red dot/failed check to understand better what caused the fail. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#all-you-need-to-know-about-this-github-repository",
      "relUrl": "/editorial_board_guide.html#all-you-need-to-know-about-this-github-repository"
    },"82": {
      "doc": "Editorial board guide",
      "title": "Label, discuss and assign issues",
      "content": ". Check open issues regularly or enable notifications by clicking the “WATCH” icon in the top-right side of the GitHub repository. Assign labels to issues. Discuss who is going to be responsible for each issue with other editors and reviewers (via issue comments or other communication channels). Assign at least one editor/reviewer to the issue, who will discuss the possible content with the contributor. When a Pull Request (PR) or a draft PR related to an issue is created, link the PR to the issue. More information about these topics can be found in the GitHub documentation: . commenting on PRs start a review . ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#label-discuss-and-assign-issues",
      "relUrl": "/editorial_board_guide.html#label-discuss-and-assign-issues"
    },"83": {
      "doc": "Editorial board guide",
      "title": "Review pull requests",
      "content": "If contributors make a pull request to make changes, by default the editors that are responsible for files that will changed by the PR will be assigned and notified. All PR should be assigned to one of the editors. Before merging a PR, pages’ tags and keywords, and tools and resources’ tags should be checked and assigned according to the established tagging system. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#review-pull-requests",
      "relUrl": "/editorial_board_guide.html#review-pull-requests"
    },"84": {
      "doc": "Editorial board guide",
      "title": "Link a pull request to an issue",
      "content": "When you make a pull request resolving an issue, it is possible to link this pull request to that specific issue. This can be easily done by writing in the conversation of the PR: closes #number_of_issue, or fixes #number_of_issue or even resolves #number_of_issue. This is definitely applicable when authors first open an issue announcing a change or requesting a new page, followed up by the pull request. For more information about this topic please visit the GitHub documentation page. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#link-a-pull-request-to-an-issue",
      "relUrl": "/editorial_board_guide.html#link-a-pull-request-to-an-issue"
    },"85": {
      "doc": "Editorial board guide",
      "title": "Adding a new event",
      "content": "Add an event to the landing page by editing the events.yml in the _data directory in this repository. Use following attributes to define an event: . - name: Contentathon startDate: 2021-06-23 startTime: '9:00' endDate: 2021-06-24 endTime: 13:30 CET description: We would like to invite you to highlight your set of data management tools as a tool assembly in the RDMkit and describe how to use it, so others can do the same. (two half days) location: Online . Only name and startDate are mandatory attributes. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#adding-a-new-event",
      "relUrl": "/editorial_board_guide.html#adding-a-new-event"
    },"86": {
      "doc": "Editorial board guide",
      "title": "Adding a news item",
      "content": "Add a news item to the landing page by editing the news.yml in the _data directory in this repository. Use following attributes to define a news item: . - name: News title date: 2021-06-23 description: A short description . All attributes are mandatory. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#adding-a-news-item",
      "relUrl": "/editorial_board_guide.html#adding-a-news-item"
    },"87": {
      "doc": "Editorial board guide",
      "title": "Create a new page",
      "content": "Simple way: using the GitHub interface . To generate a new page it is sufficient to simply copy the TEMPLATE file in the subdirectory and rename it. To copy a template you have to: . Go to the TEMPLATE_ file of choice in the GitHub repo, every section has its own TEMPLATE file. For example the TEMPLATE_your_tasks.md file. Click “Raw” on the GitHub page to open the file ‘as is’ Select and copy all the content. Go back to the main section were you want to make the new page, in our example this will be in /pages/your_tasks. Click on Add file on the right followed up by Create new file. Paste the copied content from the template. Name the file by choosing a unique self explaining short name without capitals and without spaces (replace them with underscores). Check the frontmatter/metadata of the markdown page: . delete search_exclude: true attribute. add the author names to the contributors list. optional: change the title into an appropriate one. Describe shortly which changes you made in the description of your commit below the page. Commit to the master branch by clicking Commit new file. If the markdown file is named example.md the page will be rendered at https://rdmkit.elixir-europe.org/example. This link can be provided to the contributor through the issue. Note: It is not a problem to immediately duplicate pages in the master branch, but be aware that new content always needs to be pushed to another branch which will give you the option to open a pull request. Advanced: working on your own feature branch and pushing local changes . Just like with every change you want to make to this repo, it is possible to do this through Git by working on a local copy. For more information on how to do this, please read our working with Git page. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#create-a-new-page",
      "relUrl": "/editorial_board_guide.html#create-a-new-page"
    },"88": {
      "doc": "Editorial board guide",
      "title": "Find your newly added page back on the website",
      "content": "By default your page will not be linked in the sidebar on the website, or on the landing page, but it will exist as an orphan at https://rdmkit.elixir-europe.org/markdown_file_name. In order to prevent that people will not find the page back it is better to link towards it in the sidebar or get linked within an existing page. Linking pages in the sidebar and frontpage . Make sure all pages are accessible from the navigation sidebar. Please, avoid generating sub-pages that are not directly accessible from the navigation sidebar. This website supports multiple sidebars, the one in the main sections of the website is for example different from the one in the contribute section. Both of them are defined by .yaml files in the _data/sidebars directory. Changing these yaml file will immediately impact the sidebars and the frontpage of the website. The sidebar supports multiple levels and each level in the hierarchy can contain a URL to a page within this website or an external URL. The attributes that define the structure are: . title: This is the text that will show up in the sidebar. url: The URL to the internal page you want to link to. This is mostly in the form of: /markdown_file_name.html. external_url: Use this instead of URL if you want to link to an external page. subitems: to define a sublevel. - title: Level_1_title url: level_1_url subitems: - title: Level_2_title url: level_2_url . Tip: Copy around existing parts in the yaml file to add pages to the same level Link page within existing page . Avoid manual linking to internal pages. If necessary, you can manually link to the pages like this: . If the markdown page is named example_1.md, you can link towards it using: . [Example 1](example_1) . Important: If you change the file name, you’ll have to update all of your links. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#find-your-newly-added-page-back-on-the-website",
      "relUrl": "/editorial_board_guide.html#find-your-newly-added-page-back-on-the-website"
    },"89": {
      "doc": "Editorial board guide",
      "title": "Adding extra info to the contributors",
      "content": "Do you want that the GitHub picture of a contributor is shown next to their name? Or maybe you want that the name is clickable and links towards the GitHub page of that person? To enable this please add the name and the necessary metadata to the CONTRIBUTORS.yaml file in the _data directory like this: . Bert Droesbeke: git: bedroesb email: bedro@psb.ugent.be orcid: 0000-0003-0522-5674 role: editor affiliation: VIB-UGent . Important: Make sure that the name in the yaml file is identically the same as the one used in the metadata of the page. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#adding-extra-info-to-the-contributors",
      "relUrl": "/editorial_board_guide.html#adding-extra-info-to-the-contributors"
    },"90": {
      "doc": "Editorial board guide",
      "title": "Adding an institution, infrastructure, project or funder",
      "content": "Institutions, projects, funders and infrastructures are listed in the affiliations.yml file. The info in this file is used on the support page in the about section, but also for the affiliations and audience in tool assembly pages. Make sure you make use of the same name in those assembly pages. The yaml file has following syntax: . - name: VIB image_url: /images/institutions/VIB-PSB.svg pid: https://ror.org/03xrhmk39 rdmkit_about: true type: institution url: https://www.psb.ugent.be/ . name: name image_url: relative url towards the image pid: url including the unique identifier towards the page of the association on ROR rdmkit_about: true or false, when true this association will be shown in the about section type: can be any of these values: institution, funder, infrastructure or project url: url towards the homepage of this association . The logos can be added to the /images/institutions, /images/projects, /images/infrastructures and /images/funders directory. Important: Upload vector images (.svg filetype) of the institute logo for better quality, scaleability and file size, if possible. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#adding-an-institution-infrastructure-project-or-funder",
      "relUrl": "/editorial_board_guide.html#adding-an-institution-infrastructure-project-or-funder"
    },"91": {
      "doc": "Editorial board guide",
      "title": "Related pages",
      "content": "Add “Related pages” to a page . RDMkit pages from the sections Your tasks, Your domain and Tool assembly can be displayed as “Related RDMkit pages” in a page, grouped by section. Only pages from specific sections are allowed in each page (see image below), as pre-defined in the metadata of each template page. Please, do not add extra sections in the metadata of the page. An overview of all RDMkit pages (belonging to the sections listed above) and their page_id can be found in the Website overview page. related_pages: your_tasks: [page_id1, page_id2] your_domain: [page_id1, page_id2] tool_assembly: [page_id1, page_id2] . Page ID . To find out what the page_id of an RDMkit page is, please check its metadata attribute page_id at the top of the markdown file or the Website overview page. ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html#related-pages",
      "relUrl": "/editorial_board_guide.html#related-pages"
    },"92": {
      "doc": "Editorial board guide",
      "title": "Editorial board guide",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/editorial_board_guide.html",
      "relUrl": "/editorial_board_guide.html"
    },"93": {
      "doc": "Epitranscriptome data",
      "title": "Introduction",
      "content": "Epitranscriptome modifications are emerging as important factors to fine tune gene expression and regulation in a variety of organisms and experimental conditions. To date more than 100 distinct chemical modifications to RNA have been characterized, including transient (i.e. m6A or m1A or m5C) and not-transient (A-to-I or C-to-U RNA editing) nucleotide variants. In the last few years, several methods based on deep sequencing technologies (RNAseq or MeRIPseq or miCLIP or direct RNA sequencing) have been optimized to profile the epitranscriptome in humans and various model organisms. The detection of RNA modifications requires ad hoc bioinformatics pipelines as well as the use of computing intensive tools to handle the huge amount of available deep transcriptome sequencing experiments. A fruitful organization of data and computational workflows is therefore important for a FAIRification of epitranscriptome domain. ",
      "url": "/pages/bedroesb/rdmkit/epitranscriptome_data.html#introduction",
      "relUrl": "/epitranscriptome_data.html#introduction"
    },"94": {
      "doc": "Epitranscriptome data",
      "title": "Collection of research data",
      "content": "Description . Several high-throughput experimental approaches have been developed for profiling the transcriptome-wide distribution of RNA modifications. While not-transient changes (RNA editing) can be detected using standard RNAseq data, RNA modifications like m6A or m1A or m5C can be identified by a variety of antibody based methods such as MeRIPseq or miCLIP and by means of the RNA directed sequencing. To comply with the FAIR principles, it is important to define the sequencing protocol adopted to produce data as well as related metadata. Considerations . Are you planning to profile transient or not-transient RNA modifications? Is the method based on the RNA directed sequencing? Do you collect your own data or reuse them from public databases? . Solutions . Define the sequencing protocol depending on the target RNA modification (transient or not-transient). In case of using data from public databases, carefully look at the method used to generate them. Prefer profiling methods allowing the detection of RNA modifications at single nucleotide level. Epitranscriptome data is generally reused from literature or public and established databases, such as REDIportal. All data must have an identifier depending on the original database that comes from. The source database is used also to retrieve metadata. ",
      "url": "/pages/bedroesb/rdmkit/epitranscriptome_data.html#collection-of-research-data",
      "relUrl": "/epitranscriptome_data.html#collection-of-research-data"
    },"95": {
      "doc": "Epitranscriptome data",
      "title": "Processing and analysis of epitranscriptome data",
      "content": "Description . Epitranscriptome is a novel field and in rapid expansion. Since a variety of transcriptome-wide sequencing methods exists, several computational tools have been developed. It is important here to decide which pipeline to adopt. Considerations . What are the compute resources you need to analyse your data? Do you have data storage problems due to the size of the data? Are you using RNAseq data? Are you profiling or transient not-transient RNA modifications? . Solutions . The current pipeline for RNA editing (REDItools) requires the use of time intensive computational resources to browse position by position all genomic sites covered by RNAseq reads. In order to overcome that, a novel tool (REDItools2) able to employ HPC resources and reduce the computing time has been developed. However, for transient modifications identified by direct RNA sequencing compute intensive tools are still required. The computational speed up could be obtained by using GPU graphical cards. In general, for standard RNAseq experiments, each sample requires 8-10 CPUs and at least 8-10 GB of RAM memory. Direct RNA sequencing, instead, requires 8-10 CPUs, at least 1 GPU and 8-10 GB of RAM memory. Once a pipeline has been adopted, it should be used for all samples. Data storage is a big issue and not all intermediate files produced during the analyses can be maintained. However, since original data are easily and always available from public sources, analysis files are stored until the end of the established computational workflow. Then, only the final table file including epitranscriptomic variants are recovered and included in REDIportal. Although this procedure could be time consuming in case of important updates, such as the adoption of a novel genome assembly, it preserves the storage requirements. Epitranscriptome experts often provide reviews on the best tools and practices, so a good starting point is to read such publications. A good example is Investigating RNA editing in deep transcriptome datasets with REDItools and REDIportal. For RNA editing events, prefer RNAseq data from total and rRNA depleted RNA. Strand oriented reads will improve the read mappability, mitigating mis-mapping biases. ",
      "url": "/pages/bedroesb/rdmkit/epitranscriptome_data.html#processing-and-analysis-of-epitranscriptome-data",
      "relUrl": "/epitranscriptome_data.html#processing-and-analysis-of-epitranscriptome-data"
    },"96": {
      "doc": "Epitranscriptome data",
      "title": "Preservation, sharing and reuse of analysis results",
      "content": "Description . Storing epitranscriptome data is relevant for investigating the biological properties of RNA modifications and facilitating the sharing and reuse. Considerations . Which kind of RNA modifications are you studying? Do you have data storage problems when preserving the data? Can epitranscriptome data be openly shared? . Solutions . For long term storage and for preserving epitranscriptome data, raw reads have to be submitted to public databases. This is a mandatory requirement to upload epitranscriptomic annotations in specialized databases. In case of data deposited in public databases such as ENA or SRA, RNA modifications could be uploaded in dedicated databases as REDIportal. To avoid the storage of a large amount of files, raw data is used to complete all computational steps. Soon after, they are removed as well as intermediate files. Only final tables are preserved and stored in our portal. Data is actually preserved because raw data is always available through public and established databases. All data included in the REDIportal, including individual variants, annotations and metadata, is sharable and open. Only one database is mentioned here because there is the plan of having a unique and individual resource for epitranscriptome data. ",
      "url": "/pages/bedroesb/rdmkit/epitranscriptome_data.html#preservation-sharing-and-reuse-of-analysis-results",
      "relUrl": "/epitranscriptome_data.html#preservation-sharing-and-reuse-of-analysis-results"
    },"97": {
      "doc": "Epitranscriptome data",
      "title": "Epitranscriptome data",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/epitranscriptome_data.html",
      "relUrl": "/epitranscriptome_data.html"
    },"98": {
      "doc": "Spain",
      "title": "Introduction",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/es_resources.html#introduction",
      "relUrl": "/es_resources.html#introduction"
    },"99": {
      "doc": "Spain",
      "title": "Funders",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/es_resources.html#funders",
      "relUrl": "/es_resources.html#funders"
    },"100": {
      "doc": "Spain",
      "title": "Regulations",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/es_resources.html#regulations",
      "relUrl": "/es_resources.html#regulations"
    },"101": {
      "doc": "Spain",
      "title": "Domain-specific infrastructures/resources",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/es_resources.html#domain-specific-infrastructuresresources",
      "relUrl": "/es_resources.html#domain-specific-infrastructuresresources"
    },"102": {
      "doc": "Spain",
      "title": "Spain",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/es_resources.html",
      "relUrl": "/es_resources.html"
    },"103": {
      "doc": "Events",
      "title": "Upcoming events",
      "content": ". Contentathon 23 June 2021 9:00 - 24 June 2021 13:30 CET . Online . We would like to invite you to highlight your set of data management tools as a tool assembly in the RDMkit and describe how to use it, so others can do the same (two half days). Sign up here. BioHackathon Europe 08 November 2021 - 12 November 2021 . Barcelona . Highlight your data management tool assembly in the RDMkit! Sign up here. RDMkit Focus session: Machine-Readable Metadata 24 September 2021 13:00 - 15:00 CET . Zoom . Do you have experience in generating machine-readable metadata during life sciences research projects? If yes, join this focus group! Help to describe best practices on machine-readable metadata in a your task page. We are bundling our expertise on this topic in a joint discussion and writing session. For more information, please contact the rdm-editors@elixir-europe.org. ",
      "url": "/pages/bedroesb/rdmkit/events.html",
      "relUrl": "/events.html"
    },"104": {
      "doc": "Events",
      "title": "Past events",
      "content": ". Contentathon 23 June 2021 9:00 - 24 June 2021 13:30 CET . Online . We would like to invite you to highlight your set of data management tools as a tool assembly in the RDMkit and describe how to use it, so others can do the same (two half days). Sign up here. BioHackathon Europe 08 November 2021 - 12 November 2021 . Barcelona . Highlight your data management tool assembly in the RDMkit! Sign up here. RDMkit Focus session: Machine-Readable Metadata 24 September 2021 13:00 - 15:00 CET . Zoom . Do you have experience in generating machine-readable metadata during life sciences research projects? If yes, join this focus group! Help to describe best practices on machine-readable metadata in a your task page. We are bundling our expertise on this topic in a joint discussion and writing session. For more information, please contact the rdm-editors@elixir-europe.org. ",
      "url": "/pages/bedroesb/rdmkit/events.html",
      "relUrl": "/events.html"
    },"105": {
      "doc": "Events",
      "title": "Events",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/events.html",
      "relUrl": "/events.html"
    },"106": {
      "doc": "Existing data",
      "title": "How can you find existing data?",
      "content": "Description . Many datasets could exist that you can reuse for your project. Even if you know the literature very well, you can not assume that you know everything that is available. Datasets that you should be looking for can either be collected for the same purpose in another earlier project, but it could also have been collected for a completely different purpose and still serve your goals. Considerations . Creation of scientific data can be a costly process. For a research project to receive funding one needs to justify, in the project’s data management plan, the need for data creation and why reuse is not possible. Therefore it is advised to always check first if there exists suitable data to reuse for your project. When the outputs of a project are to be published, the methodology of selecting a source dataset will be subjected to peer review. Following community best practice for data discovery and documenting your method will help you later in reviews. List the characteristics of the datasets you are looking for, e.g. format, availability, coverage, etc. This enables you to formulate the search terms. Please see Gregory K. et al. Eleven quick tips for finding research data. PLoS Comput Biol 14(4): e1006038 (2018) for more information. Solutions . Locate the repositories relevant for your field. Check the bibliography on relevant publications, and check where the authors of those papers have stored their data. Note those repositories. If papers don’t provide data, contact the authors. Data papers provide peer-reviewed descriptions of publicly available datasets or databases and link to the data source in repositories. Data papers can be published in dedicated journals, such as Scientific Data, or be a specific article type in conventional journals. Search for research communities in the field, and find out whether they have policies for data submission that mention data repositories. For instance, ELIXIR communities in Life Sciences. Locate the primary journals in the field, and find out what data repositories they endorse. Journal websites will have a “Submitter Guide”, where you’ll find lists of recommended deposition databases per discipline, or generalist repositories. For instance, Scientific Data’s Recommended Repositories. You can also find the databases supported by a journal through the policy interface of FAIRsharing. Search registries for suitable data repositories. FAIRsharing is an ELIXIR resource listing repositories. Re3data lists repositories from all fields of science. Google Dataset search or Datacite for localization of datasets. The Omics Discovery Index (OmicsDI) provides a knowledge discovery framework across heterogeneous omics data (genomics, proteomics, transcriptomics and metabolomics). Search through all repositories you found to identify what you could use. Give priority to curated repositories. ",
      "url": "/pages/bedroesb/rdmkit/existing_data.html#how-can-you-find-existing-data",
      "relUrl": "/existing_data.html#how-can-you-find-existing-data"
    },"107": {
      "doc": "Existing data",
      "title": "How can you reuse existing data?",
      "content": "Description . When you find data of interest, you should first check if the quality is good and if you are allowed to use the data for your purpose. This process might be difficult, so you can find guidelines and tools below. Considerations . Before reusing the data, make sure to check if a licence is attached and that it allows your intended use of the data. Check if metadata or documentation are provided with the data. Metadata and documentation should provide enough information for a correct interpretation and reuse of the data. The use of standard metadata schemes and ontologies increase reusability of the data. Quality of the data is of utmost importance. You should check whether there is a data curation process on the repository (automatic, manual, community). This information should be available on the repository’s website. Check if the repository provides a quality status of each dataset (e.g. star rating system or quality indicators). The data you choose to reuse may be versioned. Before you start to reuse it you should decide which version of the dataset you will use. Solutions . Verify that the data is suitable for reuse. Check the licences or repository policy for data usage. Data from publications can generally be used but make sure that you cite the publication as reference. If you cannot find the licence of the data, contact the authors. No licence means no reuse allowed. If you are reusing personal (identifiable) or even sensitive data, some extra care needs to be taken (see Human data and Sensitive data pages): . Make sure you select a data repository that has a clear, published data access/use policy. You do not want to be liable for improper reuse of personal information. For instance, if you’re downloading human data from some lab’s website make sure there is a statement/confirmation that the data was collected with ethical and legal considerations in place. Sensitive data is often shared under restrictions. Check in the description of the access conditions whether these match with your project (i.e. whether you would be able to successfully ask to get access to the data). For instance, certain datasets can only be accessed by projects with Ethics/Institutional Review Board approval or some can only be used within a specific research field. Verify the quality of the data. Some repositories have quality indicators, such as: . Star system indicating level of curation, e.g. for manually curated/non-curated entries. Evidence ontology. Detailed quality assessment methods. For instance, PDBe has several structure quality assessment metrics. If metadata is available, check the quality of metadata. For instance, information about experimental setup, sample preparation, data analysis/processing can be necessary to reuse the data and reproduce the experiments. Decide which version (if present) of the data you will use. You can decide to always use the version that is available at the start of the project. You would do this if switching to the new versions would not be very beneficial to the project or it would require major changes. In this case, you need to make sure that you and others, who want to reproduce your results, can access the old version at a later stage too. You can update to the latest versions if new ones come out during your project. You would do this if the new version does not require major changes in your project workflow, and/or if the updates could improve your project. In this case, consider that you may need to re-do all your calculations based on a new version of the dataset and make sure that everything stays consistent. ",
      "url": "/pages/bedroesb/rdmkit/existing_data.html#how-can-you-reuse-existing-data",
      "relUrl": "/existing_data.html#how-can-you-reuse-existing-data"
    },"108": {
      "doc": "Existing data",
      "title": "Existing data",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/existing_data.html",
      "relUrl": "/existing_data.html"
    },"109": {
      "doc": "Finland",
      "title": "Introduction",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/fi_resources.html#introduction",
      "relUrl": "/fi_resources.html#introduction"
    },"110": {
      "doc": "Finland",
      "title": "Funders",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/fi_resources.html#funders",
      "relUrl": "/fi_resources.html#funders"
    },"111": {
      "doc": "Finland",
      "title": "Regulations",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/fi_resources.html#regulations",
      "relUrl": "/fi_resources.html#regulations"
    },"112": {
      "doc": "Finland",
      "title": "Domain-specific infrastructures/resources",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/fi_resources.html#domain-specific-infrastructuresresources",
      "relUrl": "/fi_resources.html#domain-specific-infrastructuresresources"
    },"113": {
      "doc": "Finland",
      "title": "Finland",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/fi_resources.html",
      "relUrl": "/fi_resources.html"
    },"114": {
      "doc": "France",
      "title": "Introduction",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/fr_resources.html#introduction",
      "relUrl": "/fr_resources.html#introduction"
    },"115": {
      "doc": "France",
      "title": "Funders",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/fr_resources.html#funders",
      "relUrl": "/fr_resources.html#funders"
    },"116": {
      "doc": "France",
      "title": "Regulations",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/fr_resources.html#regulations",
      "relUrl": "/fr_resources.html#regulations"
    },"117": {
      "doc": "France",
      "title": "Domain-specific infrastructures/resources",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/fr_resources.html#domain-specific-infrastructuresresources",
      "relUrl": "/fr_resources.html#domain-specific-infrastructuresresources"
    },"118": {
      "doc": "France",
      "title": "France",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/fr_resources.html",
      "relUrl": "/fr_resources.html"
    },"119": {
      "doc": "GitHub way",
      "title": "Announce and discuss your proposal through GitHub “issues”",
      "content": ". Go to the RDMkit repo on GitHub. Click on “Issues” in the top menu bar and look at the existing issues. See if your idea or suggestion is already being discussed. If an issue exists, add your comments. If you want to edit the content discussed, let people know through the comments. If no relevant issue exists, create a new issue by clicking on the green “New issue” button on the right, and choose one of the issues templates. You can find more information on creating issues in the GitHub documentation. Discuss your idea with the editors through comments in the issues. You will be notified when others comment on your issues. Read the comments and write your opinion/questions/answers in the “Leave a comment” box and click on the green “Comment” button on the right. You can always return to your opened issue by going to the issues section of our GitHub repo. Tip: You can also get to the RDMkit pages on GitHub using the ‘GitHub’ link in the header of this site ",
      "url": "/pages/bedroesb/rdmkit/github_way.html#announce-and-discuss-your-proposal-through-github-issues",
      "relUrl": "/github_way.html#announce-and-discuss-your-proposal-through-github-issues"
    },"120": {
      "doc": "GitHub way",
      "title": "Write your content and make a pull request",
      "content": ". When you and the editors have agreed on what you will do, go to the page you want to edit on the website. Click on “Edit me” pencil icon :pencil:, shown next to the page title. If you want to add a new page, the editors will give you the link to the page via comments in the issue you created. The page will come with a predefined template, based on the kind of content you want to contribute. The “Edit me” pencil icon will take you to the GitHub repository, where you again click on the pencil icon, shown on the right, and start editing. You can now edit or add new text and images according to the provided template. GitHub provide a guide for writing and formatting in GitHub. We also provide a markdown cheat sheet to show you how to write in this webpage. Make sure to read our style guide before start writing. In general, try to avoid manual interlinking of RDMkit pages. If you have mentioned tools or resources in your text, make sure to add them to the tool and resource list. If you want to list training material in your page, add it to the metadata of the page, according to the training matadata fields provided in the page template. When you are happy with your first draft of the content, go to the “Propose changes” section at the end of the page and write a title and a brief explanation of your changes. Click on “Propose changes”. You are now redirected to the Pull Request (PR) page. A “pull request” is a request to “pull” your changes into the website. Click on the “Create Pull Request” green button. Here you can choose to: . “Create draft pull request”: choose this if you have not finished writing. Later on you can always click on “Ready for review” to switch to a normal pull request. You can find more information about draft pull requests in the GitHub documentation. “Create pull request”: choose this if you have finished your text. Editors will then review your request. In the description of your pull request you can link to the issue that relates to this change by typing a hashtag # and the correct issue number. Suggestions will appear. This way it is easy for the editors to link back the issue were this change might have been discussed beforehand. You can return to your pull request by going to the pull request section of our GitHub repo. If you change your mind about anything in your pull request and the request is not closed, or if the editor tells you to edit your request during the review process, you have to: . Go to your pull request Click on “Files changed” in the top menu bar. Click on the icon with 3 dots “…” of the file you want to edit and then click on “Edit file”. Make your changes. Click on “Commit changes”. Note: Anyone can comment on your issue or pull request and you can reply. For more information on this, please visit the GitHub documentation ",
      "url": "/pages/bedroesb/rdmkit/github_way.html#write-your-content-and-make-a-pull-request",
      "relUrl": "/github_way.html#write-your-content-and-make-a-pull-request"
    },"121": {
      "doc": "GitHub way",
      "title": "Request a review",
      "content": "If you open a normal pull request then a review is automatically requested. The relevant editors will check your changes. If your request is still in draft, click on “Ready for review” to request a review. You can find more information about draft pull requests in the GitHub documentation. ",
      "url": "/pages/bedroesb/rdmkit/github_way.html#request-a-review",
      "relUrl": "/github_way.html#request-a-review"
    },"122": {
      "doc": "GitHub way",
      "title": "Address editors’ comments",
      "content": ". When editors add comments or add a review of your pull request, you will be notified. You need to address editors’ comments and requests by editing your pull request as in step 7 (see above). Go to your pull request Click on “Files changed” in the top menu bar. * Click on the icon with 3 dots \"...\" of the file you want to edit and then click on \"Edit file\". * Make your changes. * Click on “Commit changes”. When all the requests have been addressed, the editors will mark the conversation as “Resolved” and the proposed changes as “Approved”. You content is ready to be merged and published in the main website. Editors publish your content. ",
      "url": "/pages/bedroesb/rdmkit/github_way.html#address-editors-comments",
      "relUrl": "/github_way.html#address-editors-comments"
    },"123": {
      "doc": "GitHub way",
      "title": "GitHub way",
      "content": "This guide tells you how you can easily request and edit a page on this website. You do this using GitHub. For other ways of contributing, see How to contribute. Prerequisite: create a GitHub account before you start. It’s easy and free. ",
      "url": "/pages/bedroesb/rdmkit/github_way.html",
      "relUrl": "/github_way.html"
    },"124": {
      "doc": "Google doc way",
      "title": "Contribution process",
      "content": "RDMkit is hosted on GitHub. We understand, however, that many people may be unfamiliar or inexperienced with git. We therefore provide the Google doc option for contributing. The steps are as follows: . Email the editorial team at rdm-editors@elixir-europe.org to propose a new page or a new section in an existing page. Make sure you keep other contributors in the CC of your email. The editors will create an issue in our GitHub repository to announce your contribution to others. Use our google doc template to write the page and notify the editorial team when you are finished. The editors will assign reviewers to your page, who will provide feedback as comments on the shared google doc. Address the reviewers’ comments and let the editorial team know, again by mail, that you’re finished with revisions. The editors will transfer your page to GitHub and let you know when it is published on the website. ",
      "url": "/pages/bedroesb/rdmkit/google_doc_way.html#contribution-process",
      "relUrl": "/google_doc_way.html#contribution-process"
    },"125": {
      "doc": "Google doc way",
      "title": "Google doc way",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/google_doc_way.html",
      "relUrl": "/google_doc_way.html"
    },"126": {
      "doc": "How to contribute to the RDMkit website",
      "title": "Ways of contributing",
      "content": "Everyone is welcome to contribute to this site, and we’ve tried to make it as easy as possible. Choose a way that suits you. The website is hosted on GitHub, so: . If you are not familiar with Git but want to give it a try on GitHub: follow our step-by-step instructions (no technical knowledge required!). If you are happier using Google Docs: follow the Google Doc way. If you are familiar with Git: fork the repo and create a pull request (see our instructions). If you just want to make a quick suggestion: submit your comments/suggestions using the form below. ",
      "url": "/pages/bedroesb/rdmkit/how_to_contribute.html#ways-of-contributing",
      "relUrl": "/how_to_contribute.html#ways-of-contributing"
    },"127": {
      "doc": "How to contribute to the RDMkit website",
      "title": "Contributor responsibilities",
      "content": "When writing for this website keep in mind the following: . For the sake of consistency, please follow our style guide. We give great importance to authorship credit. If others were involved in your contribution, by writing up or by providing resources such as diagrams or links, please make sure you acknowledge them in the contributors’ section of your page. ",
      "url": "/pages/bedroesb/rdmkit/how_to_contribute.html#contributor-responsibilities",
      "relUrl": "/how_to_contribute.html#contributor-responsibilities"
    },"128": {
      "doc": "How to contribute to the RDMkit website",
      "title": "Attribution of the contributors",
      "content": "Contributors will be shown at the bottom of the page if listed in the metadata of the markdown file. All contributors will also be displayed in the contributors page. If you want to link your GitHub account, ORCID or email address, please add your name and corresponding information to the CONTRIBUTORS file. ",
      "url": "/pages/bedroesb/rdmkit/how_to_contribute.html#attribution-of-the-contributors",
      "relUrl": "/how_to_contribute.html#attribution-of-the-contributors"
    },"129": {
      "doc": "How to contribute to the RDMkit website",
      "title": "Making a quick suggestion",
      "content": "Loading… ",
      "url": "/pages/bedroesb/rdmkit/how_to_contribute.html#making-a-quick-suggestion",
      "relUrl": "/how_to_contribute.html#making-a-quick-suggestion"
    },"130": {
      "doc": "How to contribute to the RDMkit website",
      "title": "How to contribute to the RDMkit website",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/how_to_contribute.html",
      "relUrl": "/how_to_contribute.html"
    },"131": {
      "doc": "Human data",
      "title": "Introduction",
      "content": "When you do research on data derived from human individuals, there are additional aspects that must be considered during the data life cycle. Note, much of the topics discussed on this page will refer to the EU General Data Protection Regulation (GDPR) as it is a central piece of legislation that affects basically all research done on human subjects in the EU and on individuals residing in the EU. Much of the information on this page is of a general nature when it comes to working with human data, an additional focus is on human genomic data and the sharing of such information for research purposes. ",
      "url": "/pages/bedroesb/rdmkit/human_data.html#introduction",
      "relUrl": "/human_data.html#introduction"
    },"132": {
      "doc": "Human data",
      "title": "Planning for, and collection of, human research data",
      "content": "Description . For research on human data, you must follow established research ethical guidelines and legislations. Preferably, planning for these aspects should be done before starting to handle personal data and in some cases such as in the case of the GDPR, it is an important requirement by laws and regulations. Considerations . Have you got an ethical permit for your research project? . To get an ethical permit, you have to apply for an ethical review by an ethical review board. The legislation that governs this differs between countries. Do seek advice from your research institute. In most cases, you should get informed consents from your research subjects. An informed consent is an agreement from the research subject to participate in and share personal data for a particular purpose. It shall describe the purpose and any risks involved (along with any mitigations to minimize those risks) in such a way that the research subject can make an informed choice about participating. It should also state under what circumstances the data can be used for the initial purpose, as well as for later re-use by others. Consider describing data use conditions using a machine-readable formalized description such as DUO. This will greatly improve the possibilities to make the data FAIR later on. Informed consents should be aquired for different purposes: . It is a cornerstone of research ethics. Regardless of legal obligations, it is important to ask for informed consents as it is a good research ethics practice and maintains trust in research. Ethical permission legislation to perform research on human subjects demand informed consents in many cases. Personal data protection legislation might have informed consent as one legal basis for processing the personal data. Note that the content of an informed consent, as defined by one piece of legislation, might not live up to the demands of another piece of legislation. For example, an informed consent that is good enough for an ethical permit, might not be good enough for the demands of the GDPR. The Global Alliance for Genomics and Health (GA4GH) has recommendations for these issues in their GA4GH regulatory and ethical toolkit. Consent Clauses for Genomic Research . Personal data protection legislation . If you are performing research in the EU on human research subjects, or on human research subject in the EU, you must adhere to the General Data Protection Regulation - GDPR. See Data protection for more information on this law. The sensitivity of your data affects what considerations you have make when handling it, see Determining the sensitivity of your data for more information. For some sensitive data you have to perform a Data Protection Impact Assessments. In general, any biomedical research on human subjects will need to do this. Outside EU . For countries outside the EU, the International Compilation of Human Research Standards list relevant legislations. Solutions . Tryggve ELSI Checklist is a list of Ethical, Legal, and Societal Implications (ELSI) to consider for research projects on human subjects. Data Information System DAISY is software tool from ELIXIR that allows the record keeping of data processing activities in research projects. DAWID is a software tool from ELIXIR that allows generation of tailor-made data sharing agreements Privacy Impact Assessment Tool is a software tool to make Data Protection Impact Assessments. MONARC is a risk assessment tool that can be used to do Data Protection Impact Assessments Data Use Ontology Informed Consent Ontology GA4GH regulatory and ethical toolkit EU General Data Protection Regulation. BBMRI-ERIC’s ELSI Knowledge Base contains a glossary, agreement templates and guidance. ",
      "url": "/pages/bedroesb/rdmkit/human_data.html#planning-for-and-collection-of-human-research-data",
      "relUrl": "/human_data.html#planning-for-and-collection-of-human-research-data"
    },"133": {
      "doc": "Human data",
      "title": "Processing and analysing human research data",
      "content": "Description . For human data, it is very important to use technical and procedural measures to ensure that the information is kept secure. There might exist legal obligations to document and implement measures to ensure an adequate level of security. Considerations . Establish adequate Information security measures. This should be done for all types of research data, but is even more important for human data. Information security is usually described as containing three main aspects - Confidentiality, Integrity, and Accessibility. Confidentiality is about measures to ensure that data is kept confidential from those that do not have rights to access the data. Integrity is about measures to ensure that data is not corrupted or destroyed. Accessibility is about measures to ensure that data can be accessed by those that have a right to access it, when they need to access it. Information security measures are both procedural and technical. What information security measures that need to be established should be defined at the planning stage (see above), when doing a risk assessment, e.g. a GDPR Data Protection Impact Assessment. This should identify information security risks, and define measures to mitigate those risks. Contact the IT or Information security office at your institution to get guidance and support to address these issues. ISO/IEC 27001 is an international information security standard adopted by data centres of some universities and research institutes. Locating tools and platforms suited to handle human data . Local research infrastructures might have established compute and/or storage solutions with strong information security measures tailored for working on human data (see some examples below). Contact your institute or your ELIXIR node for guidance. Denmark - Computerome Finland - ePouta, which is part of the CSC Tools Assembly Norway - TSD Tools Assembly Sweden - Bianca Spain - MareNostrum . There are also emerging alternative approaches to analyse sensitive data, such as doing “distributed” computation, where defined analysis workflows are used to do analysis on datasets that do not leave the place where they are stored. The GA4GH is developing standards for this in their GA4GH Cloud workstream . Solutions . EUPID is a tool that allows researchers to generate unique pseudonyms for patients that participate in rare disease studies. RD-Connect Genome Phenome Analysis Platform is a platform to improve the study and analysis of Rare Diseases. DisGeNET is a platform containing collections of genes and variants associated to human diseases. PMut is a platform for the study of the impact of pathological mutations in protein structures. IntoGen collects and analyses somatic mutations in thousands of tumor genomes to identify cancer driver genes. BoostDM is a method to score all possible point mutations in cancer genes for their potential to be involved in tumorigenesis. Cancer Genome Interpreter is designed to identify tumor alterations that drive the disease and detect those that may be therapeutically actionable. GA4GH data security toolkit GA4GH Cloud workstream . ",
      "url": "/pages/bedroesb/rdmkit/human_data.html#processing-and-analysing-human-research-data",
      "relUrl": "/human_data.html#processing-and-analysing-human-research-data"
    },"134": {
      "doc": "Human data",
      "title": "Preserving human research data",
      "content": "Description . It is a good ethical practice to ensure that data underlying research is preserved, preferably in a way that adheres to the FAIR principles. There might also exist legal obligations to preserve the data. With human data, you have to take extra precautions into account when doing this. Considerations . Depositing data in an international repository . To make the data as accessible as possible according to the FAIR principles, do deposit the data in an international repository under controlled access whenever possible, see the section Sharing &amp; Reusing of human research data below . Legal obligations for preserving research data . In some countries there are legal obligations to preserve research data long-term, e.g. for ten years. Even if the data has been deposited in an international repository, this might not live up to the requirements of the law. The legal responsibility for preserving the data would in most cases lie with the research institution where you perform your research. You should consult the Research Data and/or IT support functions of your institution. Information security . The solutions you use need to provide information security measures that are appropriate for storing personal data, see the section Processing and Analysing human research data above. Note that the providers of the solutions must be made aware that there are probably extra information security measures needed for long-term storage of this type of data. Regardless of where your data is preserved long-term, do ensure that it is associated with proper metadata according to community standards, to promote FAIR sharing of the data. Planning for long-term storage . Do address these issues of long-term preservation and data publication as early as possible, preferably already at the planning stage. If you are relying on your research institution to provide a solution, it might need time to plan for this. Solutions . GA4GH data security toolkit ISO/IEC 27001 is an international information security standard adopted by data centres of some universities and research institutes. ",
      "url": "/pages/bedroesb/rdmkit/human_data.html#preserving-human-research-data",
      "relUrl": "/human_data.html#preserving-human-research-data"
    },"135": {
      "doc": "Human data",
      "title": "Sharing and reusing of human research data",
      "content": "Description . To make human research data reusable for others, it must be discoverable, stored in a safe way, and it must be clear under what circumstances it can be reused. Considerations . Selecting suitable access modes for sharing human data . Human data often carries restrictions to its use and it would need to be shared in a manner that obeys such restrictions. There are three access modes for sharing research data: . Open access: Data is shared publicly. Open-access is a rarely used access mode for the sharing of human data. To use open-access researchers need to ensure that the shared data cannot be traced back to individual study participants. In other words the data needs to be anonymised, which is difficult in practice. Registered access: Data is shared with researchers, whose “researcher” status has been vouched for by their institution and who agree to abide by data usage policies of repositories that serve the shared data. Datasets that are shared via registered-access would typically have no restrictions besides the condition that data is to be used for research. Controlled access: Data can only be shared with researchers, whose research is reviewed and approved by a data access committee (DAC). Typically researchers, who were involved in the primary collection of data will form the DAC. Use conditions for controlled-access could be a multitude and includes allowed research topics, allowed geographical regions, allowed recipients e.g. non-profit organisations. Publishing Human Research Data . It is highly recommended that Human Research Data is shared under controlled access. There are emerging models of sharing data through repositories under federated models. The European Genome-phenome Archive (EGA) is the prime repository for human genomic and phenotypic data. The EGA applies a controlled access model. Solutions . The European Genome-phenome Archive (EGA) is an international service for secure archiving and sharing of all types of personally identifiable genetic and phenotypic data resulting from biomedical studies and healthcare centres. Human genomic data is considered Sensitive data and is protected by European GDPR, therefore access must be restricted to authorized users. The EGA platform offers secure and European law-compliant data storage, working with GA4GH standards for encryption and storage. At the same time, data is discoverable in the EGA website and shareable with other researchers through authorization and authentication protocols. The right to allow access belongs to the Data providers (and not to the EGA), who are responsible to sign a DAA (Data Access Agreement) with researchers requesting access to their data. The EGA hosts data from all around the world and distributes it where and when the data providers’ law allows. dbGAP and JGA are other international data repositories, based in the USA and Japan respectively, that adopt a controlled-access model based on their national regulations. Due to European GDPR specific requirements, it may not be possible to deposit EU subjects’ data to these repositories. The GA4GH Beacon project is a Global Alliance for Genomics &amp; Health (GA4GH) initiative that enables genomic and clinical data sharing across federated networks. A Beacon is defined as a web-accessible service that can be queried for information about a specific allele with no reference to a specific sample or patient, thereby reducing privacy risks. GA4GH Data Use Ontology DUO is an international standard, which provides codes to represent data use restrictions for controlled access datasets. ",
      "url": "/pages/bedroesb/rdmkit/human_data.html#sharing-and-reusing-of-human-research-data",
      "relUrl": "/human_data.html#sharing-and-reusing-of-human-research-data"
    },"136": {
      "doc": "Human data",
      "title": "Human data",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/human_data.html",
      "relUrl": "/human_data.html"
    },"137": {
      "doc": "Identifiers",
      "title": "Which types of identifiers can you use during data collection?",
      "content": "Description . A lot of (meta)data is collected in the form of tables, representing quantitative or qualitative measurements (values in cells) of certain named properties (variables in columns) of a range of subjects or samples (records or observations in rows). It can help your research a lot if you make sure you can address each of these records, variables and values unambiguously, i.e. if each has a unique identifier. This is also true for (meta)data that is not in tabular format (“key”:value format, unstructured data etc). Identifiers should be always used for metadata and data independently of the format. If the research institute or group has a centralized and structured system (such as a central electronic database) in place to describe and store (meta)data, this process can be quite straight forward for the researcher. However, if there is no such system, often researchers have to set up an internal “database” to keep track of each record or observation in a study. This situation can be quite challenging for many reasons, one of which is assigning identifiers. The use of identifiers for records, variables and values will increase the reusability and interoperability of the data for you, your future-self and others. Considerations . At the beginning of your research project, check if your institute or research group has a centralised database where data must be entered during data collection. Usually, large and international research projects, industries, research institutes or hospitals have a centralised electronic database, an Electronic Data Capture (EDC) system, a Laboratory Information Management System (LIMS) or an Electronic Lab Note (ELN) with an user interface for data entry. If you can choose how to manage your data entry system, consider what’s the level of exposure of the identifier for each record or observation in the dataset. Define the context in which the identifier should be used and unique. This is a key aspect to define what kind of identifier for each individual record is appropriate in your case. Should the identifier of a record or observation be unique within your spreadsheet, your entire research project files or across the whole institute? What’s the reference system (or “target audience”) of your identifier? Will your reference system change in due time? If it will be opened up later, assigning globally unique identifiers from the beginning may be saving time! Will the identifiers for individual records or observations be made openly accessible on the internet, during data collection? . If the identifier of an individual record or observation should be unique only within your research group (within an intranet), and it will not be available on the internet, it can be considered an “internal or local identifier”. A local identifier is unique only in a specific local context (e.g., single collection or dataset). Local identifiers can be applied not only for individual records or observations in a tabular dataset but also for each variable or even value (respectively, columns and cells in a tabular dataset). Identifiers for an individual record, variable and value in a dataset can be assigned by using ontology terms (see metadata page) or accession numbers provided by public databases such as, EBI and NCBI repositories. Here there are few examples for tabular (meta)data, but the same type of identifiers can be applied independently of the (meta)data structure and format. The patient ID is in its own row, a column header is the variable “disease” from the EFO ontology (ID EFO:0000408), and the value in the cell is the child term “chronic fatigue syndrome” (ID EFO:0004540) of “disease”. The specimen ID is in its own row, a column header is the variable “Ensembl gene ID” from the Ensembl genome browser and the value in the cell is the identifier for BRCA1 gene ENSG00000012048. Solutions . If your institute or research group makes use of centralised electronic databases (EDC, LIMS, ELN, etc), follow the related guidelines for generating and assigning identifiers to individual records or observations, within the database. Some institutes have a centralised way of providing identifiers; ask the responsible team for help. Internal or local identifiers should be unique names based on specific naming convention and formal pattern, such as regular expression. Encode the regular expression into your spreadsheet or software; make sure to describe your regular expression in the documentation (README file or codebook). Avoid any ambiguity! Identifiers that identify specimens (such as a biopsy or a blood sample), animal or plant models or patients could be written to the specimen tubes, the animal or plant model tags and patients files, respectively. Avoid embedding meaning into your local identifier. If you need to convey meaning in a short name implement a “label” for human readability only (Lesson 4. Avoid embedding meaning or relying on it for uniqueness). Do not use problematic characters and patterns into your local identifier (Lesson 5. Avoid embedding meaning or relying on it for uniqueness). Problematic strings can be misinterpreted by some software. In this case it is better to fix the bugs or explicitly declare this possible issue in documentation. Ontology terms or accession numbers provided by public databases, such as EBI and NCBI repositories, can be applied to uniquely identify genes, proteins, chemical compounds, diseases, species, etc. Choose exactly one for each type in order to be the most interoperable with yourself. Identifiers for molecules, assigned by EBI and NCBI repositories, keep track of relations between identifiers (for instance, different versions of a molecule). You can also submit your newly identified molecules to EBI or NCBI repositories to get a unique identifier. Applying ontologies to variables keeps clear structure and relations between variables (i.e.,”compound &amp; dose”, “variable &amp; unit”) . Software that allow you to integrate ontology terms into a spreadsheet are: RightField and OntoMaton; see the table below for additional tools. If you keep track of each records in a tabular format that gets new rows every day, use a versioning system to track the changes. Many cloud storage services offer automatic versioning, or keep a versioning log (see data organisation page). Some parts of the tabular (meta)data file must be stable to be useful: do not delete nor duplicate essential columns. Generate documentation about your tabular (meta)data file (README file, Codebook, etc..). If you collect data from a database that is frequently updated (dynamic or evolving database), it is recommended to keep track not only of the database ID, but also of the used version (by timestamp, or by recording date and time of data collection) and of the exact queries that you performed. In this way, the exact queries can be re-executed against the timestamped data store (Data citation of evolving data). If you reuse an existing dataset, keep the provided identifier for provenance and give a new identifier according to your system, but preserve the relation with the original identifier to be able to trace back to the source. Use a spreadsheet or create a mapping file to keep the relation between provenance and internal identifier. To set up a centralised machine readable database, a EDC, a LIMS or an ELN for large research projects or institutes (available on intranet), highly specialised technical skills in databases, programming and computer science might be needed. We encourage you to talk to the IT team or experts in the field to find software and tools to implement such a system. Software to make a machine-readable system for databases and data collection are available. Their interfaces are quite user friendly but command-line skills might be needed depending on the kind of use that you need. Molgenis is a modular web application for scientific data. MOLGENIS was born from molecular genetics research but has grown to be used in many scientific areas such as biobanking, rare disease research, patient registries and even energy research. MOLGENIS provides researchers with user friendly and scalable software infrastructures to capture, exchange, and exploit the large amounts of data that is being produced by scientific organisations all around the world. Castor is an EDC system for researchers and institutions. With Castor, you can create and customize your own database in no time. Without any prior technical knowledge, you can build a study in just a few clicks using our intuitive Form Builder. Simply define your data points and start collecting high quality data, all you need is a web browser. REDCap is a secure web application for building and managing online surveys and databases. While REDCap can be used to collect virtually any type of data in any environment, it is specifically geared to support online and offline data capture for research studies and operations. We don’t encourage setting up of centralised electronic database that will be exposed to the internet, unless really necessary. We encourage you to use existing and professional deposition databases to publish and share your datasets (see below). ",
      "url": "/pages/bedroesb/rdmkit/identifiers.html#which-types-of-identifiers-can-you-use-during-data-collection",
      "relUrl": "/identifiers.html#which-types-of-identifiers-can-you-use-during-data-collection"
    },"138": {
      "doc": "Identifiers",
      "title": "Which type of identifiers should you use for data publication?",
      "content": "Description . When all records and measurements have been collected and you are ready to share your entire dataset with others, it is good practise to assign globally unique persistent identifiers in order to make your dataset more FAIR. “A Globally Unique Identifier (GUID) is a unique number that can be used as an identifier for anything in the universe and the uniqueness of a GUID relies on the algorthm that was used to generate it” (What is a GUID?). “A persistent identifier (PID) is a long-lasting reference to a resource. That resource might be a publication, dataset or person. Equally it could be a scientific sample, funding body, set of geographical coordinates, unpublished report or piece of software. Whatever it is, the primary purpose of the PID is to provide the information required to reliably identify, verify and locate it. A PID may be connected to a set of metadata describing an item rather than to the item itself” (What is a persistent identifier, OpenAIRE).This means that any dataset with a PID will be findable even if the location of the dataset and its web address (URL) changes. The central registry that manage PID will ensure that the given PID will point you to the digital resource’s current location. There are different types of PID, such as DOI, PURL, Handle and URN. GO FAIR organisation provides examples of GUID, PID and services that supply identifiers. Considerations . PIDs are essential to make your digitale object (datasets or resources) citable, enabling you to claim and receive credit for your research output. In turn, when you reuse someone else research output, you have to cite it. There are a couple of different ways by which you can obtain a globally unique persistent identifier, and you need to decide which one is the best solution for your dataset or resource. By publishing into an existing public repository. For most types of data, this is usually the best option because the repository will assign a globally unique persistent identifier or an accession number. Update your internal database to keep the relationship with public identifiers. By opening up your local database to the public. This requires that the resource has a sustainability plan, as well as policies for versioning and naming of identifiers. While this option could be a viable solution if there is no public repository that allows for the right level of exposure of your data, it puts a lot of responsibility on your shoulders for future maintenance and availability. Solutions . If you want to publish your data into an existing public repository, please first see our data publication page. The repository will provide globally unique persistent identifiers for your data. Check their guidelines if you need to edit or update your dataset after publication. Generic repositories (such as Zenodo and Figshare) use versioning DOI to update a public dataset or document. If you want to publish your data in an institutional public repository, ask the institution to obtain a namespace at identifiers.org in order to obtain globally unique persistent identifiers for your data. If you have the resources and skills to open up your database to the public, obtain a namespace at identifiers.org in order to acquire globally unique persistent identifiers for your data. ",
      "url": "/pages/bedroesb/rdmkit/identifiers.html#which-type-of-identifiers-should-you-use-for-data-publication",
      "relUrl": "/identifiers.html#which-type-of-identifiers-should-you-use-for-data-publication"
    },"139": {
      "doc": "Identifiers",
      "title": "Identifiers",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/identifiers.html",
      "relUrl": "/identifiers.html"
    },"140": {
      "doc": "IFB",
      "title": "What is the IFB data management tool assembly?",
      "content": "The IFB is the French national Bioinformatics Infrastructure that supports research projects in Life Sciences by provisioning a bioinformatics environment, which consists of IT infrastructure (such as storage and computing resources), software and training, distributed across the country. The IFB federates 20 bioinformatics platforms which make physical, operational and human resources available to researchers in a synergistic and efficient way. Each platform brings its own IT infrastructure and bioinformatics expertise to create a better support network, distributed over the country, for Life Sciences research activities. IFB supports scientists since the beginning of a project and relies on the OPIDoR tool to write a data management plan. ",
      "url": "/pages/bedroesb/rdmkit/ifb_assembly.html#what-is-the-ifb-data-management-tool-assembly",
      "relUrl": "/ifb_assembly.html#what-is-the-ifb-data-management-tool-assembly"
    },"141": {
      "doc": "IFB",
      "title": "Who can use the IFB data management tool assembly?",
      "content": "IFB and the underlying infrastructure are accessible to researchers in France and their foreign collaborators. Researchers that would like to know more about IFB services can find specific contact details at the unified IFB help desk page and get support through the dedicated help pages. Depending on the resources, fees may apply. It is therefore advisable to contact them during the planning phase of the project. The way you can access the IFB depends on the type of resources (for instance, cluster or cloud), and there will be different authentication procedures (local, national or international). For example, the Biosphere cloud federation uses the EduGAIN federation for authentication, while useGalaxy.fr uses the ELIXIR AAI authentication. To have additional information on how to access the IFB contact the help desk. ",
      "url": "/pages/bedroesb/rdmkit/ifb_assembly.html#who-can-use-the-ifb-data-management-tool-assembly",
      "relUrl": "/ifb_assembly.html#who-can-use-the-ifb-data-management-tool-assembly"
    },"142": {
      "doc": "IFB",
      "title": "For what can you use the IFB data management tool assembly?",
      "content": "Figure 1. The French Bioinformatics Institute (IFB) tool assembly. Data management planning . IFB relies on Inist infrastructure for the planning phase of the data life cycle and recommends DMP-OPIDoR as a tool for writing a Data Management Plan (DMP). DMP-OPIDoR is hosted and maintained at Inist-CNRS, it is based on DMPRoadmad, but it is tailored to meet the needs of the many French research institutes. You will find 37 DMP templates, in French and/or English, created by funders and research institutes. It is a collaborative tool and as such enables sharing of DMPs amongst partners and also experts or services. A dedicated team offers training and support for DMP templates and DMPs. They can be reached via this contact form. A new machine actionable version of DMP-OPIDoR will allow the production of structured standardized DMP content. It will enable the integration of information from funding agencies such as the French National Agency (ANR), and also integration and interactions with computing infrastructures provided by IFB and Genci, the organization in charge of the three supercomputing centres in France. DMP OPIDoR is freely accessible to anyone. First, one has to create an account (login, password). Then this account can be linked to the Renater identity federation. For support about OPIDoR, you can check the cat-OPIDoR support providers page. Data collection . Although they are not part of IFB, other infrastructures in France can also help you generate new data. Specifically, some facilities can assist you with in vivo and in vitro experiments, synthetic biology, omics techniques, imaging, structural biology and other techniques and expertise. To find the adequate facility you may use the Ministry search engine or the IBiSA directory of french facilities in Life Sciences. Once your data have been generated by the facility, you will need to transfer it to your local system or to the IFB infrastructure, if you intend to use the IFB’s compute services. In both cases it is a good practice to get in touch with IT support (local or IFB), especially if the volume of your data is large. If you have to reuse previously generated data, keep in mind that the different IFB platforms provide many specialized databases. A list of the databases is available here. These databases are, for the most, freely available. Data processing and analysis . IFB infrastructure gives you access to several flavours of computing resources, according to your needs and expertise: . Several clusters hosted either at IFB-Core or on any of the member platforms. You can request accounts on any of the member clusters. The Galaxy France portal operated by the IFB or any of the local instances operated by IFB bioinformatics facilities. The cloud federation Biosphere allows the deployment of ready-to-use appliances (virtual machines with all required software installed for analysis) for several scientific domains (Genomics, Bioimaging, Metabolomics, etc.). A list of the different appliances is available on the RainBio catalogue. You can log in here using your academic credentials. Each of the computing resources offers its own storage solution tailored for the needs of the users (fast access, capacitive). You may have to choose a resource according to what its service offers and also according to its proximity to your own location in order to benefit from better support and also better data transfer speed. IFB infrastructure can also help you with bioinformatics analysis of your data. Many of the IFB member platforms can provide expertise for data analysis in many domains (genomics, metagenomics, transcriptomics) as well as software development. To check the expertise of the platforms, you can use this catalog. A list of the tools developed by all IFB members is available here. Data sharing and publishing . It is good practice to publish your data on repositories. IFB encourages researchers to browse the list of ELIXIR depostion databases for biomolecular data to find the appropriate repository. If you are a member of INRAE (one of the stakeholders of IFB infrastructure), you can access the institutional instance of the Dataverse platform Data INRAE. Data INRAE can be used by researchers to store and describe datasets during the project, and to share them according to specific sharing settings. You can also browse cat-OPIDoR for an overview of the different services related to data management provided by IFB infrastructure and its stakeholders in France. Compliance monitoring &amp; measurement . IFB infrastructure promotes the implementation of the FAIR principles. To this end, IFB provides and encourages the use of the FAIR-Checker, a web interface aimed at monitoring the level of FAIRification of resources. This tool uses the FAIRMetrics APIs to provide a global assessment and recommendations. It also uses semantic technologies to help users in annotating their resources with high-quality metadata. ",
      "url": "/pages/bedroesb/rdmkit/ifb_assembly.html#for-what-can-you-use-the-ifb-data-management-tool-assembly",
      "relUrl": "/ifb_assembly.html#for-what-can-you-use-the-ifb-data-management-tool-assembly"
    },"143": {
      "doc": "IFB",
      "title": "IFB",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/ifb_assembly.html",
      "relUrl": "/ifb_assembly.html"
    },"144": {
      "doc": "Intrinsically disordered proteins",
      "title": "Introduction",
      "content": "Intrinsically disordered proteins (IDP) domain brings together databases and tools needed to organize IDP data and knowledge in a Findable, Accessible, Interoperable and Reusable (FAIR) manner. Experimental data created by users must be complemented by metadata in order to be deposited in an IDP resource. This document describes what community standards must be followed and where to find information needed to complete the metadata of an IDP experiment or study. ",
      "url": "/pages/bedroesb/rdmkit/intrinsically_disordered_proteins.html#introduction",
      "relUrl": "/intrinsically_disordered_proteins.html#introduction"
    },"145": {
      "doc": "Intrinsically disordered proteins",
      "title": "Annotating or curating data from an idp related experiment or study",
      "content": "Description . As a researcher in the field of Intrinsically Disordered Proteins (IDPs), you want to know how to process an experimental result in a FAIR way. As a final aim, you want to deposit the data in a community database or registry for wider adoption. Considerations . You can split the experimental process in several steps: . How should you describe properly an IDP experiment? Are there any community standards that you should follow? How do you add metadata in order to make IDP data more machine readable? How should you publish IDP data to a wider audience? . Solutions . The IDP community developed a MIADE standard under a PSI-ID workgroup. The standard specifies the minimum information required to comprehend the result of a disorder experiment. The standard is available in XML and TAB format. You can check example annotation in XML and TAB format and adapt it to your data. The IDP community developed an Intrinsically Disordered Proteins Ontology (IDPO). The ontology is an agreed consensus of terms used in the community, organized in a structured way. The ontology is available in OWL and OBO format. You should deposit primary data into relevant community databases (BMRB, PCDDB, SASBDB). You should deposit literature data to the manually curated database DisProt. DisProt is built on MIADE standard and IDPO ontology. As such, DisProt requires curators to annotate all new data according to community standards. IDP data from primary databases, together with curated experimental annotations and software predictions, is integrated in the comprehensive MobiDB database. DisProt and MobiDB add and expose Bioschemas markup to all data records increasing data findability and interoperability. ",
      "url": "/pages/bedroesb/rdmkit/intrinsically_disordered_proteins.html#annotating-or-curating-data-from-an-idp-related-experiment-or-study",
      "relUrl": "/intrinsically_disordered_proteins.html#annotating-or-curating-data-from-an-idp-related-experiment-or-study"
    },"146": {
      "doc": "Intrinsically disordered proteins",
      "title": "Issues annotating or describing an idp related term or study",
      "content": "Description . IDP field is actively evolving. It integrates newly published experimental evidence of protein disorder and translates it in a machine readable way in an IDP database. This mapping process relies on accurate knowledge of protein identifiers, protein regions under study and disorder region functional annotation. Considerations . Most common issues that you as a researcher can encounter during the mapping process are: . how to properly and uniquely identify the protein (or fragment) under study? how to deal with missing terms in IDPO? . Solutions . In order to uniquely identify the protein under study, you should identify the protein on UniProt reference protein database. The protein identifier must be complemented with an isoform identifier (if needed) in order to completely match the experimental protein sequence. Use the SIFTS database to precisely map the experimental protein fragment (deposited at PDB) to a reference protein database (UniProt) at an amino acid level. Experimental evidence from literature must be mapped to relevant IDPO terms. If no suitable term could be found in IDPO, try with following resources: . Evidence &amp; Conclusion Ontology (ECO) for experimental methods Molecular Interactions Controlled Vocabulary for molecular interactions Gene Ontology for functional terms . If there isn’t an appropriate term in ontologies or vocabularies, you can submit a new proposal for community review at DisProt feedback. ",
      "url": "/pages/bedroesb/rdmkit/intrinsically_disordered_proteins.html#issues-annotating-or-describing-an-idp-related-term-or-study",
      "relUrl": "/intrinsically_disordered_proteins.html#issues-annotating-or-describing-an-idp-related-term-or-study"
    },"147": {
      "doc": "Intrinsically disordered proteins",
      "title": "Intrinsically disordered proteins",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/intrinsically_disordered_proteins.html",
      "relUrl": "/intrinsically_disordered_proteins.html"
    },"148": {
      "doc": "Licensing",
      "title": "Why should you assign a licence to your research data?",
      "content": "Description . Loosely said, a licence defines what a user is allowed to do with a dataset. This can take into account ownership rights (copyright) as well as subject rights if the data is describing human beings. There are large differences between how copyrights and subject rights are to be addressed! . Complying with copyright is primarily the responsibility of the user of the data. Copyright laws allow only the creator of a work to reproduce and use it. If anyone else wants to use the work, then that person needs explicit permission from the holder of the copyright. A copyright licence describes the nature of this agreement, and does not need a signature: the user can never deny the existence of any conditions, because without the licence they would not be able to use the work at all. Complying with subject rights is primarily the responsibility of the controller (frequently called: owner) of the data. In Europe, the GDPR is an important law specifying the rights of subjects, and it is the controller of the data who needs to ensure that any usage of the data has a legal basis; not only his or her own use of the data, but also the use by others. If others use data about human subjects, this will require contracts between the controller and such others. These contracts, unlike copyright licences, will require a signature. Important contract forms are Data Transfer Agreements and Data Processing Agreements. Licensing is an important aspect of meeting the R (reusable) principle in FAIR data management. As part of the publication process, you need to decide under which licence the data is released. Considerations . If you are producing a dataset and make it available to others, you should be explicit about what others are allowed to do with this. A document describing that is called a licence. If you are reusing a dataset that comes from somewhere, you will want to have a licence that explains what you can do with it. Without a licence, reusing a dataset could be setting you up for legal trouble. Note that different interpretations of copyright laws think differently about copyrights on data. Under some jurisdictions, some data is explicitly not subject to copyrights. An example is data describing the earth under the laws of the United States of America. Copyright law specifies that it only applies to a “creative work”, and arguably, just collecting data does not have sufficiently creative steps to claim copyrights. Relying on this as a reuser of data, however, is dangerous. Look for a licence and apply it. As a data producer you should be aware that you may not be able to uphold licence restrictions in court, because it may be decided that the dataset is not copyrightable in the first place. It is therefore best to apply a permissive licence, not asserting strong copyrights. Be sure of data ownership before publishing data. Are there rights belonging to a third party? . Solutions . Make your research data available under an appropriate licence, which defines the degree of publicity and rights to use your data. Choose a licence that ensures your data is correctly attributed and makes the terms of reusing your data explicit to the user. ",
      "url": "/pages/bedroesb/rdmkit/licensing.html#why-should-you-assign-a-licence-to-your-research-data",
      "relUrl": "/licensing.html#why-should-you-assign-a-licence-to-your-research-data"
    },"149": {
      "doc": "Licensing",
      "title": "What licence should you apply to your research data?",
      "content": "Description . This depends on what rights protect your research data. Which licence to choose might be governed by university policy or funders’ mandates. Research data can have varying degrees of publicity. There are circumstances in which data may be subject to restrictions eg. if datasets contain sensitive information. Considerations . If possible, choose and apply the least restrictive licence to ensure the widest possible reuse. Remember that if you publish your data in a data repository of your choice, a licence agreement will be applied to your data. Repositories can be selected based on data licence and sharing policy by using re3data.org. ELIXIR data resources ideally have terms of use or a licence that enables the reuse and remixing of data. Remember that the rights granted in a licence cannot be revoked once it has been applied. Solutions . Apply to your data one of the recommended licenses conformant to the Open Definition, so that your data can be shared and reused. The Open Definition sets out principles that define the meaning of “open” in relation to data and content. Creative Commons licenses are the best known open data licences and are available in human-readable and machine-readable forms, with different levels of permissions. Creative Commons License Chooser helps you choose the right Creative Commons licence for your needs. The video tutorial from Kingsborough E-Learning shows how to add a Creative Commons licence to your work in practice. The following tools helps you find the right licence for your software and data: . EUDAT licence selector wizard Choose a license is an online guide provided by GitHub to help you choose a license for open-source projects. data.world provides list of common license types for datasets. If your research data is a database or a dataset, consider putting it in the public domain by using the Creative Commons CC0 tool. CC0 let you waive all your rights to the work (“No Rights Reserved”). ",
      "url": "/pages/bedroesb/rdmkit/licensing.html#what-licence-should-you-apply-to-your-research-data",
      "relUrl": "/licensing.html#what-licence-should-you-apply-to-your-research-data"
    },"150": {
      "doc": "Licensing",
      "title": "Licensing",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/licensing.html",
      "relUrl": "/licensing.html"
    },"151": {
      "doc": "Machine actionability",
      "title": "What does machine-readable, machine-actionable or machine-interpretable mean for data and metadata in RDM?",
      "content": "Description . More and more often, funders, data managers/stewards, IT staff and institutions in general encourage researchers in Life Sciences to generate metadata (and data) in ways that can be retrieved, read and processed by computers (machines). Considerations . It is common to come across different terms, such as machine-readable, machine-actionable and machine-interpretable, which express different levels of making “(meta)data for machines”. The definition and the differences between these terms are not always clear and depend on the current technology. Computers, software, programming languages, formats and standards evolve quite fast and therefore new inventions could potentially make machine-readable/actionable any digital object that wasn’t before. One example is how developments in computer vision are making more and more the information contained in images, and not just the images themselves, available for processing. While providing an all-encompassing definition for this topic is not within the scope of this platform, it is important to clarify that (meta)data can be used by machines to different extents, depending on its characteristics. Here, we report a few common definitions. “Machine-readable: data in a data format that can be automatically read and processed by a computer, such as CSV, JSON, XML, etc. Machine-readable data must be structured data.”, Open Data Handbook. “Machine-readable data, or computer-readable data, is data in a format that can be processed by a computer. Machine-readable data must be structured data.”, Wikipedia. “Machine-actionable: this term refers to information that is structured in a consistent way so that machines, or computers, can be programmed against the structure.”, DDI. Machine-interpretable: machines can put the provided information into context and “understand” the meaning (semantics) and relations contained in the digital object. This concept is strictly related to the Semantic Web vision and the Linked Data concept. See e.g. What Is the Semantic Web?. The terms machine-readable and machine-actionable are often used interchangeably as synonymous. It is because of the variety of possible definitions for data that can be processed in some form by computers, that we decided to use the term machine-actionable in the remainder of this document to refer to this type of (meta)data. Machine-actionable (meta)data doesn’t mean just digital. For computers and software, it might not be possible to process the information contained in a digital object (e.g. scanned image). It is also NOT just: . A digital file that is readable by some software (i.e. not broken or corrupted). A digital file in an open (non-proprietary) or free format (ex: .txt, .pdf) that can be read by some software. A digital file that is readable by some non-proprietary software (e.g.txt, .pdf). “The appropriate machine-actionable/readable format may vary by type of data - so, for example, machine-actionable/readable formats for geographic data may differ from those for tabular data.”, Open Data Handbook. For instance, GML is one of the appropriate format for geographical information. Machine-actionable/readable formats are typically difficult to read by humans. Human-readable data is “in a format that can be conveniently read by a human. Some human-readable formats, such as PDF, are not machine-actionable/readable as they are not structured data, i.e. the representation of the data on disk does not represent the actual relationships present in the data.”, Open Data Handbook. For instance, have you ever tried to extract or copy-paste a table from a PDF into a spreadsheet? It is usually very difficult and sometimes even impossible. This is a practical example of why PDF is not easy to read by machines, but it is very easy to read by humans. This occurs because the content in a PDF is described as “characters painted or drawn on a space”. So text is not text and tables are not tables for a PDF. They are just characters on the page space. Tabular data in CSV file can be quite easy to read by humans, unless the table is very very big. A file in CSV format can be read by machines since it is organised in records (lines) and fields (columns) separated by comma, that is as a table. So, the computer reads whatever information stored as CSV in this tabular format. Solutions . For RDM in Life Sciences, machine-actionable metadata and data should: . Be structured data: “data where the structural relation between elements is explicit in the way the data is stored on a computer disk.”, Open Data Handbook. Be in a format that allows “many types of structure to be represtented.”, Open Data Handbook. For instance, JSON and XML for text files; certain formats for e.g. images that include structured (meta)data in a structured format. Common formats such as XML and JSON contribute to syntactic interoperability between machines. Be interpreted by computer systems unambiguously. The meaning (semantic) of the (meta)data should be unique and shared among computer systems. Syntaxes such as JSON-LD and RDF/XML contribute to semantic interoperability. Not be in PDF format (scanned images of lab books, tables, articles or papers in .pdf) Not be in plain text (.txt) nor Word documents (.docx) formats (e.g. README.txt file). Not be images, audio nor video (.jpg, png, etc). ",
      "url": "/pages/bedroesb/rdmkit/machine_actionability.html#what-does-machine-readable-machine-actionable-or-machine-interpretable-mean-for-data-and-metadata-in-rdm",
      "relUrl": "/machine_actionability.html#what-does-machine-readable-machine-actionable-or-machine-interpretable-mean-for-data-and-metadata-in-rdm"
    },"152": {
      "doc": "Machine actionability",
      "title": "What are the advantages of generating machine-actionable metadata and data, during and after the project?",
      "content": "Description . Numerous research institutes have already introduced or are going to introduce the use of Electronic Laboratory Notebook (ELN), Laboratory Information Management System (LIMS) or similar systems to manage samples, reagents, metadata and data, during a research project. The reason for this is that this software could organize information in a structured way and make (meta)data “more” machine-actionable, compared to traditional lab books or individual folders and files in a computer. The use of machine-actionable (meta)data allows for scalable solutions that can be applied during a project’s lifetime, increasing efficiency and ensuring that findings and contributions remain relevant within the research group. Similarly, funders and institutions ask researchers to make their (meta)data FAIR and available in a machine-actionable way. This means that (meta)data should be in databases that can expose it in such a way to allow search engines and harvesting servers to discover it, index it and link it to other relevant contextual information, thus vastly enhancing the likelihood of reusing the data (see Horizon Europe DMP template). Considerations . During a research project, scientists and researchers should utilize metadata in order to use, reuse, and expand knowledge by: . Managing experiments, samples and analysis pipelines. Expanding current datasets e.g. to increase the sample size. Repeating experiments done by colleagues in the team. Reproducing and confirming findings done by others. Testing new hypotheses on data generated for different purposes. Scalable reuse of existing data is possible only if (meta)data is annotated with commonly used terms and findable by computers (e.g. database browser or search engines). The alternative could be very tedious and inefficient since you might have to: . Read the lab book of previous colleagues until you find the right page(s) where information about previously generated data is provided. Look through numerous (shared) folders to find the documentation about a specific experiment done by previous colleagues that generated the dataset you are interested in. Read all publications about a topic and check if there is a dataset linked to it and/or available upon request. Integration of multiple datasets can be straightforward only if each dataset can be easily queried, processed and formatted via software/programmes that can properly handle structured and big (meta)data files, such as OpenRefine and programming languages such as Python or R. Otherwise, manual data integration and processing can be very slow and error-prone. Advantages . The advantages of having machine-actionable data and metadata are numerous for all the parties involved. For researchers . By providing structured metadata and data to a database that follows standards (metadata schemas, ontologies, file formats, programmatic access, etc.), at the level of each recorded value or observation, researchers: . Could more easily query and filter (meta)data based on specific variables, experimental conditions, biological sources and many other parameters, based on the capabilities of the used ELN or data management software. Can more easily find and reproduce experiments performed in the past by others in literature or in databases e.g. by using Europe PMC and EBI Search. Can easily integrate data from multiple datasets and studies, sharing the same experimental conditions or variables. Datasets integration and manipulation are easier to achieve, more reproducible and can be automated by using common programmes/software such as R and OpenRefine. Can make use of visualization and exploration tools, provided by some repositories, to browse and explore the data of multiple datasets at once. For instance, you can use Expression Atlas to easily make a query about the expression of a gene in specific conditions, even without knowledge of any data analysis software. As another example, GISAID allows you to visualise the spreading of viral variants. See the pages in the “Your Domain” section to find domain-specific databases, atlas or portals. Can import, export and exchange (meta)data between tools/systems/platforms without data loss. Exchanging and integrating (meta)data between two software or platforms is possible only if the format in which the information is contained can be read and interpreted by both. For instance, (meta)data from both UniProt and PDBe-KB can be accessed in 3DBioNotes to enrich the structural analysis with sequence features. Can explore and visualise biological knowledge graphs by using software such as KnetMiner and AgroLD. Can perform complex queries, from a single entry point, across multiple distributed databases and across domains via APIs or via SPARQL Query Language. For instance: “Retrieve the number of UniProtKB/Swiss-Prot human enzymes that metabolize cholesterol or cholesterol derivatives and that are involved in diseases?” in the Integrated Database of Small Molecules. Can more easily find reference data and existing data in general, since machine-actionable (meta)data could be found by search engines and domain specific or generic data catalogs and portals. For software developers and repositories managers . Implementation of domain specific metadata schemas and ontologies for data and metadata increases the reusability for researchers. The use of machine-actionable formats and ontologies contribute to syntactic and semantic interoperability of the (meta)data, which can be used by other tools/software or platforms. Applying RDF syntax to the database can make the (meta)data available for knowledge graphs and semantic web applications. If Application Programming Interface (API) is available, other software/applications could make complex queries, access the database programmatically and always get up-to-date data. If the metadata of your database or repository is exposed according to specific standards, it could function as data provider or data source, and be harvested and indexed by . Data catalogues or data portals, such as OmicsDI and COVID-19 Data Portal. Other instances of your data repository software, such as Dataverse and EUDAT B2FIND, which use OAI-PMH for metadata harvest. Search engines such as Google Dataset Search, which relies on sitemaps.org, schema.org, DCAT and other approaches to datasets discovery. Machine actionable metadata facilitates the automatization of data handling and validation, allowing for easier development of new tools and analysis strategies (e.g. data visualization tools, machine learning and artificial intelligence applications). For the authors of a machine-actionable public dataset . High impact of the published data. More citations. More opportunity for collaborations. Opportunity for reproducibility test and confirmation of their results by others. Easy way to reuse the same (meta)data for other research. Improved scalability of their research efforts. For public funders and institutions/governments . Proof that their fundings produced knowledge that is findable and reusable. Transparency. Straightforward collection and indexing of research output in registries for easier impact assessment and report. ",
      "url": "/pages/bedroesb/rdmkit/machine_actionability.html#what-are-the-advantages-of-generating-machine-actionable-metadata-and-data-during-and-after-the-project",
      "relUrl": "/machine_actionability.html#what-are-the-advantages-of-generating-machine-actionable-metadata-and-data-during-and-after-the-project"
    },"153": {
      "doc": "Machine actionability",
      "title": "Machine actionability",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/machine_actionability.html",
      "relUrl": "/machine_actionability.html"
    },"154": {
      "doc": "Marine metagenomics",
      "title": "Introduction",
      "content": "The marine metagenomics domain is characterized by large datasets that require access to substantial storage and High-Performance Computing (HPC) for running complex and memory-intensive analysis pipelines, and therefore are difficult to handle for typical end-users and beyond the resources of many service providers. With respect to sharing metagenomics datasets in compliance with the FAIR principles, so that they can be reused, it hinges entirely on recording rich metadata about all the steps from sampling to data analysis. ",
      "url": "/pages/bedroesb/rdmkit/marine_metagenomics.html#introduction",
      "relUrl": "/marine_metagenomics.html#introduction"
    },"155": {
      "doc": "Marine metagenomics",
      "title": "Managing marine metagenomic metadata",
      "content": "Description . Metagenomics is a highly complex process the encompasses several steps including: sampling, isolation of DNA, generation of sequencing libraries, sequencing, pre-processing of raw data, taxonomic and functional profiling using reads, assembly, binning, refinement of bins, generation of MAGs, taxonomic classification of MAGs, and archiving of raw or processed data. To comply with the FAIR principles, you need to collect metadata about all these steps. Moreover, in marine metagenomics, it is also necessary to characterize the marine environment of the sample, including geolocation, and the physico-chemical properties of the water. Solutions . As a starting point to get acquainted with the intricacies of reporting marine metagenomics experiments, the following publications are recommended reading: . The metagenomic data life-cycle: standards and best practices which describes the metagenomics data life-cycle in detail. Marine microbial biodiversity, bioinformatics and biotechnology (M2B3) data reporting and service standards, guided by marine microbial research, and providing clear examples and colour-coded illustrations. Metadata standards that apply to marine metagenomics data are the Genome Standards Consortium family of minimum information standards, including the core standard, Minimum Information about any (x) Sequence (MIxS), the derived Minimum Information about (Meta)genome Sequence (MIGS/MIMS), and the also derived Minimum Information About a Metagenome-Assembled Genome (MIMAG) that is presently only available as a scientific publication. ",
      "url": "/pages/bedroesb/rdmkit/marine_metagenomics.html#managing-marine-metagenomic-metadata",
      "relUrl": "/marine_metagenomics.html#managing-marine-metagenomic-metadata"
    },"156": {
      "doc": "Marine metagenomics",
      "title": "Tools and resources for analyzing metagenomics datasets",
      "content": "Description . The field of marine metagenomics has been in rapid expansion, with many statistical/computational tools and databases developed to explore the huge influx of data. You need to be able to choose between the multiple bioinformatics techniques, tools, and methodologies available for performing each step of a typical metagenomics analysis, while ensuring that your choice conforms to the best practices for the domain. Moreover you need access to HPC facilities with capacity to execute the analysis and store the resulting data, and therefore should be aware of what computing infrastructures are available to you (and at what cost). Considerations . Are there particular characteristics of your dataset that would restrict the choice of applicable tools? Are the recommended tools freely available? . If not, can you afford the software licensing cost? If not, are there freely available alternatives? . Does your institution have its own HPC facilities, and what are the access conditions? Does your country have a research HPC infrastructure, and what are the access conditions? . Solutions . Experts in the field often provide reviews on the best tools and practices, so a good starting point is to look up such publications. A good example is Metagenomics: tools and insights for analyzing next-generation sequencing data derived from biodiversity studies. Freely available software and pipelines, such as those listed below, can be an option compared to commercial analysis packages. To get access to compute and storage you may contact your local IT department or national ELIXIR node which can guide you to the right facilities. ",
      "url": "/pages/bedroesb/rdmkit/marine_metagenomics.html#tools-and-resources-for-analyzing-metagenomics-datasets",
      "relUrl": "/marine_metagenomics.html#tools-and-resources-for-analyzing-metagenomics-datasets"
    },"157": {
      "doc": "Marine metagenomics",
      "title": "Marine metagenomics",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/marine_metagenomics.html",
      "relUrl": "/marine_metagenomics.html"
    },"158": {
      "doc": "Marine Metagenomics",
      "title": "What is the Norwegian tool assembly for marine metagenomics data management?",
      "content": "The Norwegian tool assembly for marine metagenomics aims to provide a comprehensive toolkit for management of marine genomic research data throughout a project’s data life cycle. The toolkit, developed by students and researchers in Norway, contains resources and software tools for both data management (Planning, Processing, Storing and Sharing), data analysis and training. It is built on the Norwegian e-Infrastructure for Life Sciences (NeLS) tool assembly of ELIXIR Norway and the Marine Metagenomics Platform (MMP). ",
      "url": "/pages/bedroesb/rdmkit/marine_metagenomics_assembly.html#what-is-the-norwegian-tool-assembly-for-marine-metagenomics-data-management",
      "relUrl": "/marine_metagenomics_assembly.html#what-is-the-norwegian-tool-assembly-for-marine-metagenomics-data-management"
    },"159": {
      "doc": "Marine Metagenomics",
      "title": "Who can use the marine metagenomics data management tool assembly?",
      "content": "This tool assembly is useful for students and researchers in Norway who are interested in analysing marine datasets (e.g. genomes and metagenomes and transcriptomes). Parts of the assembly, such as data storage, are based on national infrastructures, laws and regulations, and consequently limited to Norwegian users, while other parts, such as data analysis tools and data repositories, are globally accessible. ",
      "url": "/pages/bedroesb/rdmkit/marine_metagenomics_assembly.html#who-can-use-the-marine-metagenomics-data-management-tool-assembly",
      "relUrl": "/marine_metagenomics_assembly.html#who-can-use-the-marine-metagenomics-data-management-tool-assembly"
    },"160": {
      "doc": "Marine Metagenomics",
      "title": "How can you access the marine metagenomics data management tool assembly?",
      "content": "To be able to use resources and tools that are mentioned here, you are recommended to have a Feide account. In addition, it is important for you to have a NeLs account in order to access usegalaxy.no. In case your institution does not use the national Feide secure login service, you can apply for a NeLs IDP through the ELIXIR Norway help desk. Note, that Marine Metagenomics Platform (MMP) is an open-access platform that can be accessed without a FEIDE account at https://mmp2.sfb.uit.no/. ",
      "url": "/pages/bedroesb/rdmkit/marine_metagenomics_assembly.html#how-can-you-access-the-marine-metagenomics-data-management-tool-assembly",
      "relUrl": "/marine_metagenomics_assembly.html#how-can-you-access-the-marine-metagenomics-data-management-tool-assembly"
    },"161": {
      "doc": "Marine Metagenomics",
      "title": "For what purpose can you use the marine metagenomics data management tool assembly?",
      "content": "Figure 1. The Marine Metagenomics data management tool assembly. Data management planning . The support for data management planning for marine metagenomics in Norway is provided through the ELIXIR-NO instance of the Data Stewardship Wizard. You can access the Data Management Plan model for marine metagenomics in Norway here. Read more on standards and best practices for the metagenomics data life-cycle here. Questions regarding the DSW and data management in general can be directed to the ELIXIR Norway help desk. Data collection . If you use one of the National Norwegian research infrastructures, such as the Norwegian sequencing infrastructure NorSeq, they can directly upload data to your NeLS project for you, as described in this page . Data storage, sharing and compute . The solutions for data storage, sharing and computation are built on the services and infrastructure delivered by ELIXIR Norway described in the Norwegian e-Infrastructure for Life Sciences (NeLS) tool assembly. Data processing and analysis . The Marine Metagenomics Portal provides a complete service for analysis of marine metagenomic data through the tool META-pipe. META-pipe is a pipeline that can assemble your high-throughput sequence data, functionally annotate the predicted genes, and taxonomically profile your marine metagenomics samples, helping you to gain insight into the phylogenetic diversity, metabolic and functional potential of environmental communities. You can find more details about META-pipe here. Norwegian users with Feide access can access the online version of META-pipe. For other users META-pipe is downloadable and can easily be run on any computing environment (e.g. any Linux workstation, SLURM cluster or Kubernetes). Usegalaxy.no is a Norwegian instance of the Galaxy web-based platform for data intensive life science research that provides users with a unified, easy-to-use graphical interface to a host of more than 200 different analysis tools. Here you can find tools for a wide variety of analysis for your marine metagenomic and genomic data. The tools are publicly available in the Galaxy Toolshed which serves as an “appstore” so you can easily transfer them to your favourite Galaxy instance anywhere. You can run the tools interactively, one by one, or combine them into multi-step workflows that can be executed as a single analysis. Premade workflows (i.e for Taxonomic classification of metagenomic sequences) are provided, and you can request installation of your favourite tool by contacting the ELIXIR Norway help desk. Data sharing and publishing . ELIXIR Norway acts as a broker for Norwegian end-users that wish to submit data to ELIXIR Deposition Databases (such as ENA), providing support in submitting the data on behalf of the data owners directly from the National e-infrastructure for Life Science (NeLS). If you need help with publishing or are interested in using the brokering service, please contact the ELIXIR Norway help desk. Data reuse . The Marine Metagenomics Portal (MMP) provides you with high-quality curated and freely accessible microbial genomics and metagenomics resources. Through MMP you can access the The Marine reference databases (MarRef), Marine Genome Database (MarDb), (MarFun), and (SalDB) contextual databases. They are built by aggregating data from a number of publicly available sequence, taxonomy and literature databases in a semi-automatic fashion. Other databases or resources such as bacterial diversity and culture collections databases, web mapping service and ontology databases are used extensively for curation of metadata. At present the MarRef contains nearly 1,000 complete microbial genomes, and MarDB hosts more than 13,000 non-complete genomes. The MAR database entries are cross-referenced with ENA and the World Register of Marine Species (WoRMS). You can read more about the Mar databases here. ",
      "url": "/pages/bedroesb/rdmkit/marine_metagenomics_assembly.html#for-what-purpose-can-you-use-the-marine-metagenomics-data-management-tool-assembly",
      "relUrl": "/marine_metagenomics_assembly.html#for-what-purpose-can-you-use-the-marine-metagenomics-data-management-tool-assembly"
    },"162": {
      "doc": "Marine Metagenomics",
      "title": "Marine Metagenomics",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/marine_metagenomics_assembly.html",
      "relUrl": "/marine_metagenomics_assembly.html"
    },"163": {
      "doc": "Media kit",
      "title": "RDMkit logo",
      "content": "For use in presentations, tutorials and all other RDMkit related activities the RDMkit logo is available in several flavours, both in PNG and SVG: . RDMkit logo horizontal . [svg] [png] . RDMkit logo horizontal inverted . [svg] [png] . RDMkit logo condensed . [svg] [png] . RDMkit logo condensed inverted . [svg] [png] . ",
      "url": "/pages/bedroesb/rdmkit/media_kit.html#rdmkit-logo",
      "relUrl": "/media_kit.html#rdmkit-logo"
    },"164": {
      "doc": "Media kit",
      "title": "Data life cycle diagram",
      "content": "Data life cycle diagram basic . [svg] [png] . Data life cycle diagram with RDMkit logo . [svg] [png] . Data life cycle diagram with central condensed RDMkit logo . [svg] [png] . ",
      "url": "/pages/bedroesb/rdmkit/media_kit.html#data-life-cycle-diagram",
      "relUrl": "/media_kit.html#data-life-cycle-diagram"
    },"165": {
      "doc": "Media kit",
      "title": "Promotion gif",
      "content": "[gif] . ",
      "url": "/pages/bedroesb/rdmkit/media_kit.html#promotion-gif",
      "relUrl": "/media_kit.html#promotion-gif"
    },"166": {
      "doc": "Media kit",
      "title": "Media kit",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/media_kit.html",
      "relUrl": "/media_kit.html"
    },"167": {
      "doc": "Documentation and metadata",
      "title": "How can you document data during the project?",
      "content": "Description . Data documentation could be defined as the clear description of everything that a new “data user” or “your future-self” would need to know in order to find, understand, reproduce and reuse your data, independently. Data documentation should clearly describe how you generated or used the data, why, and where to find the related files. It could be used also as onboarding documentation for new colleagues, even if the responsible researcher leaves the project. Due to the large variety of experiments, techniques and collaborative studies that usually occur within the same project, it is challenging to keep good documentation. However, lack of good data documentation often leads to data loss, not reproducible results and therefore, waste of money and time for scientists. Here we provide best practices and guidelines to help you properly document your data. Considerations . Write the documentation in such a way that someone else who is known to the field can not mis-interpret any of the data, even if they tried. It is best practice to use one appropriate tool or an integration of multiple tools (also called tool assembly or ecosystem) for data documentation during a project. Suitable tools for data documentation are Electronic Lab Notebooks (ELNs), Electronic Data Capture (EDC) systems, Laboratory Information Management Systems (LIMS). Moreover, online platforms for collaborative research and file sharing services (such as OSF) could also be used as ELN or data management systems. Check with your institute to know what is offered. Independently of the tools you will use, data documentation is needed at two levels: documentation about the entire study or project and documentation about individual records, observations or data points. Study-level documentation describes the project title and summary, study aims, authors, institutions involved, funds, methods, licence and identifier for each dataset, folders structure, file naming conventions, versioning system, relation between files or tables and other general information. Data-level documentation provides information about individual records or data point, such as the meaning of each variable name, label, ID or type (numeric, string, regular expression, date, etc), units (i.e., cm, kg…), experimental factors, categories, controlled vocabulary or ontology terms accepted as values for each variable, missing values code and so on. An example could be a data file that contains a “sex” field: someone known to the field could try to misinterpret that from “external sex organs present at birth” to “chromosomal XX or XY” or “high or low testosterone level” or “social gender” or other. In order to avoid this, the way the assignment is made must be part of the documentation or of the data itself (controlled vocabulary). Both the study- and data-level documentation must be generated as early as possible in the research process and also maintained, in order to be accurate and complete . Documentation is also required when publishing your data. General-purpose repositories usually require only study-level documentation, while discipline-specific repositories generally require both study-level and data-level documentation. Importantly, repositories often accept data and documentation in a very strict format: they can require a predefined set of attributes or fields (metadata checklists) to be filled, ontology terms to be used, specific (meta)data schemas (e.g., ISA model, MAGE-TAB) to be adopted. We recommend familiarizing yourself with the requirements of the repositories that could be appropriate for publishing your data already at the beginning of the project, so that you can start documenting and formatting your data accordingly as early as possible. Make sure the documentation is kept close to the data, so that nobody will be exposed to the data without being able to find the documentation. Solutions . There are many appropriate tools for data documentation during the project. Check with your institute to know what is offered. Electronic Lab Notebooks (ELNs) are usually better for more disparate and unstructured information that requires flexibility. Researchers can use ELN in a personalized way and adapt it to document their every-day work. Laboratory Information Management Systems (LIMS) typically follow pre-defined and highly structured experimental workflow. LIMS are used to document and track biological samples through the experimental processes and can support direct import of data from sources such as instruments. Electronic Data Capture (EDC) systems are usually designated for collection of clinical trial data. Online platforms for collaborative research and file sharing services, which integrate with several data management tools, could also be used for data documentation during the project. For instance, OSF.io has integrations with Mendeley, Dropbox, GitHub, Figshare etc. There is a major area of overlap between the aforementioned tools for data documentation, so it is better to choose the tool(s) that best address your specific need. Some tools can be used at the same time to address different needs and they can be complementary. Comparative lists can help with the choice: . Harvard Medical School – ELN Comparison Grid. University of Cambridge - Electronic Research Notebook Products. Independently of the tools, you should agree on and establish a data organisation system for files (or tables in a database) together with your team or Data Management Working Group: . Folder structure File naming convention Versioning system . The established data organization system has to be described in detail in the documentation, preferably in open and machine-readable formats (i.e., XML, JSON, CSV, RDF, HTML). The description of the data organization system has to be placed in the folder at the highest level (e.g. “Project” folder). Study-level and data-level documentation can be provided as . README file Codebook Data dictionary (see an example) Data list . Each of these files can be made in several formats depending on the features available in your data documentation tool, your needs or skills. Machine-readable or -actionable formats (such as .xml, .json, .csv, .rdf) are preferred to non-machine-readable ones (.txt, .xls, .pdf). Also non-proprietary formats are preferred over proprietary ones. Highly structured data documentation is called metadata. Generating metadata in machine-readable or -actionable format makes your data more FAIR . Metadata provides structured and searchable information so that a user can find existing data, evaluate its reusability and cite it. It is good practice to use international standard metadata schemas to organise and store your (meta)data in a structured way. A metadata schema describes the relations, such as hierarchy, of the elements that belong to the structure. It is also good practice to use international standard metadata checklists to describe the content your (meta)data. A (meta)data checklist is a fixed set of attributes about the data that needs to be provided. Some attributes are mandatory, some are only recommended or optional. International standard metadata schemas and checklists are developed by and accepted as standards by communities. There are many standard metadata schemas and checklists, some generic, while others discipline-specific. See the paragraph about how to find standard metadata. You can use the attributes of metadata schemas and checklists in a format that is not machine-readable or -actionable (e.g., by copying the metadata fields in a README.txt file or in a Codebook.xls). However, using standard metadata in a machine-readable or -actionable format will increase the findability of your data. Metadata schemas and checklists usually rely on ontologies and controlled vocabularies, which make your data more reusable and interoperable. See the paragraph about how to find ontologies and controlled vocabularies. We recommend familiarizing yourself with the requirements of the repositories that could be appropriate for publishing your data already at the beginning of the project, so that you can start documenting and formatting your data according to their requirements as early as possible. ",
      "url": "/pages/bedroesb/rdmkit/metadata_management.html#how-can-you-document-data-during-the-project",
      "relUrl": "/metadata_management.html#how-can-you-document-data-during-the-project"
    },"168": {
      "doc": "Documentation and metadata",
      "title": "How do you find appropriate standard metadata for datasets or samples?",
      "content": "Description . There are multiple standards for different types of data, ranging from generic dataset descriptions (e.g. DCAT, Dublin core, (bio)schema.org) to specific data types (e.g. MIABIS for biosamples). Therefore, how to find standard metadata, and how to find an appropriate repository for depositing your data become relevant questions. Considerations . Decide at the beginning of the project what are the recommended repositories for your data types. Note that you can use several repositories if you have different data types. Distinguish between generic (e.g. Zenodo) and data type (technique) specific repositories (e.g. EBI repositories). Solutions . If you have a repository in mind: . Go to the repository website and check the “help”, “guide” or “how to submit” tab to find information about required metadata. On the repository website, go through the submission process (try to submit some dummy data) to identify metadata requirements. For instance, if you consider publishing your transcriptomic data in ArrayExpress, you can make your metadata spreadsheet by using Annotare 2.0 submission tool, at the beginning of the project. Be aware that data type specific repositories usually have check-lists for metadata. For example, the European Nucleotide Archive provides sample checklists that can also be downloaded as a spreadsheet after log in. If you don’t know yet what repository you will use, look for what is the recommended minimal information (i.e. “Minimum Information …your topic”, e.g. MIAME or MINSEQE or MIAPPE) required for your type of data in your community, or other metadata, at the following resources: . Research Data Alliance (RDA): Metadata Dictionary: Standards FAIRsharing.org at “Standards” and “Collections” The Digital Curation Centre (DCC): List of Metadata Standards . ",
      "url": "/pages/bedroesb/rdmkit/metadata_management.html#how-do-you-find-appropriate-standard-metadata-for-datasets-or-samples",
      "relUrl": "/metadata_management.html#how-do-you-find-appropriate-standard-metadata-for-datasets-or-samples"
    },"169": {
      "doc": "Documentation and metadata",
      "title": "How do you find appropriate vocabularies or ontologies?",
      "content": "Description . Vocabularies and ontologies are meant for describing concepts and relationships within a knowledge domain. Used wisely, they can enable both humans and computers to understand your data. There is no clear-cut division between the terms “vocabulary” and “ontology”, but the latter is more commonly used when dealing with complex (and perhaps more formal) collections of terms. There are many vocabularies and ontologies to be found on the web. Finding a suitable one can be both difficult and time-consuming. Considerations . Check whether you really need to find a suitable ontology or vocabulary yourself. Perhaps the repository where you are about to submit your data have recommendations? Or the journal where you plan to publish your results? Understand your goal with sharing data. Which formal requirements (by e.g. by funder or publisher) need to be fulfilled? Which parts of your data would benefit the most from adopting ontologies? Learn the basics about ontologies. This will be helpful when you search for terms in ontologies and want to understand how terms are related to one another. Accept that one ontology may not be sufficient to describe your data. It is very common that you have to combine terms from more than one ontology. Accept terms that are good enough. Sometimes you you cannot find a term that perfectly match what you want to express. Chosing the best available term is often better than not chosing a term at all. Note that the same concept may also be present in multiple ontologies. Solutions . Define a list of terms that you want to find ontologies for. Include in the list also any alternative term names that you are aware of. Search for your listed terms on dedicated web portals. These are a few: . Linked Open Vocabularies (LOV) EMBL-EBI Ontology Lookup Service Ontobee Schemapedia . ",
      "url": "/pages/bedroesb/rdmkit/metadata_management.html#how-do-you-find-appropriate-vocabularies-or-ontologies",
      "relUrl": "/metadata_management.html#how-do-you-find-appropriate-vocabularies-or-ontologies"
    },"170": {
      "doc": "Documentation and metadata",
      "title": "Documentation and metadata",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/metadata_management.html",
      "relUrl": "/metadata_management.html"
    },"171": {
      "doc": "Microbial biotechnology",
      "title": "Introduction",
      "content": "Microbial Biotechnology and DBTL cycle . The Microbial Biotechnology domain is a very broad field that encompasses the application of microorganisms to the development of useful products and processes. As such, there are a very wide variety of experimental tools, approaches, and ultimately data, that arise in this field. A convenient representation of microbial biotechnology for organisational purposes is the stages of the engineering life cycle drawn from the related field of synthetic biology. The Design-Build-Test-Learn (DBTL) cycle represents experimental design and analysis steps in synthetic biology. Current data management best practices and guidelines that should be applied throughout the DBTL cycle will be described and discussed here. Design . The design for a system in microbial biotechnology essentially involves two, interrelated exercises: (i) Identification of the biological entities/hosts that will be used to develop the product in question (ii) Identification of the genetic modifications/circuitry/constructs necessary to modify the host if appropriate. The design stage may also include optional approaches: (iii) Metabolic engineering of biosynthetic pathways (iv) Using mathematical modelling to aid the design of the system. Data management best practices and guidelines should be applied for each exercise and approach. The components of the design stage could be summarised as below. Biological host or organism. Synthetic parts. Metabolomic pathways and enzymes. Mathematical model for system design. Build . The build stage in the microbial biotechnology and/or synthetic biology life cycle is about building of the microbial systems and involves the application of any number of a range of experimental techniques. In this stage the synthetic parts are assembled and transformed into the biological host. The main aspects of the build stage are: . methods, protocols and procedures used to build the modified organism. Test . The test stage of a biotechnological study is the most variable in terms of the types of data produced. This stage is mostly about: . testing the outcome or output variables and analyse the modified organism. Characterising the synthetic parts using experimental data. Learn . The learning stage consists in interpreting the obtained results, share the acquired knowledge and reuse it in combination with other existing data to improve the creation of the modified organism. Publish and share data and results. Reuse existing data and combine it with new data. Feed data back into model(s) to inform the next iteration. Here, we adopt the stages of design, build and test, and their components or aspects, to categorise the various approaches available for the management of data in microbial biotechnology. Data management challenges . Ultimately, the ideal scenario is that data is captured in a standard format and then uploaded to a repository to ensure that it is Findable, Accessible, Interoperable and Reusable (FAIR). However, for the biotechnology field, data standards are still under development or missing completely and there are still gaps in database provision for some data types. Due to the interdisciplinary nature of the field, data arising from studies in microbial biotechnology relate to both computational studies, such as modelling and simulation, and the results of wet-lab based studies used for the construction and experimental characterisation of microbial systems. Given the breadth, scope and rapid development of the field of microbial biotechnology, this guide is by no means exhaustive. This guide is by no means comprehensive. Please get in touch with further suggestions for relevant standards and data sharing tools that can make it more complete. Sites such as Fairsharing can provide a wealth of data about standards that may be appropriate for a given data type and not mentioned in this brief guide. ",
      "url": "/pages/bedroesb/rdmkit/microbial_biotechnology.html#introduction",
      "relUrl": "/microbial_biotechnology.html#introduction"
    },"172": {
      "doc": "Microbial biotechnology",
      "title": "Design: Biological hosts - Metadata, ontologies and (meta)data publication",
      "content": "Description . Metadata standards and ontologies to capture the taxonomic and phenotypic data about the biological hosts or organism are still evolving, therefore finding and using a correct standard to describe the biological host can be challenging. It is recommended to publish and share information about biological hosts in dedicated public data repositories and databases. Considerations . The recording of taxonomic and genetic data must be considered carefully as part of the design stage. Metadata surrounding the host is essential, such as where it was isolated, growth conditions, recommended protocols etc. Genetic information relating to strains and any modifications needs to be kept track of as modifications are made. Note that capturing the metadata describing a genome sequence and its host is vitally important to facilitate further studies in comparative genomics and phenotypic analysis. To choose what are the appropriate repositories for your (meta)data, you should consider what kind of information about the host you are going to share, since each type of information could be published in a different repository. Species, taxonomy, strain. Phenotypic information. Genomic or nucleotide information. Solutions . Metadata schemas and ontologies . Current data standards to capture the taxonomic and phenotypic data are still evolving, with notable work on the Access to Biological Collection Data Schema (ABCD) and the activities of the Biodiversity Information Standards task force (TDWG). The Darwin Core standard from the (TDWG) is an appropriate standard to provide metadata about the taxonomic properties of a particular microorganism. The NCBI taxonomy homepage can also provide appropriate taxon IDs for recording taxonomic information. Information about proposed standardised nomenclature for prokaryotes can be found at the List of Prokaryotic names with Standing in Nomenclature (LPSN) (Parte et al., 2020). Data standards for recording the information about where a microorganism was isolated from do exist and this topic is covered in other RDMkit pages such as the marine metagenomics domain. Information can also be found in a publication by Ten Hoopen and colleagues (Ten Hoopen et al., 2015). The Environment Ontology is also relevant here to describe environmental entities of all kinds, from microscopic to intergalactic scales. A set of genetic nomenclature standards have been established by microbiologists and have been used for many years. These are still a useful way of communicating data about the genotype of a strain (Maloy and Hughes, 2007). Minimal information standards have been established to specify this metadata, such as the MIGS standard (Field et al., 2008). (Meta)Data publication and sharing . For sharing host information, you can use databases such as the Bacterial Diversity Metadatabase (Bacdive). You can also deposit strains and associated information in a strain repository such as the National Collection of Industrial, Food and Marine Bacteria (NCIMB) or the American Type Culture Collection (ATCC). There are also many organisations established for individual species of microorganisms, the Bacillus Genetic Stock Centre (BGSC) being one example. Databases such as CellRepo allow strains that have been barcoded to be tracked using a version control type system (Tellechea-Luzardo et al., 2020). Genomic information can be captured at the nucleotide level using the well-known European Nucleotide Archive standard (ENA) and submitted to the ENA database to allow the information to be shared. The database collection from the International Nucleotide Sequence Database Collaboration provides an umbrella for gathering and sharing a variety of sequence data from different sequence databases internationally. Other databases such as GenBank and the DNA Data Bank of Japan (DDBJ) also cater for sequence information. ",
      "url": "/pages/bedroesb/rdmkit/microbial_biotechnology.html#design-biological-hosts---metadata-ontologies-and-metadata-publication",
      "relUrl": "/microbial_biotechnology.html#design-biological-hosts---metadata-ontologies-and-metadata-publication"
    },"173": {
      "doc": "Microbial biotechnology",
      "title": "Design: Synthetic parts - Existing data, metadata collection and publication",
      "content": "Description . Appropriate and detailed description of the synthetic parts design is critical for reproducibility. It is important to consider how to record metadata at each point in the design process in a standard way, so that it can be clear to others and reproducible. Considerations . Format of designs may vary depending on the application, whether this be at the sequence level or an entire system. Consider existing management tools that can help visualise and modify genetic designs. Think about how the information about characterisation of genetic constructs assist in the selection of parts and modelling designs. At this stage, it may be desirable to assert which host the designed device is intended to express in and also the intended method of replication in the host - for example, cloned on a particular plasmid or integrated in the host chromosome. Solutions . Existing data . Sequences are characterised as parts which can be found with the assistance of various repositories such as: . iGEM Parts Registry The Joint BioEnergy Institute’s Inventory of Composable Elements (JBEI-ICE) (Ham et al., 2012) SynBioHub . Sequences can be isolated from standard genetic databases such as ENA and GenBank. Tools for metadata collection . You can manage the design stage using genetic computer aided design tools, such as Benchling for example, where information can be shared within small teams. Benchling supports a number of different data standards including FASTA, GenBank and SBOL1. Sometimes FASTA will be the most relevant format, for example when sending for DNA synthesis. Formats like GenBank, DICOM-SB (Sainz de Murieta, Bultelle and Kitney, 2016) or SBOL may be more applicable for instances where more information, such as functional annotation, would be useful to be shared. SBOL 2.0 and higher allows more than just the genetics of a system to be captured and shared. Using SBOL allows interactions between components in the design to be specified, information about RNA and proteins can be included and the provenance of a design can also be captured. Experimental information relating to the test and build of a system can also be captured and shared. SBOL data can be made using tools such as Benchling (SBOL1 only), SBOL Designer (Zhang et al., 2017) and ShortBOL to name but a few. A more comprehensive list of SBOL tools can be found on the sbolstandard website. More generally, The Investigation/Study/Assay (ISA) model can be used in systems biology, life sciences, environmental and biomedical domains to structure research outputs. The ISA-Tab format provides a framework for capturing these data in CSV files. Rightfield provides a mechanism for capturing metadata using easy to use spreadsheets. (Meta)Data publication and sharing . Once the design is complete, you can share this information via a repository such as: . iGEM Parts Registry SynBioHub JBEI-ICE Addgene . Much information about its performance can be included, varying from experimental results such as fluorescence curves to predicted performance based on modelling. It would be recommended to use standard figures that can be easily understood. SBOL-Visual is a good example of a graphical standard; it utilises standard shapes to represent different genetic parts which can help clarify a complex synthetic construct. SBOL-Visual can be crafted using tools such as VISBOL. Platforms such as SEEK, built on technologies such as ISA, support a large range of systems and synthetic biology projects. SEEK provides a web-based resource for sharing scientific research datasets, models or simulations, and processes. SEEK can be installed locally or FAIRDOMHub, a version of SEEK which is hosted by FAIRDOM, is available for general community use. ",
      "url": "/pages/bedroesb/rdmkit/microbial_biotechnology.html#design-synthetic-parts---existing-data-metadata-collection-and-publication",
      "relUrl": "/microbial_biotechnology.html#design-synthetic-parts---existing-data-metadata-collection-and-publication"
    },"174": {
      "doc": "Microbial biotechnology",
      "title": "Design: Metabolomic pathways and enzymes - Metadata, ontologies and (meta)data publication",
      "content": "Description . Here we describe some of the available options to accurately represent and store information about the designs of metabolic pathways and functional information about assays. Considerations . Enzymes have specific data standards that should be considered when accessing and recording their data. Solutions . Metadata and ontologies . SBOL allows information about the enzymes and the metabolic pathways to be captured in the design document and so this is a viable approach for sharing more than just the genetics of the system. Enzymes can be assigned EC numbers, according to the guidance from the International Union of Biochemistry and Molecular Biology (IUBMB), to indicate their function and an entry made in the BRaunschweig ENzyme DAtabase (BRENDA). More generally, the IUPAC-IUBMB Joint Commission on Biochemical Nomenclature (JCBN) encourages the communication of biochemical information using generally understood terminology. (Meta)Data publication . Databases such as SBOLME (Kuwahara et al., 2017) or SynBioHub can be used to share the data. Metabolite information can also be submitted to, or referred to in, ChEBI. BRaunschweig ENzyme DAtabase (BRENDA). ",
      "url": "/pages/bedroesb/rdmkit/microbial_biotechnology.html#design-metabolomic-pathways-and-enzymes---metadata-ontologies-and-metadata-publication",
      "relUrl": "/microbial_biotechnology.html#design-metabolomic-pathways-and-enzymes---metadata-ontologies-and-metadata-publication"
    },"175": {
      "doc": "Microbial biotechnology",
      "title": "Design: Mathematical model - Standards and (meta)data publication",
      "content": "Description . What tools and standards need to be considered when building mathematical models to aid the design of genetic systems? . How can the models be shared via repositories and made available in a way that makes results replicable? . Considerations . A variety of standards and tools are available for model building. It is important to associate the genetic design with its corresponding model. Solutions . Systems Biology Markup Language (SBML) is a popular standardised format for sharing mathematical models for which a variety of tools are available for model building. More generally, the COmputational Modeling in BIology NEtwork (COMBINE), provides a platform for coordinating standardisation of models in biology. SBOL can also be used to associate a genetic design with its corresponding model. Models can be shared in model repositories such as biomodels. ",
      "url": "/pages/bedroesb/rdmkit/microbial_biotechnology.html#design-mathematical-model---standards-and-metadata-publication",
      "relUrl": "/microbial_biotechnology.html#design-mathematical-model---standards-and-metadata-publication"
    },"176": {
      "doc": "Microbial biotechnology",
      "title": "Build: Methods - Documentation and (meta)data publication",
      "content": "Description . The build stage in the microbial biotechnology and/or synthetic biology life cycle involves the application of any number of a range of experimental techniques and, since these techniques are so varied, the domain is therefore very difficult to standardise in terms of the data and metadata to be shared. The current method of sharing information about the building of microbial systems is to write a detailed free text in the materials and methods section of a scientific paper. Considerations: . Capturing the information about the build process involves collecting the information arising from DNA amplification, DNA preparation and purification, primer design, restriction enzyme analysis, gel electrophoresis and DNA sequencing to name but a few techniques. If using a protein expression device, the intended vector for its replication in a given host will need to be named. The cloning strategy used to assemble the protein expression device and the vector will also need to be specified and shared. The information about how the “final system” was built is highly variable, depending on the DNA synthesis and/or assembly approach used. Consider ways to share this information. Solutions . Documentation . To the authors’ knowledge, there are no proposed standards that exist that are able to capture this diverse set of data. Currently, from a pragmatic point of view, the best a data manager can do is to make sure data is captured in some form from the lab scientist and grouped together with as much metadata as possible. The metadata standards for a build exercise are still to be defined and so at the discretion of the data manager. SBOL versions 2.0 and above provides a data standard that allows build data that has been grouped to be associated with design data for a part, device or system along with a minimal amount of metadata. Similarly, research object bundles, and more recently RO-Crates, can be used to gather together build data and test data with information about the overall study. (Meta)Data publication and sharing . The design information about the vector DNA or RNA sequence should be shared via public databases such as ENA or Genbank. Various DNA synthesis companies build DNA from a computer specification of the sequence and also a variety of experimental approaches for assembling DNA molecules. This information can be shared as free text attached to a design in SBOL format and uploaded to a repository that supports SBOL2 format and above such as SynBioHub. Once grouped together in a free form the data can be archived along with the metadata, collecting the data together in an archived form using a file compression format. The combine archive format may also be useful. ",
      "url": "/pages/bedroesb/rdmkit/microbial_biotechnology.html#build-methods---documentation-and-metadata-publication",
      "relUrl": "/microbial_biotechnology.html#build-methods---documentation-and-metadata-publication"
    },"177": {
      "doc": "Microbial biotechnology",
      "title": "Test: Outcome tests - Metadata standards and (meta)data publication",
      "content": "Description . The test stage of a biotechnological study is the most variable in terms of the types of data produced. The types of experiments carried out to test a microbial system are highly dependent on the intended function of the system under construction. Some common approaches include at the simplest level, characterising the growth of an organism at various scales in different growth regimes and assaying the production of desired product. The data arising from assays for product development is highly variable and beyond the scope of this short guide, however we propose some recommended resources. Considerations . What types of experiments, e.g. organism growth, organism characterisation, will you undertake to test your microbial system? What types of data result from those experiments? Will you combine multi-omics assays in your study? . Is there a reporting guideline for the type of you are generating? Will you reuse existing testing protocols or generate and share your own protocols? . Since (meta)data repositories often require compliance to their metadata standards, ontologies and file formats, it is recommended to be aware of those requirements when describing the test stage. Solutions . Metadata standards . Minimum Information Standard for Engineered Organism Experiments (MIEO). Minimal information necessary to record the growth of an organism in culture, has been described by Hect and colleagues (Hecht et al., 2018). Enzyme. If your product is a protein such as an enzyme then some standards developed by the Standards for Reporting Enzyme Data (STRENDA) Consortium may be helpful (‘Standards for Reporting Enzyme Data: The STRENDA Consortium: What it aims to do and why it should be helpful’, 2014). Microscopy. Microscopy is often also used to characterise the behaviour of engineered microorganisms. Standards such as the Open Microscopy Environment Ontology and the Cellular Microscopy Phenotype Ontology (CMPO) can help provide standardised metadata terms. Flow Cytometry data. The International Society for the Advancement of Cytometry (ISAC) provides information on a variety of appropriate data standards for capturing Flow Cytometry data (used to characterise microbial populations at a single cell level) (Spidlen et al., 2021). Nucleic acids information. The ENA, amongst others, provides guidance on the metadata for RNAseq datasets. Proteomics. HUPO proteomics standards initiative provides a range of guidance for capturing and sharing proteomics data. (Meta)Data publication and sharing . Protocols. Protocols used for testing can be shared using platforms such as: . protocols.io. iGEM engineering hub, which also provides some guidance for a variety of data capture protocols and standardised units. Images. Images can be shared with the community by repositories such as the Image Data Resource (IDR). Nucleic acids information. Information about nucleic acids can be shared via . ENA GEO ArrayExpress . Proteomics. Proteomics data can be shared via HUPO proteomics standards initiative. Metabolic studies. Metabolomic studies can be shared through the Metabolome Exchange Database, which provides a resource for sharing data from metabolic studies and guidance for the submission of metabolome data. Biological sources. Information about biological sources can be shared via the BioStudies database, which has been set up to capture and share information about multi-omics and other biological studies (Sarkans et al., 2018). ",
      "url": "/pages/bedroesb/rdmkit/microbial_biotechnology.html#test-outcome-tests---metadata-standards-and-metadata-publication",
      "relUrl": "/microbial_biotechnology.html#test-outcome-tests---metadata-standards-and-metadata-publication"
    },"178": {
      "doc": "Microbial biotechnology",
      "title": "Bibliography",
      "content": "Field, D. et al. (2008) ‘The minimum information about a genome sequence (MIGS) specification’, Nature biotechnology, 26(5), pp. 541–547. doi: 10.1038/nbt1360. Ham, T. S. et al. (2012) ‘Design, implementation and practice of JBEI-ICE: an open source biological part registry platform and tools’, Nucleic acids research, 40(18), p. e141. doi: 10.1093/nar/gks531. Hecht, A. et al. (2018) ‘A minimum information standard for reproducing bench-scale bacterial cell growth and productivity’, Communications biology, 1, p. 219. doi: 10.1038/s42003-018-0220-6. Kuwahara, H. et al. (2017) ‘SBOLme: a Repository of SBOL Parts for Metabolic Engineering’, ACS synthetic biology, 6(4), pp. 732–736. doi: 10.1021/acssynbio.6b00278. Maloy, S. R. and Hughes, K. T. (2007) ‘Strain Collections and Genetic Nomenclature’, Methods in Enzymology, pp. 3–8. doi: 10.1016/s0076-6879(06)21001-2. Parte, A. C. et al. (2020) ‘List of Prokaryotic names with Standing in Nomenclature (LPSN) moves to the DSMZ’, International journal of systematic and evolutionary microbiology, 70(11), pp. 5607–5612. doi: 10.1099/ijsem.0.004332. Sainz de Murieta, I., Bultelle, M. and Kitney, R. I. (2016) ‘Toward the First Data Acquisition Standard in Synthetic Biology’, ACS synthetic biology, 5(8), pp. 817–826. doi: 10.1021/acssynbio.5b00222. Sarkans, U. et al. (2018) ‘The BioStudies database—one stop shop for all data supporting a life sciences study’, Nucleic Acids Research, pp. D1266–D1270. doi: 10.1093/nar/gkx965. Spidlen, J. et al. (2021) ‘Data File Standard for Flow Cytometry, Version FCS 3.2’, Cytometry. Part A: the journal of the International Society for Analytical Cytology, 99(1), pp. 100–102. doi: 10.1002/cyto.a.24225. ‘Standards for Reporting Enzyme Data: The STRENDA Consortium: What it aims to do and why it should be helpful’ (2014) Perspectives in Science, 1(1-6), pp. 131–137. doi: 10.1016/j.pisc.2014.02.012. Tellechea-Luzardo, J. et al. (2020) ‘Linking Engineered Cells to Their Digital Twins: A Version Control System for Strain Engineering’, ACS synthetic biology, 9(3), pp. 536–545. doi: 10.1021/acssynbio.9b00400. Ten Hoopen, P. et al. (2015) ‘Marine microbial biodiversity, bioinformatics and biotechnology (M2B3) data reporting and service standards’, Standards in genomic sciences, 10, p. 20. doi: 10.1186/s40793-015-0001-5. Zhang, M. et al. (2017) ‘SBOLDesigner 2: An Intuitive Tool for Structural Genetic Design’, ACS synthetic biology, 6(7), pp. 1150–1160. doi: 10.1021/acssynbio.6b00275. ",
      "url": "/pages/bedroesb/rdmkit/microbial_biotechnology.html#bibliography",
      "relUrl": "/microbial_biotechnology.html#bibliography"
    },"179": {
      "doc": "Microbial biotechnology",
      "title": "Microbial biotechnology",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/microbial_biotechnology.html",
      "relUrl": "/microbial_biotechnology.html"
    },"180": {
      "doc": "NeLS",
      "title": "What is the NeLS data management tool assembly?",
      "content": "The Norwegian e-Infrastructure for Life Sciences (NeLS) is an infrastructure provided by ELIXIR Norway. NeLS provides necessary tools for Data Management and covers Planning, Processing, Analysing and Sharing Data Life Cycle stages and offers Data Storage capacities. ",
      "url": "/pages/bedroesb/rdmkit/nels_assembly.html#what-is-the-nels-data-management-tool-assembly",
      "relUrl": "/nels_assembly.html#what-is-the-nels-data-management-tool-assembly"
    },"181": {
      "doc": "NeLS",
      "title": "Who can use the NeLS data management tool assembly?",
      "content": "NeLS and the underlying infrastructure are accessible for researchers in Norway and their collaborators. Eligible researchers can apply for storage quotas and get support through the National (Norwegian) bioinformatics support desk contact@bioinfo.no. Most of the tools in NeLS are open source and can be reused. ",
      "url": "/pages/bedroesb/rdmkit/nels_assembly.html#who-can-use-the-nels-data-management-tool-assembly",
      "relUrl": "/nels_assembly.html#who-can-use-the-nels-data-management-tool-assembly"
    },"182": {
      "doc": "NeLS",
      "title": "For what can you use the NeLS data management tool assembly?",
      "content": "Figure 1. The Norwegian e-Infrastructure for Life Sciences (NeLS) Data Management tool assembly. You can access all tools in NeLS using the the national solution for secure login and data sharing in the educational and research sector FEIDE, when coupled with ELIXIR AAI. The NeLS Data Management tool assembly provides support with Data Management Planning through an instance of the Data Steward Wizard following the guidelines of the major national and European funding bodys. Dedicated references guide you through national infrastructure, resources, laws and regulations and also include the Tryggve ELSI Checklist for Ethical, Legal and Social Implications. Soon you will be able to submit storage request forms for Data Storage in NeLS with defined access permissions through the Data Stewardship Wizard. Data Storage is the core functionality of NeLS and builds upon a 3 layer tiered system: the first layer is intended for short-term storage when computing, processing and analysing data; the second layer of medium capacity (NeLS) is intended for sharing and storing active research data, while the third layer (StoreBioinfo) of high capacity is intended for longer storage until end of a project. Data in the second (NeLS) layer is protected against hardware failure on disk or server level and snapshots of the data are kept for 4 weeks. The third layer is implemented on top of the national research data storage solutions operated by Sigma2 Uninett A/S and is protected against data loss by snapshots and geo-replication. National Norwegian research infrastructures, such as the Norwegian sequencing infrastructure NorSeq can directly upload data to your NeLS project for you. For Processing and Analysing your data, the NeLS Data Management tool assembly provides access to a national instance of Galaxy with ~2000 tools. Data stored in NeLS is directly available within this Galaxy instance, hence you do not need to keep local copies of your data. In order to help you keeping track of metadata, NeLS is integrated with the SEEK web-based cataloguing and sharing platform. You can use any instance of SEEK such as the public FAIRDOMHub to manage metadata associated with your data stored in NeLS and access the data through SEEK. SEEK uses the ISA (Investigation, Study, Assay) structure to organise your data and recommended minimal information such as sample characteristics, technologies, measurements and relationships between samples, data and models. Public SEEK instances like the FAIRDOMHub can also be used to collaborate on data and to share them publicly. If you are doing modelling, you can also use the inbuilt JWS Online simulator for your SBML models. One recommended way to share your data is to deposit them in the ELIXIR Deposition Databases for Biomolecular Data. The NeLS Data Management tool assembly will soon offer tools to help you with the deposition step for data stored in NeLS. ",
      "url": "/pages/bedroesb/rdmkit/nels_assembly.html#for-what-can-you-use-the-nels-data-management-tool-assembly",
      "relUrl": "/nels_assembly.html#for-what-can-you-use-the-nels-data-management-tool-assembly"
    },"183": {
      "doc": "NeLS",
      "title": "NeLS",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/nels_assembly.html",
      "relUrl": "/nels_assembly.html"
    },"184": {
      "doc": "News",
      "title": "News",
      "content": ". | RDMkit nominated for NFDI4Ing Community Award 27 September 2021 . RDMkit was nominated for the NFDI4Ing Community Award beside other tools at the German NFDI4Ing conference. | BioData.pt TALKS: RDM Toolkit 20 September 2021 . BioData.pt talks comprise a series of monthly webinars given by BioData.pt collaborators and guest experts. The webinar on 2nd November 2021 aims to provide an introduction on the use and features of RDMkit. More information about the event . | Open Science FAIR 20 September 2021 . 3rd Open Science FAIR Conference, an international event for all topics related to Open Science OS Fair 2021 aims to bring together and empower open science communities and services; to identify common practices related to open science; to see what are the best synergies to deliver and operate services that work for many; and to bring experiences from all around the world and learn from each other. - Program zoom links for panel sessions - Plenaries on youtube . | . For more news please visit our news page. ",
      "url": "/pages/bedroesb/rdmkit/news.html",
      "relUrl": "/news.html"
    },"185": {
      "doc": "Norway",
      "title": "Introduction",
      "content": "This page provides a set of necessary information, giving an overview of the data management toolkit and it is aimed at the current and future scientists in Norway and their collaborators, in life sciences areas. Norway has released a National strategy on access to and sharing of research data in 2018. ",
      "url": "/pages/bedroesb/rdmkit/no_resources.html#introduction",
      "relUrl": "/no_resources.html#introduction"
    },"186": {
      "doc": "Norway",
      "title": "Funder policies on research data in Norway",
      "content": "List of funders with Data Management Policies in Norway: . Forskningsrådet (also known as Research Council Norway) . ",
      "url": "/pages/bedroesb/rdmkit/no_resources.html#funder-policies-on-research-data-in-norway",
      "relUrl": "/no_resources.html#funder-policies-on-research-data-in-norway"
    },"187": {
      "doc": "Norway",
      "title": "Institutional policies on research data in Norway",
      "content": "We provide here a list of higher education institutions with Data Management Policies in Norway: . Norwegian University of Life Sciences Norwegian university of science and technology University of Bergen University of Oslo The Arctic University of Norway . ",
      "url": "/pages/bedroesb/rdmkit/no_resources.html#institutional-policies-on-research-data-in-norway",
      "relUrl": "/no_resources.html#institutional-policies-on-research-data-in-norway"
    },"188": {
      "doc": "Norway",
      "title": "Acts and Laws Relevant to Life Sciences Research and Data in Norway",
      "content": "Privacy . Personvernforordningen Forskrift om behandling av personopplysninger Overgangsregler om behandling av personopplysninger . Health Research . Health Register Act Lov om helseregistre og behandling av helseopplysninger (helseregisterloven) Lov om medisinsk og helsefaglig forskning (helseforskningsloven) Forskrift om organisering av medisinsk og helsefaglig forskning Merknader til forskrifter til Helseforskningsloven Veileder til Helseforskningsloven Forskrift om befolkningsbaserte helseundersøkelser Lov om helsepersonell mv (helsepersonelloven) Lov om pasient- og brukerrettigheter (pasient- og brukerrettighetsloven) Lov om legemidler mv (legemiddelloven) Forskrift om klinisk utprøving av legemidler til mennesker Lov om humanmedisinsk bruk av bioteknologi mm (bioteknologiloven) Other laws of relevance in this context . Lov om arkiv [arkivlova] Lov om organisering av forskningsetisk arbeid * (forskningsetikkloven) Lov om endringer i patentloven mv. (forenklinger) Lov om opphavsrett til åndsverk mv. (åndsverkloven) Lov om universiteter og høyskoler (universitets- og høyskoleloven) Lov om nasjonal sikkerhet (sikkerhetsloven) . ",
      "url": "/pages/bedroesb/rdmkit/no_resources.html#acts-and-laws-relevant-to-life-sciences-research-and-data-in-norway",
      "relUrl": "/no_resources.html#acts-and-laws-relevant-to-life-sciences-research-and-data-in-norway"
    },"189": {
      "doc": "Norway",
      "title": "Regulations",
      "content": "Privacy . Datatilsynet: Journalistiske, akademiske, kunstneriske og litterære formål Atferdsnormer Informasjonssikkerhet og internkontroll Health research . e-helse Direktoratet: “Normen”: Norm for helsedata . Institutional guidelines of Personal Data . Norwegian University of Life Sciences Privacy in Research guidlines, Guidlines onf health research Norwegian university of science and technology University of Bergen University of Oslo The Arctic University of Norway . ",
      "url": "/pages/bedroesb/rdmkit/no_resources.html#regulations",
      "relUrl": "/no_resources.html#regulations"
    },"190": {
      "doc": "Norway",
      "title": "Domain-specific infrastructures/resources",
      "content": "Following resources and tools could be useful to implement data management practices, relevant to your research area/topic: . Norwegian node of the European genome-phenome archive for sensitive human (genetic) data National Norwegian services for sensitive (personal) data - Tool Assembly RDMkit Page Covid-19 Data Portal Marine metagenomics Portal - Tool Assembly RDMkit Page . ",
      "url": "/pages/bedroesb/rdmkit/no_resources.html#domain-specific-infrastructuresresources",
      "relUrl": "/no_resources.html#domain-specific-infrastructuresresources"
    },"191": {
      "doc": "Norway",
      "title": "Norway",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/no_resources.html",
      "relUrl": "/no_resources.html"
    },"192": {
      "doc": "OMERO",
      "title": "What is OMERO?",
      "content": "OMERO is a software platform for managing, sharing and analysing images data. OMERO supports over proprietary 150 file formats, including all major microscopy formats, medical images, digital pathology images, high content screening, etc. using Bio-Formats. Bio-Formats is a Java software tool for reading proprietary image data and metadata and writing image data using standardized open formats. OMERO handles all your images in a secure central repository. Users can view, organize, analyze and share data from anywhere via the internet. Users can work with image data and metadata from a Desktop application, from the Web or from 3rd party tools e.g. Fiji. OMERO stores image metadata in a relational database and offers a more flexible structure based on HDF5 to store for example, analytical results. This allows analytical results generated by 3rd party softwares e.g. CellProfiler, ilastik, etc. to be stored alongside the images. Schematic overview of the OMERO tool assembly. ",
      "url": "/pages/bedroesb/rdmkit/omero_assembly.html#what-is-omero",
      "relUrl": "/omero_assembly.html#what-is-omero"
    },"193": {
      "doc": "OMERO",
      "title": "Who is OMERO intended for?",
      "content": "OMERO is designed to be an institutional repository. It offers a secure central way for scientists, researchers and data stewards to handle their imaging data. All the image data from a facility can be securely stored and managed, using group permissions and user roles to allow controlled access tailored to your institution. From private repositories for sensitive data to hosting public data for your website and latest publications, the permissions model is designed to meet the range of researchers’ needs. OMERO is tried and tested in hundreds of institutions world-wide, with extensive installation and configuration documentation for system administrators and community support via dedicated mailing lists and forums. The OMERO platform uses a Group/User permission system. ​​The degree to which their data is available to other members of the group depends on the permissions settings for that group. Whenever a user logs on to an OMERO server, they are connected under one of their groups. All data they import and any work that is done is assigned to the current group, however the user can move their data into another group. Users require login credentials to access the system. OMERO also supports the use of an LDAP server. OMERO is an open-source system designed to be extended and integrated with other tools. The OMERO API allows clients to be written in Java, Python, C++ or MATLAB. The OMERO platform includes a Java client OMERO.insight, a Python-based web client OMERO.web, a Command Line Interface which uses Python, and a Java ImageJ plugin. OMERO also supports a scripting service which allows Python scripts to be run on the server and called from any of the other clients. A vast amount of documentation and code examples for scientists, developers and system administrators are available. A demo server maintained by the OME team is also available for users wishing to try out, request an account. 3rd party tools and OMERO ",
      "url": "/pages/bedroesb/rdmkit/omero_assembly.html#who-is-omero-intended-for",
      "relUrl": "/omero_assembly.html#who-is-omero-intended-for"
    },"194": {
      "doc": "OMERO",
      "title": "Which tasks can be solved with OMERO?",
      "content": "OMERO can be used for the day-to-day data management of data. Users can remotely view, handle metadata, analyze, generate figures ready for publication, etc. The plaform can also be used to publish data either using public repository like Image Data Repository (IDR) or by enabling the public user within the OMERO installation in a given institution e.g. Liverpool CCI gallery. OMERO is the platform between several public image repositories e.g. Image Data Repository, EMPIAR . OMERO is also used as a teaching platform by several institutions e.g. University of Dundee, Harvard Medical school. ",
      "url": "/pages/bedroesb/rdmkit/omero_assembly.html#which-tasks-can-be-solved-with-omero",
      "relUrl": "/omero_assembly.html#which-tasks-can-be-solved-with-omero"
    },"195": {
      "doc": "OMERO",
      "title": "OMERO",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/omero_assembly.html",
      "relUrl": "/omero_assembly.html"
    },"196": {
      "doc": "Outreach",
      "title": "Presentations",
      "content": "RDMkit ELIXIR All Hands DM-Symposium . ",
      "url": "/pages/bedroesb/rdmkit/outreach.html#presentations",
      "relUrl": "/outreach.html#presentations"
    },"197": {
      "doc": "Outreach",
      "title": "Videos",
      "content": "ELIXIR Webinar: Research Data Management Kit (RDMKit) . Promotion video for RDMkit users . Promotion video for RDMkit contributors . ",
      "url": "/pages/bedroesb/rdmkit/outreach.html#videos",
      "relUrl": "/outreach.html#videos"
    },"198": {
      "doc": "Outreach",
      "title": "Outreach",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/outreach.html",
      "relUrl": "/outreach.html"
    },"199": {
      "doc": "Planning",
      "title": "What is Data Management Planning?",
      "content": "Data Management Planning consists of defining the strategy that you plan to use for managing data and documentation generated within the project. It is about thinking upfront what’s the best way to avoid problems or unexpected costs related to data management, and set the conditions for your research data to achieve the highest possible impact in science, even after the end of the project. Solutions regarding the handling of the data generated within a project is usually formalised in a Data Management Plan (DMP). A DMP is a document describing several aspects of the data management process which occur before, during and after the end of a project. Common components of a DMP are: . General information about the project. Description of the datasets that will be used and generated. Use of metadata, ontologies and the way data documentation will be provided. Storage solutions, data security and preservation strategy during and after the project. Sharing of the data. Costs and resources needed for data management. Ethical and legal issues, such as privacy, intellectual property and licences. ",
      "url": "/pages/bedroesb/rdmkit/planning.html#what-is-data-management-planning",
      "relUrl": "/planning.html#what-is-data-management-planning"
    },"200": {
      "doc": "Planning",
      "title": "Why is Data Management Planning important?",
      "content": "It is good research practice to take care of your research data and have a data management plan. It will make your work more efficient, facilitate team work and use of services and tools. Moreover, a detailed DMP would help in making your research data more FAIR. Advantages of making a data management plan: . It is often a requirement of research organisations and funders. It helps to plan and budget necessary resources and equipment. It defines roles and responsibilities in data management among the project team. It helps to identify risks in data handling and apply solutions at early stage. It facilitates data sharing, reuse and preservation. ",
      "url": "/pages/bedroesb/rdmkit/planning.html#why-is-data-management-planning-important",
      "relUrl": "/planning.html#why-is-data-management-planning-important"
    },"201": {
      "doc": "Planning",
      "title": "What should be considered for Data Management Planning?",
      "content": ". Research organisation and funders often require a DMP as part of the application for grants or latest, when the project is funded. Therefore, consider guidelines, policies and tools for data management planning required by your funder. Data management should be planned in the early stages of a research project. Preferably, the DMP should be filled in before starting data collection. However, the DMP is a living document and should be updated as the research project progresses to match e.g. an update of the infrastructures, research softwares or a novel collaboration. Consider standards or best practices required by facilities and infrastructures that you plan to use. Due to the variety of aspects that need to be addressed in a DMP, it is better to find recommendations and obtain help from your institution support services, such as IT department, library, data managers or data stewards, legal or tech transfer team and data protection officer. Explore best practices, guidelines, tools and resources for research data management described in this website. ",
      "url": "/pages/bedroesb/rdmkit/planning.html#what-should-be-considered-for-data-management-planning",
      "relUrl": "/planning.html#what-should-be-considered-for-data-management-planning"
    },"202": {
      "doc": "Planning",
      "title": "Planning",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/planning.html",
      "relUrl": "/planning.html"
    },"203": {
      "doc": "Plant Genomics",
      "title": "What is the plant genomics tool assembly?",
      "content": "The plant genomics tool assembly is a toolkit for managing plant genomics and genotyping data throughout their life cycle, with a particular focus on ensuring traceability of the biological material to enable interoperability with plant phenotyping data. To enable this, the same persistent identifiers must be used in both the genotyping and phenotyping experiments. It is recommended that the biological plant material is accurately described using rich metadata and stored in a central repository. The tool assembly also provides guidance on how users should structure their analysis results in the form of VCF files to achieve a higher degree of interoperability. ",
      "url": "/pages/bedroesb/rdmkit/plant_genomics_assembly.html#what-is-the-plant-genomics-tool-assembly",
      "relUrl": "/plant_genomics_assembly.html#what-is-the-plant-genomics-tool-assembly"
    },"204": {
      "doc": "Plant Genomics",
      "title": "Who can use the plant genomics tool assembly?",
      "content": "This tool assembly can be used by any researcher producing plant genomic or genotyping data interested in ensuring their data complies with the FAIR principles. ",
      "url": "/pages/bedroesb/rdmkit/plant_genomics_assembly.html#who-can-use-the-plant-genomics-tool-assembly",
      "relUrl": "/plant_genomics_assembly.html#who-can-use-the-plant-genomics-tool-assembly"
    },"205": {
      "doc": "Plant Genomics",
      "title": "How can you access the plant genomics tool assembly?",
      "content": "All the components of this tool assembly are publicly available, but most require registration. So anyone can access the tool assembly provided they register for each tool that requires it. ",
      "url": "/pages/bedroesb/rdmkit/plant_genomics_assembly.html#how-can-you-access-the-plant-genomics-tool-assembly",
      "relUrl": "/plant_genomics_assembly.html#how-can-you-access-the-plant-genomics-tool-assembly"
    },"206": {
      "doc": "Plant Genomics",
      "title": "For what purpose can you use the plant genomics tool assembly?",
      "content": "Figure 1. The plant genomics tool assembly. Metadata collection and tracking . Accurate documentation of the plant biological materials and samples is critical for interoperability, and should comply with the MIAPPE standard. This information should be submitted to BioSamples, with MIAPPE compliance validated using BioSamples’ plant-miappe.json template available on the sample validation page. Submission of sample descriptions to BioSamples can be done as early as the data collection stage, but at the latest, must acompany submission of the genomic data to the European Nucleotide Archive (ENA) or of genotyping data to the European Variation Archive (EVA). The complete timeline for submitting plant biological material to BioSamples and resulting genotyping experiment results to ENA and EVA should look like this: . Register plant biological material information to BioSamples Submit Sequencing reads to ENA (using BioSamples IDs to identify material) Check if used reference genome assembly is INSDC available (GCF / GCA accesion number available) . If yes proceed to submit VCF at step 4, if no proceed to step 3 b Submit reference genome assembly to INSDC (NCBI Genbank / EBML-EBI ENA / DDBJ) and wait until accession number is issued, then proceed to step 4 . Submit VCF file to EVA (using BioSamples IDs to identify material, GCF/GCA accession for the reference genome assembly) . Note: Metadata associated with a single sample registered with BioSamples can only be updated from the original account. e!DAL-PGP, FAIRDOM-SEEK instances such as FAIRDOMHub or Data INRAE can be used to manage and share experimental metadata, as well as data. Data processing and analysis . Reference genomes for genome assembly and annotation should be obtained from ENSEMBL Plants or PLAZA, if available. Genetic variant data must be produced in the VCF format, and validated using the EVA vcf-validator (https://github.com/EBIvariation/vcf-validator). Please note to only use identifiers of sequences that match the reference genome assembly identifiers. In order to ensure interoperability of VCF files, the VCF meta-information lines should be used: see the Plant sciences page for more details. Data sharing and publishing . All sequencing data collected in plant genotyping experiments should be submitted to ENA together with metadata compliant to the GSC MIxS plant associated checklist. Final results of such studies in the form of VCF files should be submitted to EVA. Additionally, supplemental data complementing these two data types is encouraged to be submitted to e!DAL-PGP or Data INRAE. ",
      "url": "/pages/bedroesb/rdmkit/plant_genomics_assembly.html#for-what-purpose-can-you-use-the-plant-genomics-tool-assembly",
      "relUrl": "/plant_genomics_assembly.html#for-what-purpose-can-you-use-the-plant-genomics-tool-assembly"
    },"207": {
      "doc": "Plant Genomics",
      "title": "Plant Genomics",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/plant_genomics_assembly.html",
      "relUrl": "/plant_genomics_assembly.html"
    },"208": {
      "doc": "Plant sciences",
      "title": "Introduction",
      "content": "The plant science domain includes studying the adaptation of plants to their environment, with applications ranging from improving crop yield or resistance to environmental conditions, to managing forest ecosystems. Data integration and reuse are facilitators for understanding the play between genotype and environment to produce a phenotype, which requires integrating phenotyping experiments and genomic assays made on the same plant material, with geo-climatic data. Moreover, cross-species comparisons are often necessary to understand the mechanisms behind phenotypic traits, especially at the genotypic level, due to the gap in genomic knowledge between well-studied plant species (namely Arabidopsis) and newly sequenced ones. The challenges to data integration stem from the multiple levels of heterogeneity in this domain. It encompasses a variety of species, ranging from model organisms, to crop species, to wild plants such as forest trees. These often need to be detailed at infra-specific levels (e.g. subspecies, variety), but naming at these levels sometimes lacks consensus. Studies can take place in a diversity of settings including indoor (e.g. growth chamber, greenhouse) and outdoor settings (e.g. cultivated field, forest) which differ fundamentally on the requirements and manner of characterizing the environment. Phenotypic data can be collected manually or automatically (by sensors and drones), and be very diverse in nature, spanning physical measurements, the results of biochemical assays, and images. Some omics data can be considered as well as molecular phenotypes (e.g. transcriptome, metabolomes, …). Thus the extension and depth of metadata required to describe a plant experiment in a FAIR-compliant way is very demanding for researchers. Another particularity of this domain is the absence of central deposition databases for certain important data types, in particular data deriving from plant phenotyping experiments. Whereas datasets from plant omics experiments are typically deposited in global deposition databases for that type of experiment, those from phenotyping experiments remain in institutional or at best national repositories. This makes it difficult to find, access and interconnect plant phenotyping data. ",
      "url": "/pages/bedroesb/rdmkit/plant_sciences.html#introduction",
      "relUrl": "/plant_sciences.html#introduction"
    },"209": {
      "doc": "Plant sciences",
      "title": "Plant phenotyping metadata management",
      "content": "Description . It is recommended that metadata collection is contemplated from the start of the experiment and that the working environment facilitates (meta)data collection, storage, and validation throughout the project. Detailed metadata needs to be captured on a number of aspects. One of the most critical is the description of the biological materials used in the study—the plant varieties and, when applicable, the seed lots or the parent plants—as they are the key to integrating omics and phenotyping datasets. Particularly in field studies, it is equally critical to record the geographical coordinates and time of the experiment, for linkage with geo-climatic data. In growth chamber or greenhouse studies, the environmental conditions that were fixed should be described in detail. Considerations . Is your plant material provided by a genebank or derived from material provided by a genebank? Have you documented your phenotyping assays (trait, method, units) both for direct measures (data collection) and computed data (after data processing or analysis)? . Is there an existing Crop Ontology for the species you experiment and does it describe your assay? . Do you have your own system to collect data and is it compliant with the MIAPPE standard? . Solutions . The metadata standard applicable to plant phenotyping experiments is MIAPPE. It contains a section dedicated to the identification of plant biological materials that follows the general standard for the identification of plant genetic resources, The Multi-Crop Passport Descriptors (MCPD). It is also possible to describe samples that were collected from the plants chosen for specific phenotyping assays. For woody plants, particularly those in forest settings, it is common to use GPS coordinates as a unique identifier for plant material. There is a section to describe the phenotyping assays based on the Crop Ontology recommendations. There is section describing the type of experiment (greenhouse, field, etc…) and it is advisable to collect the location (geographical coordinates) and time where it was performed for linkage with geo-climatic data. Tools and resources for data collection and management: . The ISA-Tools also include a configuration for MIAPPE and can be used both for filling-in metadata and for validating. SEEK and Dataverse are free data management platforms for which MIAPPE templates are in development. COPO is a data management platform specific for the plant sciences. FAIRsharing is a manually curated registry of reporting guidelines, vocabularies, identifier scheme, models, formats, repositories, knowledgebases, and data policies that includes many resources relevant for managing plant phenotyping data. Validation of MIAPPE compliance can be done via ISA-Tools or upon data deposition in a Breeding API (BrAPI) compliant repository. ",
      "url": "/pages/bedroesb/rdmkit/plant_sciences.html#plant-phenotyping-metadata-management",
      "relUrl": "/plant_sciences.html#plant-phenotyping-metadata-management"
    },"210": {
      "doc": "Plant sciences",
      "title": "Plant phenotyping data sharing and deposition",
      "content": "Description . Archiving, sharing, and publication of plant phenotyping data can be challenging, given that there is no global centralized archive for this type of data. Research projects often involve multiple partners, some of which collate data into their own (institutional) data management platforms, whereas others collate data into Excel spreadsheets. For researchers, it would be highly desirable that the datasets collected in different media by the partners in a research project (or across different collaborative projects) can be shared in a way that enables their integration, for collective analysis and for facilitating deposition into a dedicated repository. For managers of plant phenotyping data repositories that support a project or institution, it is essential to ensure that the uptake of data is easy and includes a step of metadata validation upon intake. Considerations . Are you exchanging data with individual researchers? . In what media are data being collected? Are the data described in a MIAPPE-compliant manner? . Are you exchanging data across different data management platforms? . Do these platforms implement BrAPI? If not, are they MIAPPE-compliant and do they enable automated data exchange? . Solutions . If you or your partners collect data manually, it is critical to adopt a spreadsheet template that is compatible with the structure of the database that will be used for data deposition. This template should make use of tools for handling ontology annotations in a spreadsheet, such as OntoMaton If the database is MIAPPE compliant, you can use the MIAPPE-compliant spreadsheet template . If you or your partners collect data into data management platforms: . If it implements BrAPI, you can exchange data using BrAPI calls. If it doesn’t implement BrAPI, the simplest solution would be to export data into the MIAPPE spreadsheet template, or another formally defined data template. For data deposition, it is highly recommended that you opt for one of the many repositories that implement BrAPI, as they enhance findability through the ELIXIR plant data discovery service, FAIDARE, enable machine actionable access to MIAPPE compliant data and validation of that compliance. ",
      "url": "/pages/bedroesb/rdmkit/plant_sciences.html#plant-phenotyping-data-sharing-and-deposition",
      "relUrl": "/plant_sciences.html#plant-phenotyping-data-sharing-and-deposition"
    },"211": {
      "doc": "Plant sciences",
      "title": "Plant genotyping data sharing and deposition",
      "content": "Solutions . In order to ensure interoperability of VCF files, the following VCF meta-information lines should be used: . Obligatory meta-information line : . ##fileformat : file format. Examples: ##fileformat=VCFv4.3 ##fileformat=VCFv4.1 . Recommended meta-information lines : . ##bioinformatics_source (URL or URI): Analytic approach usually consisting of chains of bioinformatics tools for creating the VCF file specified as the DOI of a publication, or more generally as URL/URI, like a public repository for the scripts used. Examples: ##bioinformatics_source=”doi.org/10.1038/s41588-018-0266-x” ##bioinformatics_source=”doi.org/10.3389/fpls.2021.628439” . ##reference_ac (assembly_accession): accession number, including the version, of the reference sequence on which the variation data of the present VCF is based. Examples: ##reference_ac=GCA_902498975.1 ##reference_ac=GCA_000005005.5 . ##reference_url (DOI): a DOI (or URL/URI) for downloading of this reference genome, preferably from one INSDC archive. Examples: ##reference_url=”ftp.ncbi.nlm.nih.gov/genomes/all/GCA/902/498/975/GCA_902498975.1_Morex_v2.0/GCA_902498975.1_Morex_v2.0_genomic.fna.gz” ##reference_url=”ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/005/005/GCA_000005005.5_B73_RefGen_v3/GCA_000005005.5_B73_RefGen_v3_genomic.fna.gz” . ##contig (&lt;ID=ctg1, length=sequence_length, assembly=gca_accession, md5=md5_hash, species=species_of_interest&gt;) : The individual sequence(s) of the reference genome Examples: ##contig=&lt;ID=chr1H,length=522466905,assembly=GCA_902498975.1,md5=8d21a35cc68340ecf40e2a8dec9428fa,species=\"Hordeum vulgare\"&gt; ##contig=&lt;ID=GK000031.3,length=301433382,assembly=GCA_000005005.5,md5=74dfe85ad898416814fa98e8d7048f76,species=”Zea mays”&gt; . ##SAMPLE(&lt;ID=BioSample_accession, DOI=url, Original=Accession_number, Name=Genotype_name&gt;) : Describe the material whose variants are given in the genotype call columns in greater detail and can be extended using the specifications of the VCF format. Examples: ##SAMPLE=&lt;ID=SAMEA7836897,DOI=\"doi.org/10.25642/IPK/GBIS/17527\",Original=\"HOR 1361 BRG\",Name=\"Hordeum vulgare L. convar. vulgare var. densum Sér.\"&gt; ##SAMPLE=&lt;ID=SAMEA9111398,DOI=”www.ipk-gatersleben.de”,Original=”CAPFRU07”,Name=”RCAT077650”&gt; . Optional meta-information lines : . ##fileDate: creation date of the VCF in the basic form without separator: YYYYMMDD Examples: ##fileDate=20211028 ##fileDate=20120316 . In case of adding new fields : Please check the official format specifications to avoid redundancy and possible incompatibilities. ",
      "url": "/pages/bedroesb/rdmkit/plant_sciences.html#plant-genotyping-data-sharing-and-deposition",
      "relUrl": "/plant_sciences.html#plant-genotyping-data-sharing-and-deposition"
    },"212": {
      "doc": "Plant sciences",
      "title": "Integrating plant phenotypic and molecular data",
      "content": "Description . Genomic-based prediction of plant phenotypes requires the integration of genomic and phenotypic data of the plants with data about their environment. While phenotypic and environmental data are typically stored together in phenotyping databases, genomic and other types of molecular data are typically deposited in international deposition databases, for example, those of the INSDC global consortium. It can be challenging to integrate phenotypic and molecular data even within a single project, particularly if the project involves studying a panel of genetic resources in different conditions. It is paramount to maintain the link between the plant material in the field, the samples extracted from them (e.g. at different development stages), and the results of omics experiments (e.g. transcriptomics, metabolomics) performed on those samples, across all datasets that will be generated and published. Integrating phenotyping and molecular data, both within and between studies, hinges entirely on precise identification of the plant material under study (down to the variety, or even the seed lot), as well as of the samples that are collected from these plants. Considerations . Are you working with established plant varieties, namely of crop plants? . Can you trace their provenance to a genebank and/or are they identified in a germplasm database? . Are working with crosses of established plant varieties? . Can you trace the geneology of the crosses to plant varieties from a genebank or identified in a germplasm database? . Solutions . The identification and description of plant materials should comply with the standard for the identification of plant genetic resources, The Multi-Crop Passport Descriptors (MCPD). If your plant materials that cannot be traced to an existing genebank or germplasm database, you should be describe them in accordance with the MCPD in as much detail as possible. If your plant materials can be traced to an existing genebank or germplasm database, you need only complement the MCPD information already published in the genebank or germplasm database. For wild trees, or plant materials derived from them, precise identification often requires the GPS coordinates of the tree. For identifying your plant material in a genebank or germplasm database, you can consult the European Cooperative Programme for Plant Genetic Resources (ECPGR), which includes a central germplasm database and a catalogue of relevant external databases. Another key database for identifying plant material is the European Search Catalogue for Plant Genetic Resources (EURISCO), which provides information about more than 2 million accessions of crop plants and their wild relatives, from hundreds of institutes in 43 member countries. For identifying samples from which molecular data was produced, the Biosamples database is recommanded as a provider of international unique identifiers. When detailing your sample in Biosamples, it is critical that you provide either a global identifier to your plant materials in a genebank or germplasm database, or a precise description of the plant materials in accordance with the MCPD. It is also recommended that you provide permanent access to a description of the project or study, that contains links to all the data, molecular or phenotypic. The Biostudies database is recommended for this purpose. ",
      "url": "/pages/bedroesb/rdmkit/plant_sciences.html#integrating-plant-phenotypic-and-molecular-data",
      "relUrl": "/plant_sciences.html#integrating-plant-phenotypic-and-molecular-data"
    },"213": {
      "doc": "Plant sciences",
      "title": "Plant sciences",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/plant_sciences.html",
      "relUrl": "/plant_sciences.html"
    },"214": {
      "doc": "Preserving",
      "title": "What is data preserving?",
      "content": "Data preservation consists of a series of activities necessary to ensure safety, integrity and accessibility of data for as long as necessary, even decades. Data preservation is indeed more than just data storage and backup, since data can be stored and backed up without being preserved. Data preservation prevents data from becoming unavailable and unusable over time through appropriate activities, such as: . Ensure data safety and integrity. Change the file format (format migration) and update software to make sure that they do not become outdated or obsolete. Change hardware and other storage media (such as paper, magnetic tape, etc) to avoid degradation. Ensure that data is organised and described with appropriate metadata and documentation to be always understandable and reusable. ",
      "url": "/pages/bedroesb/rdmkit/preserving.html#what-is-data-preserving",
      "relUrl": "/preserving.html#what-is-data-preserving"
    },"215": {
      "doc": "Preserving",
      "title": "Why is data preserving important?",
      "content": "The main reasons for research data preservation are: . Guarantee that your data can be verified and reproduced for several years after the end of the project. Allow the reuse of the data in the future for different purposes, such as teaching or further research. Funders, publishers, institutions and organisations could require a specific period for preservation of certain data for a specific purpose. Preserve data that have significant value for an organisation, a Nation, the environment or for the entire society. ",
      "url": "/pages/bedroesb/rdmkit/preserving.html#why-is-data-preserving-important",
      "relUrl": "/preserving.html#why-is-data-preserving-important"
    },"216": {
      "doc": "Preserving",
      "title": "What should be considered for preserving data?",
      "content": ". Not all data should be preserved. Preservation should be applied to an appropriate selection of data, since it takes relevant effort and costs. Common criteria to select the data to preserve for a certain amount of time are: . Funder, publisher and institution policies (usually, data should be preserved for at least 5 or 10 years after the end of the project). Legal or ethical requirements (e.g. clinical trial data). Unique data or that cannot be easily re-generated (e.g. raw data, analysis workflow). Data that will probably being reused in the future. Data of great value for society (scientifically, historically or culturally). Data preservation must be done by experts and dedicated services. Preservation of digital information requires planning, policies, resources (time, funds, people) as well as the right technology to ensure that the data stays functional and that it can be accessed (see ISO Standards for quality, preservation and integrity of information). Hence, special long term data repositories should be used for digital preservation, where the data is actively maintained and information integrity is monitored. Therefore, it is best to: . Contact the IT department or the library or the data center of your institution. Check if national services are available. Choose trustworthy research data repositories or deposition databases, based on your data type. Repositories could be publicly accessible and allow you to also publish your data. When preparing data for preservation: . Do not include data that are temporary or mutable. Ensure well described and self-explanatory documentation. Include information about provenance. Include sufficient licensing information. Ensure that data is well organised. Ensure that a consistent naming scheme is used. Use standard, open source, file formats instead of proprietary ones. If you need to preserve non-digital data (e.g. paper), consider whether digitalising the data is feasible or consult with data management support services in your institution. If you need to preserve materials, such as micro-organisms, biomaterials or biomolecules, consult with data management support services in your institution to find appropriate centers or biobanks. ",
      "url": "/pages/bedroesb/rdmkit/preserving.html#what-should-be-considered-for-preserving-data",
      "relUrl": "/preserving.html#what-should-be-considered-for-preserving-data"
    },"217": {
      "doc": "Preserving",
      "title": "Preserving",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/preserving.html",
      "relUrl": "/preserving.html"
    },"218": {
      "doc": "Privacy",
      "title": "What data we collect",
      "content": "We collect data about your activity on the site, such as: . which pages you have visited how long you spent on each page which browsers and devices you have used to view the site which country you view the site from. The information is collected anonymously and aggregated. We cannot identify you from the information gathered, or track your web browsing outside of this site. ",
      "url": "/pages/bedroesb/rdmkit/privacy.html#what-data-we-collect",
      "relUrl": "/privacy.html#what-data-we-collect"
    },"219": {
      "doc": "Privacy",
      "title": "How we collect the data",
      "content": "We use Google Analytics to collect this data. When you visit this site the Google Analytics program puts a small file called a cookie in your browser. Cookies are widely used to store preferences and to enable shopping carts and log-in areas on websites. ",
      "url": "/pages/bedroesb/rdmkit/privacy.html#how-we-collect-the-data",
      "relUrl": "/privacy.html#how-we-collect-the-data"
    },"220": {
      "doc": "Privacy",
      "title": "Why we collect the data",
      "content": "We collect the data so that we can then generate reports about the site for our funders. We also collect the date to help us improve the site, and ensure we design the site for the browsers and devices that most people are using. ",
      "url": "/pages/bedroesb/rdmkit/privacy.html#why-we-collect-the-data",
      "relUrl": "/privacy.html#why-we-collect-the-data"
    },"221": {
      "doc": "Privacy",
      "title": "Who has access to the data",
      "content": "The data collected from you visit to the site is sent to Google but aggregated and anonymised. We do not have access to personally identifiable information, but Google records and stores your IP address in order to generate the site statistics. See Google Analytics cookies information and Google’s Safeguarding your data article. ",
      "url": "/pages/bedroesb/rdmkit/privacy.html#who-has-access-to-the-data",
      "relUrl": "/privacy.html#who-has-access-to-the-data"
    },"222": {
      "doc": "Privacy",
      "title": "If you have concerns about your data",
      "content": "We ask for your permission to set the cookie when you visit the site, but if you change your mind you can: . Remove the cookie manually. See the Norton Security website for detailed instructions. Use a browser extension to stop the cookie, such as Google’s Opt-out Browser Add-on. Please contact rdm-editors@elixir-europe.org if you have any queries concerning the data we may have. ",
      "url": "/pages/bedroesb/rdmkit/privacy.html#if-you-have-concerns-about-your-data",
      "relUrl": "/privacy.html#if-you-have-concerns-about-your-data"
    },"223": {
      "doc": "Privacy",
      "title": "Your GitHub information",
      "content": "If you are a contributor to the site then you will be automatically listed as a contributor, with your profile picture (if you have one) and a link to your GitHub pages. If you would like to be removed from the contributor list then please contact rdm-editors@elixir-europe.org. You may also want to see the GitHub Privacy Statement. ",
      "url": "/pages/bedroesb/rdmkit/privacy.html#your-github-information",
      "relUrl": "/privacy.html#your-github-information"
    },"224": {
      "doc": "Privacy",
      "title": "Privacy",
      "content": "This Privacy Policy explains what personal data is collected by the Research Data Management (RDM) Toolkit website. ",
      "url": "/pages/bedroesb/rdmkit/privacy.html",
      "relUrl": "/privacy.html"
    },"225": {
      "doc": "Processing",
      "title": "What is data processing?",
      "content": "Data processing is the phase in the project where data is converted into a desired format and prepared for analysis. When data has been freshly collected, data processing includes some automated steps in a workflow that perform format conversion, quality check and preprocessing following a standardised protocol. The main aim of processing is to: . Convert data into readable format giving it the shape and form necessary for downstream analysis. Discard bad or low quality data in order to create clean, high-quality dataset for reliable results. When data is imported from existing sources, e.g. data to be reused from another project, processing can also include manual steps to make it suitable for analysis. These steps include but are not limited to: . Making changes to data formats such that different datasets will be compatible for integration with each other. Changing coding systems or ontologies for the data to bring everything to the same level. Filtering data such that only data suitable for the project is retained. After data processing, clean data is ready for analysis and should therefore be available to the members of the project team that need to perform the next steps. ",
      "url": "/pages/bedroesb/rdmkit/processing.html#what-is-data-processing",
      "relUrl": "/processing.html#what-is-data-processing"
    },"226": {
      "doc": "Processing",
      "title": "Why is data processing important?",
      "content": "Data processing is important to ensure good quality of the collected data and to prepare it for meaningful data analysis. Accurate data processing is also essential for combining two or more datasets into a single dataset. An accurate documentation of every step done during data processing is key for the reproducibility of your result. Processing data correctly makes it easy to arrange, analyse and also saves a lot of space. ",
      "url": "/pages/bedroesb/rdmkit/processing.html#why-is-data-processing-important",
      "relUrl": "/processing.html#why-is-data-processing-important"
    },"227": {
      "doc": "Processing",
      "title": "What should be considered for data processing?",
      "content": "The following considerations are important for data processing: . Sensitive data should be Pseudonymised/anonymised. Not only should you remove the directly identifying data, but also be attentive to other sources e.g. names written on images. Appropriate standards for encoding different data fields should be used. All steps of anonymisation and encoding should be properly documented, For e.g. The chosen encoding formats used for data fieds should be documented. The special significance of empty or otherwise special data fields. All relationships between data fields should be made explicit (e.g. if a dataset contains “medication” and “disease”, is that medication actually used to treat the disease? Or is it a medication that the patient is using for other reasons?). ",
      "url": "/pages/bedroesb/rdmkit/processing.html#what-should-be-considered-for-data-processing",
      "relUrl": "/processing.html#what-should-be-considered-for-data-processing"
    },"228": {
      "doc": "Processing",
      "title": "Processing",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/processing.html",
      "relUrl": "/processing.html"
    },"229": {
      "doc": "Proteomics",
      "title": "Introduction",
      "content": "The proteomics domain deals with standard data formats, software tools and data repositories for mass spectrometry-based proteomics data. In proteomics, the relatively wide range of mass spectrometry technologies, devices, protocols, study designs and data analysis approaches poses a particular challenge for the standardized description and storage of data and associated metadata. This circumstance forced the proteomics community to deal with the complex definition of suitable standard data formats relatively early in its history. This encouraged, among other things, the development of software tools that can import and export results in standardized formats, and of data repositories in which proteomics data can be stored in a standardized way. The particular challenge for the proteomics community now is to evolve its achievements in data management to date towards a more complete fulfillment of FAIR research data management and to close the remaining gaps in this regard. ",
      "url": "/pages/bedroesb/rdmkit/proteomics.html#introduction",
      "relUrl": "/proteomics.html#introduction"
    },"230": {
      "doc": "Proteomics",
      "title": "Standard data formats",
      "content": "Description . To make proteomics data interoperable and reproducible from the first to the last mile of proteomics data analysis pipelines, comprehensive metadata accompanying the data is needed. The crucial metadata includes information on study design, proteomics technology, lab protocol, device, device settings and software settings. All of them have an enormous impact on the resulting data. Thus, to enable data reusability in proteomics appropriate standard data formats are needed. Considerations . For different proteomics experiments and different steps of the respective data analysis pipelines there are different kinds of data and metadata that should be recorded. Consequently, the main challenges for data and metadata standardization include: . What are the definitions of proteomics-specific terms that are needed to describe proteomics experiments? Which is the minimal information that is needed to describe a proteomics experiment? How should the data and metadata of proteomics raw data and peak lists be stored? How should the data and metadata of proteomics identification results be stored? How should the data and metadata of proteomics quantification results be stored? How can proteomics data and metadata be stored in a simple and human-readable way? . Solutions . The Human Proteome Organisation (HUPO) Proteomics Standards Initiative (HUPO-PSI), a proteomics community-driven organization, provides several different controlled vocabularies, standard data formats, converter and validator software tools. The most important include: . Controlled vocabularies: PSI-MS, PSI-MI, XLMOD and sepCV, which are provided as OBO files. The Minimum Information About a Proteomics Experiment (MIAPE) guidelines document. mzML - a standard format for encoding raw mass spectrometer output. mzIdentML - a standard exchange format for peptides and proteins identified from mass spectra. mzQuantML - a standard format that is intended to store the systematic description of workflows quantifying molecules (principly peptides and proteins) by mass spectrometry. mzTab - a tab delimited text file format to report proteomics and metabolomics results. ",
      "url": "/pages/bedroesb/rdmkit/proteomics.html#standard-data-formats",
      "relUrl": "/proteomics.html#standard-data-formats"
    },"231": {
      "doc": "Proteomics",
      "title": "Processing and analysis of proteomics data",
      "content": "Description . For all steps within a FAIR proteomics data analysis pipeline software is needed that imports standard data formats and exports standard data formats including all needed results and metadata. Considerations . Can your proteomics raw data recorded by a mass spectrometer be stored as an mzML file? Is it possible to convert your raw data to mzML? Does your search engine support mzML and/or mzIdentML? Does your quantification software support mzQuantML or mzTAB? . Solutions . Within the proteomics community various converter software tools such as msconvert were implemented, which support the conversion of mass spectrometer output formats to the mzML standard data format as well as other conversions to standard data formats. Information on software tools that support HUPO-PSI standard data formats can be found on the standard format-specific web pages of the HUPO-PSI (e.g., mzML , mzIdentML and MZTAB ). ",
      "url": "/pages/bedroesb/rdmkit/proteomics.html#processing-and-analysis-of-proteomics-data",
      "relUrl": "/proteomics.html#processing-and-analysis-of-proteomics-data"
    },"232": {
      "doc": "Proteomics",
      "title": "Preserving and sharing proteomics data",
      "content": "Description . In order to make proteomics data and results worldwide findable and accessible for other researchers and software, FAIR public data repositories are needed. Consideration . How can I find an appropriate proteomics data repository? How can I upload my proteomics data into a specific proteomics data repository? What are the requirements for my data to be uploaded into a proteomics data repository? What are the advantages of uploading data into proteomics data repositories? How can public proteomics data be used by other researchers? . Solution . You can find an appropriate data repository via the website of the ProteomeXchange Consortium. ProteomeXchange was established to provide globally coordinated standard data submission and dissemination pipelines involving the main proteomics repositories, and to encourage open data policies in the field. Currently, member repositories include PRIDE, PepideAtlasq, MassIVE, jPOST, iProx and PanoramaPublic. Information on data uploads can be found on proteomexchange.org or on the websites of the particular data repositories. E.g. PRIDE uploads are conducted via a submission tool. There are data repository-specific requirements. Advantages of data publication: fulfillment of journal requirements, higher visibility of research, free storage, worldwide accessibility, basic re-analysis by repository-associated tools . ",
      "url": "/pages/bedroesb/rdmkit/proteomics.html#preserving-and-sharing-proteomics-data",
      "relUrl": "/proteomics.html#preserving-and-sharing-proteomics-data"
    },"233": {
      "doc": "Proteomics",
      "title": "Proteomics",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/proteomics.html",
      "relUrl": "/proteomics.html"
    },"234": {
      "doc": "Researcher",
      "title": "Applies to",
      "content": "PhD candidates, research grant applicants, project managers, group leaders, PIs . ",
      "url": "/pages/bedroesb/rdmkit/researcher.html#applies-to",
      "relUrl": "/researcher.html#applies-to"
    },"235": {
      "doc": "Researcher",
      "title": "Scenario",
      "content": "The funding organisation I am applying to requires a data management plan (DMP). I have little experience in writing a DMP, and I am not sure of the level of detail I am required to provide. I have limited access to data management experts within my institution. I am considering using the RDMkit for my data management needs. I also hope to find useful references to local training about data management requirements, data archives and DMP tools. I know the types and the approximate amount of data I will generate, but I have not thought about how to share data with my collaborators and how to store data securely. Initially, my plan was to buy a powerful computer and portable hard drive, but I am now thinking that I need to use a national computing infrastructure. The field I work in has well defined data and curation standards, for example, capturing information (metadata) about how to collect and sample my data. However, I am not yet familiar with the importance of storing provenance data, such as tool and database versions used in analysis. ",
      "url": "/pages/bedroesb/rdmkit/researcher.html#scenario",
      "relUrl": "/researcher.html#scenario"
    },"236": {
      "doc": "Researcher",
      "title": "Focus",
      "content": ". Write data management plans, also in the context of grant applications Ensure compliance with institution policy, including legal and ethical aspects Ensure proper data organisation and storage Ensure secure sharing, reproducibility and preservation of data Transmits the good practices in RDM to his group . ",
      "url": "/pages/bedroesb/rdmkit/researcher.html#focus",
      "relUrl": "/researcher.html#focus"
    },"237": {
      "doc": "Researcher",
      "title": "Getting started",
      "content": ". Check out the various steps of the RDM life cycle, in particular the planning stage Identify and contact the data steward in your local organisation or your national contact in the ELIXIR network Start planning your project taking the DMP into account . ",
      "url": "/pages/bedroesb/rdmkit/researcher.html#getting-started",
      "relUrl": "/researcher.html#getting-started"
    },"238": {
      "doc": "Researcher",
      "title": "Researcher",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/researcher.html",
      "relUrl": "/researcher.html"
    },"239": {
      "doc": "Reusing",
      "title": "What is data reuse?",
      "content": "Data reuse means using data for other purposes than it was originally collected for. Reuse of data is particularly important in science, as it allows different researchers to analyse and publish findings based on the same data independently of one another. Reusability is one key component of the FAIR principles. Data that is well-described, curated and shared under clear terms and conditions is more likely to be reused. Integration with other data sources is also important, since that can enable new, yet unanticipated, uses for the data. ",
      "url": "/pages/bedroesb/rdmkit/reusing.html#what-is-data-reuse",
      "relUrl": "/reusing.html#what-is-data-reuse"
    },"240": {
      "doc": "Reusing",
      "title": "Why is data reuse important?",
      "content": "By reusing existing data you can: . obtain reference data for your research avoid doing new, unnecessary experiments run analyses to verify that reported findings are correct, and thereby making subsequent findings more robust make research more robust by aggregating results obtained from different methods or samples gain novel insights by connecting and meta-analysing datasets . ",
      "url": "/pages/bedroesb/rdmkit/reusing.html#why-is-data-reuse-important",
      "relUrl": "/reusing.html#why-is-data-reuse-important"
    },"241": {
      "doc": "Reusing",
      "title": "What should be considered for data reuse?",
      "content": "Consider the following when reusing data: . Explore different sources for reusable data. A starting point can be to look for value added databases with curated content. Other possibilities include searching data deposition repositories for suitable datasets based on their annotation, or obtaining data directly from the author of a scientific article. Check under which terms and conditions the data is shared. Make sure that there is a licence, and that the licence gives you permission to do what you intend to. Check whether there is sufficient metadata to enable data reuse. Some types of data can be straightforward to reuse (e.g. genome data), while other may require extensive metadata to interpret and reuse (e.g. gene expression experiment data). Assess the quality of the data. Is the data from a trusted source? Is it curated? Does the data adhere to a standard? Verify that the data has been ethically collected and that your reuse of the data conforms with policies and regulations you are expected to follow. For personal (sensitive) data, there are usually legal and technical requirements that have to met before data can be accessed. Getting access to personal (sensitive) data will therefore involve additional steps. If the data has been updated, make sure to document which version of the data you are using. Also consider what impact the changes may have on your results. Cite the data properly. Include a persistent identifier (such as a DOI) in the citation if there is one. ",
      "url": "/pages/bedroesb/rdmkit/reusing.html#what-should-be-considered-for-data-reuse",
      "relUrl": "/reusing.html#what-should-be-considered-for-data-reuse"
    },"242": {
      "doc": "Reusing",
      "title": "Reusing",
      "content": " ",
      "url": "/pages/bedroesb/rdmkit/reusing.html",
      "relUrl": "/reusing.html"
    },"243": {
      "doc": "Sensitive data",
      "title": "Sensitive data",
      "content": "## Is your data sensitive? ### Description In general, the term \"sensitive data\" is used for any data that could do harm (for example to people, organisations, countries, or ecosystems) if it would be openly available. This can for example be personal or commercial information, but also information such as breeding grounds of endangered species. Any such data must be protected against unauthorized access. What is considered sensitive information is usually regulated by national laws and may differ between countries. You should be cautious when you are dealing with sensitive, or potentially sensitive, information. ### Considerations * If you deal with any information about individuals from the EU, you are bound by the [General Data Protection Regulation (GDPR)](https://gdpr.eu/what-is-gdpr/). In GDPR, such data is called \"personal data\". * In the context of GDPR \"special category data\" is a subclass of \"personal data\" that is potentially even more harmful, and GDPR prescribes very strict rules for dealing with this data. Article 9 of GDPR defines the special categories as data consisting of racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade union membership, genetic data, biometric data, data concerning health or data concerning a natural person's sex life or sexual orientation. Confusingly, these special categories are sometimes colloquially called \"sensitive data\". Note that this page is concerned with the broader definition of \"sensitive data\". * Information in Life Science projects are for the most part categorised under health and genetic data and are considered special category data under the GDPR. * You need to assess whether or not your dataset contains personally identifying attributes. Note that combinations of attributes that are themselves not identifyable can be identifyable together. * You need to know the de-identification status of your data. Life Science research data rarely contains directly identifying attributes. Research data would typically be pseudonymised or anonymised. If you work with personal data, you must understand the difference between these two (see under de-identification below). * For some studies there is a cohort owner, often a clinical party or a trusted third party that can map study participant keys back to names and surnames. Such data is considered pseudonymous. * If there are no means to map the data back to individuals, then the data is considered anonymous and is out of the scope of the GDPR. * You should keep in mind that anonymising data is a notoriously difficult task. Does your dataset contain a wide array of attributes, or exhibit unique traits/patterns such that one can reasonably expect that not more than a dozen people in the world have those together? In that case, you can not assume that it is anonymous. Such data run the risk of being linked back to individuals through various technical means. You need to take into account that technical means to identify people in the future may be more powerful than than they are right now: i.e. data that is anonymous right now may not be anonymous forever. ### Solutions * Identify what legislations and regulations there are that you are expected to follow. Your institution's website may give you hints on where you can look for information about sensitive data. * If you cannot determine if your data is sensitive, contact someone with expert knowledge in that area. ## How can you de-identify your data? ### Description Data anonymization is the process of irreversibly modifying personal data in such a way that subjects cannot be identified directly or indirectly by anyone, including the study team. If data are anonymized, no one can link data back to the subject. Pseudonymization is a process where identifying-fields within data records are replaced by artificial identifiers called pseudonyms or pseudonymized IDs. Pseudonymization ensures no one can link data back to the subject, apart from nominated members of the study team who will be able to link pseudonyms to identifying records, such as name and address. Data anonymization involves modifying a dataset so that it is impossible to identify a subject from their data. Pseudonymization involves replacing identifying data with artificial IDs, for example, replacing a healthcare record ID with an internal participant ID only known to a named clinician working in the study. ### Considerations Both anonymization and pseudonymization are approaches that comply with the GDPR. Simply removing identifiers cannot guarantee data anonymity. A dataset may contain unique traits/patterns that could identify individuals. An example of this would be recording 2 potentially unrelated attributes such as the instance of a rare disease and country of residence, where there is only a single case of this disease in this country. Data that is anonymous currently may not be anonymous in the future. Future datasets on the same individual may disclose their identity. Anonymization techniques can sometimes damage the statistical properties of the data, for example, translating current participant age into an age range. ### Solutions An example of pseudonymization is where participants in a study are assigned a non-identifying ID and all identifying data (such as name and address) are removed from the metadata to be shared. The mapping of this ID to personal data is held separately and securely by a named researcher who will not share this data. There are well-established data anonymization approaches, such as k-anonymity, l-diversity, and differential privacy. ",
      "url": "/pages/bedroesb/rdmkit/sensitive_data.html",
      "relUrl": "/sensitive_data.html"
    },"244": {
      "doc": "Sharing",
      "title": "Sharing",
      "content": "## What is data sharing? Sharing data means making your data known to other people. You can share your data with collaboration partners in the context of a collaborative research project, or you can publish your data to share it with the global research community and society at large. It’s important to know that data sharing doesn’t mean open data or public data. You can choose to share your data with restricted access or even closed access. Moreover, sharing or publishing data is different from publishing a paper or a manuscript in a journal. Here we focused on data (i.e. raw observations and measurements, analysis workflows, code, etc), not on papers or articles. Data sharing can be done at any time during the research data life cycle but, at the latest, data should be made available at the time of publication of articles that use the data to make scientific conclusions. ## Why is data sharing important? In a collaborative project, being able to easily share data makes research more efficient. Sharing of data is a cornerstone of good science. It is a good research practice to ensure that data underlying research is preserved and made available to the research community and society at large. Sharing data is a prerequisite for making your research reproducible. To be useful for others, you should strive to make the shared data adhere to the FAIR principles. In the EU, the 'Open Data Directive' ([Directive (EU) 2019/1024](https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1561563110433&uri=CELEX:32019L1024)) states that \"_Member States shall support the availability of research data by adopting national policies and relevant actions aiming at making publicly funded research data openly available (‘open access policies’), following the principle of ‘open by default’ and compatible with the FAIR principles._\" Many research funders, institutions and reputable journals/publishers now have data sharing mandates, from which you normally cannot opt out of unless there are legitimate reasons (ethical or legal reasons). Additional reasons to share your datasets: * [Ten reasons to share your data.](https://www.natureindex.com/news-blog/ten-reasons-to-share-your-data) * [Ask not what you can do for open data; ask what open data can do for you.](http://blogs.nature.com/naturejobs/2017/06/19/ask-not-what-you-can-do-for-open-data-ask-what-open-data-can-do-for-you/) Even though it may not be possible to openly share all data because of ethical, legal, contractual, or intellectual property reasons, do strive to make data [\"_as open as possible, as closed as necessary_\"](https://www.allea.org/wp-content/uploads/2017/05/ALLEA-European-Code-of-Conduct-for-Research-Integrity-2017.pdf). Making the data as FAIR as possible will ensure that maximum value can be obtained out of it in future. ## What should be considered for data sharing? * If you are part of a collaborative research project, it is recommended to plan and establish the following in advance: * The use of repositories and sharing services which allow controlled access to share your preliminary data with project partners. * The use of storage solutions that guarantee shared, controlled and secure access to the data and appropriate data transfer. * The deposition of your data to a public repository as early as possible. This saves a lot of trouble later on. Data can be put under embargo until you want to release it, e.g. at the time of article publication. * The use of common data organisation, data formats, standards, data documentation and metadata. * If you want to share or publish your data, you should: * Make sure you have the rights to do so (i.e., are you the creator of the data?). * Consider all possible ethical, legal, contractual, or intellectual property restrictions related to your data (GDPR, consent, patent, etc). * Check funders and institutional requirements about data sharing policy and data availability. * Establish if you need to limit reusability of your data for legitimate reasons (consider applying a specific licence). * Make the data citable so that you can receive credit (use identifiers). * Based on the considerations listed above, you should be able to determine the right type of access for you data. Even if the access to the data is restricted, it is good practice to openly and publicly share the metadata of your data. * **Open access**: data is shared publicly. Anyone can access the data freely. * **Registered access or authentication procedure**: potential users must register before they are able to access the data. The “researcher” status is guaranteed by the institution and the user agrees to abide by data usage policies of repositories that serve the shared data. Datasets that are shared via registered-access would typically have no restrictions besides the condition that data is to be used for research. Registered access allows the data archive to monitor who can access data, enabling reminders about conditions of use to be issued. * **Controlled access or Data Access Committees (DACs)**: data can only be shared with researchers, whose research is reviewed and approved by a Data Access Committee (DAC). DAC is an organization of one or more named individuals responsible for data release to external requestors based on specific criteria (research topics, allowed geographical regions, allowed recipients etc). Criteria established by DAC for data access are usually described on the website of the organization. * **Access upon request (not recommended)**: in order to manage this type of access a named contact is required for the dataset who would be responsible for making decisions about whether access is granted. The owner of the data must provide his/her contact in the documentation associated with the datasets (metadata). Metadata about the datasets must be open. * Share and publish your data in professional deposition databases that provide the appropriate access type and licence: * If there are discipline-specific repositories available for you data, this should be your primary choice. They will work towards a high level of FAIRness by recommending appropriate community standards for describing the data. * If there are no suitable discipline-specific repositories for your data: * Deposit the data in an _Institutional repository_, if there is one. These often provide stewardship and curation, helping to ensure that your dataset is preserved and accessible. Contact the Research Data Office function at your institution, if there is one. * Deposit the data in a [_General purpose repository_](https://www.nature.com/sdata/policies/repositories#general). * If there isn't any suitable repository that can harbour your controlled access data, it is recommended that you at least create a metadata record for the data in an Institutional or General purpose repository. ",
      "url": "/pages/bedroesb/rdmkit/sharing.html",
      "relUrl": "/sharing.html"
    },"245": {
      "doc": "Data storage",
      "title": "Data storage",
      "content": "## What features do you need in a storage solution when collecting data? ### Description The need for Data storage arises early on in a research project, as space will be required to put your data when starting collection or generation. Therefore, it is a good practice to think about storage solutions during the data management planning phase, and request storage in advance and/or pay for it. The storage solution for your data should fulfil certain criteria (e.g. space, access & transfer speed, duration of storage etc), which should be discussed with the IT team. You may choose a tiered storage system for assigning data to various types of storage media based on requirements for access, performance, recovery and cost. Using tiered storage allows you to classify data according to levels of importance and assign it to the appropriate storage tiers or move it to different tier for e.g. once analysis is completed you have the option to move data to lower tier for preservation or archiving. Tiered Storage is classified as “Cold” or “Hot” Storage. “Hot” storage is associated with fast access speed, high access frequency, high value data and consists of faster drives such as the Solid State Drives (SSD). This storage is usually located in close proximity to the user such as on campus and incurs high costs. “Cold” storage is associate with low access speed and frequency and consists of slower drives or tapes. This storage is usually off-premises and incurs low cost. ### Considerations When looking for solutions to store your data during the collection or generation phase, you should consider the following aspects: * The volume of your data is an important discerning factor to determine the appropriate storage solution. At the minimum, try to estimate the volume of raw data that you are going to generate or collect. * What kind of access/transfer speed and access frequency will be required for your data? * Knowing where the data will come from is also crucial. If the data comes from an external facility or needs to be transferred to a different server, you should think about an appropriate data transfer method. * It is a good practice to have a copy of the original raw data in a separate location, to keep it untouched and unchanged (not editable). * Knowing for how long the raw data, as well as data processing pipelines and analysis workflows need to be stored, especially after the end of the project, is also a relevant aspect for storage. * It is highly recommended to have metadata, such as an identifier and file description, associated with your data (see Metadata management page). This is useful if you want to retrieve the data years later or if your data needs to be shared with your colleagues for collaboration. Make sure to keep metadata together with the data or establish a clear link between data and metadata files. * In addition to the original “read-only” raw (meta)data files, you need storage for files used for data processing and analysis as well as the workflows/processes used to produce the data. For these, you should consider: * Who is allowed to access the data (in case of collaborative projects), how do they expect to access the data and for what purpose. * Check if you have the rights to give access to the data, in case of legal limitations or third party rights (for instance, collaboration with industry). * Consult policy for data sharing outside the institute/country (see Compliance and Monitoring page). * Keeping track of the changes (version control), conflict resolution and back-tracing capabilities. ### Solutions * Provide an estimate about the volume of your raw data (i.e., is it in the order of Megabytes, Gigabytes or Terabytes?) to the IT support in your institute when consulting for storage solutions. * Clarify if your data needs to be transferred from one location to another. Try to provide IT with as much information as possible about the system where the data will come from. See our Data Transfer page for additional information. * Ask for a tiered storage solution that gives you easy and fast access to the data for processing and analysis. Explain to the IT support what machine or infrastructure you need to access the data from and if other researchers should have access as well (in case of collaborative projects). * Ask if the storage solution includes an automatic management of versioning, conflict resolution and back-tracing capabilities (see also our Data Organisation page). * Ask the IT support in your institute if they offer technical solutions to keep a copy of your (raw)data secure and untouched (snapshot, read-only access, backup…). You could also keep a copy of the original data file in a separate folder as “read-only”. * For small data files and private or collaborative projects within your institute, commonly accessible Cloud Storage is usually provided by the institute, such as NextCloud (on-premises), Microsoft OneDrive, DropBox, Box, etc. Do not use personal accounts on similar services for this purpose, adhere to the policies of your institute. * It is a requirement from the funders or universities to store raw data and data analysis workflows (for reproducible results) for a certain amount of time after the end of the project (see our Preserve page). This is usually a requirement. Check the data policy for your project or institute to know if a copy of the data should be also stored at your institute for a specific time after the project. This helps you budget for storage costs and helps your IT support with estimation of storage resources needed. * Make sure to generate good documentation (i.e., README file) and metadata together with the data. Follow best practices for folder structure, file naming and versioning systems (see our Data Organisation page). Check if your institute provides a (meta)data management system, such as iRODS, DataVerse, FAIRDOM-SEEK or OSF. See All tools and resources table below for additional tools. ### Planning your data storage solution You can plan your data storage, including the above considerations and solutions, using the [Data Stewardship Wizard]({{ site.dsw_deep_link_prefix }}a12aa967-28a5-4a9b-8df8-f7c533205ea4). This link provides access directly to the appropriate section of the Data Management Plan questionnaire. ## How do you estimate computational resources for data processing and analysis? ### Description In order to process and analyse your data, you will need access to computational resources. This ranges from your laptop, local compute clusters to High Performance Computing (HPC) infrastructures. However, it can be difficult to be able to estimate the amount of computational resource needed for a process or an analysis. ### Considerations Below, you can find some aspects that you need to consider to be able to estimate the computational resource needed for data processing and analysis: * The volume of total data is an important discerning factor to estimate the computational resources needed. * Consider how much data volume you need “concurrently or at once”. For example, consider the possibility to analyse a large dataset by downloading or accessing only a subset of the data at a time (e.g., stream 1 TB at a time from a big dataset of 500 TB). * Define the expected speed and the reliability of connection between storage and compute. * Determine which software you are going to use. If it is a proprietary software, you should check possible licensing issues. Check if it only runs on specific operative systems (windows, mac, linux…). * Establish if and what reference datasets you need. * In the case of collaborative projects, define who can access the data and the computational resource for analysis (specify from what device, if possible). Check policy about data access between different Countries. Try to establish a versioning system. ### Solutions * Try to estimate the volume of: * Raw data files necessary for the process/analysis. * Data files generated during the computational analysis as intermediate files. * Results data files. * Communicate your expectations about speed and the reliability of connection between storage and compute to the IT team. This could depend on the communication protocols that the compute and storage systems use. * It is recommended to ask about the time span for analysis to colleagues or bioinformatic support that have done similar work before. This could save you money and time. * If you need some reference datasets (e.g the references genomes such as human genome.), ask IT if they provide it or consult bioinformaticians that can set up automated public reference dataset retrieval. * For small data files and private projects, using the computational resources of your own laptop might be fine, but make sure to preserve the reproducibility of your work by using data analysis software such as Galaxy or R Markdown. * For small data volume and small collaborative projects, a commonly accessible Cloud Storage, such as Nextcloud (on-premises) or Owncloud might be fine. Adhere to the policies of your institute. * For large data volume and bigger collaborative projects, you need a large storage volume on fast hardware that is closely tied to a computational resource accessible to multiple users. ## Where should you store the data after the end of the project? ### Description After the end of the project, all the relevant (meta)data (to guarantee reproducibility) should be preserved for a certain amount of time, that is usually defined by funders or institution policy. However, where to preserve data that are not needed for active processing or analysis anymore is a common question in data management. ### Considerations: * Data preservation doesn’t refer to a place nor to a specific storage solution, but rather to the way or “how” data can be stored. As described in our Preservation page, numerous precautions need to be implemented by people with a variety of technical skills to preserve data. * Estimate the volume of the (meta)data files that need to be preserved after the end of the project. Consider using a compressed file format to minimize the data volume. * Define the amount of time (hours, days…) that you could wait in case the data needs to be reanalysed in the future. * It is a good practice to publish your data in public data repositories. Usually, data publication in repositories is a requirement for scientific journals and funders. Repositories preserve your data for a long time, sometimes for free. See our Data Publication page for more information. * Institutes or universities could have specific policies for data preservation. For example, your institute can ask you to preserve the data internally for 5 years after the project, even if the same data is available in public repositories. ### Solutions * Based on the funders or institutional policy about data preservation, the data volume and the retrieval time span, discuss with the IT team what preservation solutions they can offer (i.e., data archiving services in your Country) and the costs, so that you can budget for it in your DMP. * Publish your data in public repositories, and they will preserve the data for you. ",
      "url": "/pages/bedroesb/rdmkit/storage.html",
      "relUrl": "/storage.html"
    },"246": {
      "doc": "Style guide",
      "title": "Style guide",
      "content": "In general, we follow the European Commission's [Web Writing Style Guide](https://wikis.ec.europa.eu/display/WEBGUIDE/02.+Web+writing+guidelines). Below are the points that you might find most useful, though, and that relate particularly to the RDMkit. ## General style and tone * Keep the tone friendly rather than formal, and use \"you\". Imagine you were explaining something verbally to someone - how would you say it? * Use short, active sentences and short paragraphs (3-4 sentences). * Make use of headings and bullet points to break text up so it is easy to scan. * Remember that the site is there to help people, so make it clear to the readers how the information can benefit them. * Use the words your readers would use. Think of the terms they would use when searching for their problem, and use those terms. ## Text * **Acronyms:** spell them out the first time. * **Ampersands:** do not use these in the main text or headings. It is fine to use them in menus, if you need to save space. * **Capitals:** do not use all capitals for emphasis or in headings. * **Data:** treat as singular (\"Data is...\"). (Whether \"data\" is singular or plural is contentious - see the [Wikipedia article](https://en.wikipedia.org/wiki/Data_(word)) and this [Guardian article](https://www.theguardian.com/news/datablog/2010/jul/16/data-plural-singular).) * **Dates:** use Wednesday 7 July 2021 (not Wednesday 7th July 2021, or other variations). * **Datasets:** not \"data sets\". * **Email:** not \"e-mail\". * **Email addresses:** spell these out and make the email address the link e.g. [rdm-toolkit@elixir-europe.org](mailto:rdm-toolkit@elixir-europe.org). Do not hide the email address behind a word or phrase like \"contact us\". * **Gender:** avoid using gender-specific words like \"he\" or \"she\". * **Headings:** * Only the first word is capitalised, unless other words are proper nouns. * Headings must be hierarchical. They must go down in order (level one, level two, level three), and not skip a level. It is fine to skip levels when moving back up e.g. you can skip from level four to level two. * **-ise/-ize:** use the \"-ise\" form. * **Life cycle:** two separate words. * **Links:** make the link text say where the link goes e.g. \"the Contribute page\", not \"click here\". Avoid using the url as the link text. * **Lists:** * _A list of short items_: introduce with a colon, start each bullet point with a capital and don't use punctuation at the end of each bullet point: * Item 1 * Item 2 * _A list of longer items following an incomplete introductory sentence (e.g. a sentence ending in a colon)_: each item ends with a semi colon and the final item ends with a full stop. Do not capitalise the first letter of each item e.g. This is the first part of a sentence that includes: * a longer item 1; * a longer item 2; * a longer item 3. * _A list following a complete sentence (with a full stop)_: each item ends with a full stop and each point begins with a capital letter e.g. This a complete sentence. * This is item 1 of the list. * This is item 2 of the list. * This is item 3 of the list. * **Numbers:** spell the numbers one to ten out. After that, write the numbers (11, 12, 13, etc). * **Quotations:** use double quotes for quotations, and single quotes for quotes within quotes. * **References:** use the [Nature Author instructions](https://www.nature.com/srep/author-instructions/submission-guidelines#references) for books and papers. Use \"*et al.*\" for more than five authors. * Bellin, D. L. *et al.* Electrochemical camera chip for simultaneous imaging of multiple metabolites in biofilms. Nat. Commun. 7, 10535; [10.1038/ncomms10535](http://www.nature.com/articles/ncomms10535) (2016). * Lam, J. Data Management. (John Wiley & Sons, Inc., 2019). * **That/which:** use \"that\" when you are defining something and \"which\" when you are adding extra information about it e.g.: * \"The cat that was on the table suddenly got up\" is telling us which cat it was. It is important to the meaning of the sentence because you are not talking about any cat, just the cat on the table. * \"The cat, which was sitting on the table, suddenly got up\" is giving us extra information about the cat. The information is not necessary to understand the sentence. You can remove the clause and the sentence will still be clear. Clauses starting with \"which\" usually begin with a comma. * **Schema/scheme:** Use \"schema\" if you mean a pattern for structuring data. The plural is \"schemas\". * **Titles:** only the first word and acronyms are capitalised. * **Tool assembly:** there are multiple tools in **one** assembly. The plural is \"tool assemblies\". * **Training:** training is an uncountable noun and cannot have a plural. You can write \"training courses\" and \"training materials\" but not \"trainings\". ## Graphic design * **White space:** make sure there is plenty of space space so that the main elements stand out and the text does not appear overwhelming. * **Colours:** * The headings, links and text will automatically appear in the right colour if you use the site page templates. * Use only the following colours in the design, text and illustrations of the site. The RDM life cycle diagram colours are only for use in the pages related to the diagram. * | #C23669 | Magenta | Logo, Menu highlight, Second level heading (h2), Main theme colour | | #376AC3 | Blue | Link colour | | #2a2e3d | Dark blue | First level headings (h1), Third level heading (h3), Body text, Header, Footer | | #83858e | Gray | Gray text, Fourth level heading (h4) | | #f3f1f2 | Light gray | Box backgrounds | * **Fonts:** Exo 2 is used for headings and main branding font, Open Sans for body text. * **Icons:** the icons used in this site come from the [Noun Project](https://thenounproject.com/ELIXIRCommunications/kit/rdmkit/). We have a Pro license and so the right to publish them without attribution. * **Templates:** keep the structure of the pages consistent by using the site templates (see the [contribute page](how_to_contribute)). * **Illustrations:** use the colours listed above. The icons we use for illustrations come from the [Noun Project](https://thenounproject.com/ELIXIRCommunications/kit/rdmkit/). Please use these icons in any illustrations. If you need extra icons, or any help with illustrations, [open a new issue](https://github.com/elixir-europe/rdmkit/issues) on GitHub or email [rdm-toolkit@elixir-europe.org](mailto:rdm-toolkit@elixir-europe.org). * **Images:** * Do not use images to display text. * Include an 'alt' attribute in images. ## Naming of files, tags, keywords, and navigation titles * **Markdown file names:** * Do not contain spaces or special characters like &, %, #, (, ), è, é, ê, ë, ... * Are unique among the website. * Are lowercase, except for acronyms. * **Tags for tools, resources and pages:** * Do not contain special characters like &, %, #, (, ), è, é, ê, ë, ... * Are lowercase, except for acronyms. * Are short if possible, but distinguishable from existing tags. * Can contain spaces. * **Keywords:** * Are lowercase. * Can contain spaces. * **Titles in the navigation side panel:** * First word and acronyms capitalised. * Should contain the same words as the filename where this title points to. ## How to suggest amendments or additions to this style guide [Open a new issue](https://github.com/elixir-europe/rdmkit/issues) on GitHub or email [rdm-toolkit@elixir-europe.org](mailto:rdm-toolkit@elixir-europe.org). See also the [contribute page](how_to_contribute). ",
      "url": "/pages/bedroesb/rdmkit/style_guide.html",
      "relUrl": "/style_guide.html"
    },"247": {
      "doc": "Support",
      "title": "Support",
      "content": "## Institutions We thank these institutions for their contribution. {% include logo-page.html type=\"institution\"%} ## Projects We thank these projects for their efforts: {% include logo-page.html type=\"project\"%} ## Funders RDMkit is developed in ELIXIR-CONVERGE that received funding from the European Union's Horizon 2020 Research and Innovation programme under grant agreement No 871075. Additionally we thank the funders that supported some of our contributors. {% include logo-page.html type=\"funder\"%} ## Infrastructures We thank these infrastructures for their efforts: {% include logo-page.html type=\"infrastructure\"%} ",
      "url": "/pages/bedroesb/rdmkit/support.html",
      "relUrl": "/support.html"
    },"248": {
      "doc": "Updating the tool and resource list",
      "title": "Updating the tool and resource list",
      "content": "## Way of working The tools or resources you will find on pages are a filtered set from a [bigger list](all_tools_and_resources). This filtering is done using page_id. If a tool or resource is tagged with for example the page_id `researcher`, it will be automatically listed on the corresponding page. Since the `Data life cycle` pages are not listing tools, we do not allow page_id from this section in the tool table. page_id allowed in the tool table are page_id from the following sections: `Your domain`, `Your role`, `Your tasks` and `Tool assembly`. The page_id can be found in the [Website overview page](website_overview). The [all_tools_and_resources](all_tools_and_resources) list is based on the [csv file](https://github.com/elixir-europe/rdmkit/blob/master/_data/main_tool_and_resource_list.csv) in the `_data` directory of the RDMkit repository. Tools and resources can be manually linked to [FAIRsharing.org](https://fairsharing.org/), [Bio.tools](https://bio.tools) and [TeSS](https://tess.elixir-europe.org/), but every week we also run a fully automatic check that links tools and resources with the corresponding registries. A GitHub Bot will generate a Pull Request (PR) with the new links added to the main data file of the website (a yaml file). {% include important.html content=\"The link with FAIRsharing,TeSS and Bio.tools is automatically done using GitHub actions and is weekly updated. These automatic links are not seen in the table. The search query to one of these registries for a tool or resource can be overwritten in the registry column of the main csv tool table. If no FAIRsharing ID, Bio.tools ID or TeSS Query is available for a source, but there is yet one automatically given (faulty), you can overwrite the automatic linking by adding 'NA' as registry.\" %} ## The main table The main table is based on [this google spreadsheet](https://docs.google.com/spreadsheets/d/16RESor_qQ_ygI0lQYHR23kbZJUobOWZUbOwhJbLptDE/edit#gid=268211668). The table consists of 5 columns: - **name**: the name of the tool or resource - **link**: URL to the main page of the tool or resource, make sure to let the URL start with `https://` - **description**: A short description of the tool or resource. Try to not use the characters `\"` or `'` - **registry**: 3 registries are supported: [Bio.tools](https://bio.tools), [FAIRsharing.org](https://fairsharing.org/) and [TeSS](https://tess.elixir-europe.org/). The keywords you can use respectively are: `biotools`, `fairsharing`, `fairsharing-coll` and `tess`, specifying the id or query with a colon). FAIRsharing collections have an ID that follows the pattern `bsg-s000XXX`. List multiple registries using a comma `, ` between the keywords to separate the key:value pairs. The values that are given in the table will always overrule the automatic links. If no FAIRsharing ID, Bio.tools ID or TeSS Query is available for a source, you can overwrite the automatic linking by adding 'NA' as registry. - **related_pages**: This is used to tag the tools so it is listed on the correct page. We only allow page_id that are linked to a page. To find out what the page_id of a page is, please check its metadata attribute `page_id` at the top of the markdown file or the [Website overview](website_overview) page. Since the Data life cycle pages are not listing tools, we do not allow these page_id in the tool table. page_id allowed in the tool table are page_id from the following sections: `Your domain`, `Your role`, `Your tasks` and `Tool assembly`. List multiple page_id by using a comma `, ` between them. - **country** : If a tool can only be used in a specific country, list those countries with there Alpha-2 code. For a complete list of all country code please visit the [IBAN](https://www.iban.com/country-codes) website. A flag will appear in the `related pages` column. | name | link | description | registry | related_pages |----------|----------------------------------|-------------------------------------------------------------------------------------------|---------------------------------------------|--------------------------------------------------| Beacon | https://beacon-project.io/ | The Beacon protocol defines an open standard for genomics data discovery. | researcher, data manager, IT support, human data | Bioconda | https://bioconda.github.io/ | Bioconda is a bioinformatics channel for the Conda package manager | biotools:bioconda | IT support, data analysis | BrAPI | https://www.brapi.org | Specification for a standard API for plant data: plant material, plant phenotyping data | IT support, plants | Conda | https://docs.conda.io/en/latest/ | Open source package management system | IT support, data analysis | COPO | https://copo-project.org/ | Portal for scientists to broker more easily rich metadata alongside data to public repos. | biotools:copo, fairsharing-coll:bsg-d001247 | metadata, researcher, plants | ## What tool or resource can be added to the table Tools and resources specifically mentioned in the text of the pages should be present in the main table. If necessary, tools and resources equivalent to the one mentioned in the text could also be added to the table. ## Making changes Since the csv file is not user-friendly and prone to mistakes because empty fields and commas, we do not recommend making changes using the GitHub website itself, instead we point people to the [Google spreadsheet](https://docs.google.com/spreadsheets/d/16RESor_qQ_ygI0lQYHR23kbZJUobOWZUbOwhJbLptDE/edit?usp=sharing). The editors will do the work on Git for you. All you need to do is: - Check if a tool or resource is already listed. - Add or edit tools and resources as described above. - Done! The editors will update the \"tool and resource list\" in GitHub regularly. In case your change is urgent, ping an editor in an issue or pull request. ## Let the editor and GitHub bot do the rest If the PR of the editor containing the changes to the .csv table is merged, a PR will be opened by github-actions. Please check that the changes this PR proposes to the yaml file are in line with what you want to have changed. ",
      "url": "/pages/bedroesb/rdmkit/tool_resource_update.html",
      "relUrl": "/tool_resource_update.html"
    },"249": {
      "doc": "Toxicology data",
      "title": "Toxicology data",
      "content": "## Introduction Toxicology is focused on the study of the adverse effects that occur in living organisms due to their interaction with chemicals. These chemicals range from substances found in nature to those made in the laboratory for many purposes (drugs, agrochemicals, pesticides, dyes, food additives, cosmetics, household products, etc.). A part of the toxicological research is devoted to the study of the adverse effects generated by chemicals in humans, while another part is devoted to the study of the noxious effects of the chemicals in the environment. The adversity is observed for a compound at a certain concentration. Consequently, hazard characterization should always consider exposure data. Toxicology was traditionally an observational science that obtained an important part of its data by means of experiments carried out on animals. However, the limitations of animal models to produce human-relevant data, as well as the implementation of the 3R policies (reduction, replacement, and refinement of the animal experimentation) have motivated a change of paradigm towards a more mechanistic view. Many international initiatives are promoting this change. In this page, the relevant toxicological data management issues from in vitro, animal and human assays and ecotoxicology studies are explained, as well as the appropriate solutions for them. It has to be pointed out that most of the toxicology data is generated in a regulatory context, following guidelines for obtaining marketing approval and it constitutes an extremely valuable resource that should be made available to the scientific community. For that reason, efforts are being made for the systematic collection and storage of this data, as well as its standardization, which enables its integration and joint analysis. ## Data from in vitro assays - Data analysis and modelling ### Description In vitro cell culture technologies are commonly used in toxicology. They provide an alternative to animal testing and allow to assess the response of the cells to toxicant exposure. They also provide unique access to biochemical, and morphological changes that can not be observed *in vivo*. The most commonly used systems are immortalized cell lines and primary cell cultures. Although two-dimensional cell cultures are very popular, it has been shown that they do not represent the *in vivo* situation, as they are still far from the tissue organization and the cellular connections seen in an organism. Recent advances in three-dimensional cell culture technologies have allowed the widespread use of organoids. Organoids have been used for in vitro modelling of drug adverse effects, specifically in organs commonly susceptible to drug-induced toxicities (i.e. gastrointestinal tract, liver, kidney). In vitro tests in toxicology typically consist of exposing in vitro cell cultures to growing concentrations of the substance under study and recording changes using a wide variety of techniques, from high-content imaging to cell death. Among the diverse sources of toxicological in vitro data it is worth mentioning the results of the Toxicology in the 21st Century program, or [Tox21](https://ntp.niehs.nih.gov/whatwestudy/tox21/index.html). The results of this project (data and tools) are publicly available. Gene expression changes that occur in biological systems in response to exposure to xenobiotics may represent mechanistically relevant cellular events contributing to the onset and progression of xenobiotic-induced adverse health outcomes. Transcriptomics data can be used to identify changes to gene expression profiles that occur in response to drug treatment which might provide predictive and mechanistic insight into the mode of action of a drug, as well as the molecular clues linked to possible toxicity. ### Considerations Results of in vitro assays are typically collected as dose-response curves. These results should be processed to obtain indexes indicating the concentration at which relevant effects are observed like LC50, IC50, benchmark concentration (BMC). This procedure can involve non-linear curve fitting and outlier removal. It is advisable to report the details of the data processing in order to obtain reproducible results and standarization. ### Solutions - [ToxCast](https://cran.r-project.org/web/packages/tcpl/index.html) has published an R-package with the tools used to process the high throughput chemical screening data. - Benchmark concentrations (and doses) can be computed with free software as [PROAST](https://www.rivm.nl/en/proast) and [BMDS](https://www.epa.gov/bmds). - For experiments where gene expression has been measured in response to a toxicant, R packages such as [DESEq2](https://bioconductor.org/packages/release/bioc/html/DESeq2.html) for RNA-Seq data, and [limma](https://bioconductor.org/packages/release/bioc/html/limma.html) for microarray data are used to find genes that are differentially expressed. - In silico prediction models can be developed starting from a series of compounds annotated with the results on in vitro methods. The quality of the predictions provided by these methods are often comparable with those obtained by experimental methods, particularly when the models are used within their applicability domain. [Flame](https://github.com/phi-grib/flame) is an open-source modelling framework developed specifically for this purpose. ## Data from animal assays - Existing data and vocabularies ### Description Assays are expensive. Most animal data come from compiling normative studies which are compulsory for obtaining the approval of diverse regulatory agencies prior to commercialization. The choice of the species and strains was determined by their representability for the studied endpoints, and often defined in this legislation and comprises from invertebrate (e.g., daphnia is commonly used to study aquatic toxicity) and fish (e.g., zebrafish), to rodents (mice, rats, rabbits, guinea pigs) and mammals (dogs, primates). The representability of animal data to predict human toxicity is questionable and a precautionary approach or the use of extrapolation factors is recommended. In spite of their inconveniences (high costs, time consumption, requirements of significant amounts of the substance being tested, limited translatability of the observed results), in many cases, there is no suitable replacement for *in vivo* tests. The replacement of *in vivo* data for alternative approaches (often called NAM, New Approach methodologies) is an active research field. Two important toxicogenomics resources containing animal data are [TG-GATEs](https://pubmed.ncbi.nlm.nih.gov/25313160/), and [DrugMatrix](https://pubmed.ncbi.nlm.nih.gov/25058030/). These resources contain gene expression data in several rat tissues for a large number of compounds, in several doses and exposure times. They also include histopathology annotations and chemistry measurements. ### Considerations Data generated in normative studies were obtained under Good Laboratory Practices (GLP) conditions, and therefore the quality of the data is high. However, these studies were oriented to characterize a single compound, and not to carry out comparative analyses. Also, the doses used in the studies were designed to detect adversity and could be not representative of the exposure reached by consumers or patients of the marketed substances. Most of the time, data is not modelled using standards, for example, drugs are annotated using common names, and histopathology annotations are not coded in a controlled vocabulary. ### Solutions - Use information about genes, and variants associated with human adverse effects, from platforms such as [DisGeNET](https://www.disgenet.org/), [CTD](http://ctdbase.org/), and [PharmGKB](https://www.pharmgkb.org/). - Histopathology data requires the use of a controlled vocabulary like [CDISC/SEND](https://evs.nci.nih.gov/ftp1/CDISC/SEND/SEND%20Terminology.html). - The extension and curation of ontologies like CDISC/SEND to specific domains is facilitated by tools like [ONTOBROWSER](https://opensource.nibr.com/projects/ontobrowser/). - In order to reduce the number of animals used in toxicological studies, it has been suggested to replace control groups with historically collected data from studies carried out in comparable conditions (so-called Virtual Control Groups). VCGs are being developed by [eTRANSAFE project](https://etransafe.eu/virtual-control-groups-one-step-forward-into-the-future-of-animal-testing-in-toxicology). ## Data from human assays - Existing data and vocabularies ### Description Human response to toxic agents is generally excluded from toxicity assays as it entails major ethical issues. Although relevant information on potential adverse effects is available from animal and in vitro assays, human data is crucial for accurate calibration of toxicity models based on these studies. Traditionally, it was frequent that exposure to an unknown or unexpected toxic agent was eventually identified as the trigger factor of a health problem for which an evident reason did not apparently exist. Thereby, unintentional human exposure to toxic agents yielded toxicological data. Two main types of sources exist in this regard: - Individual or group case reports are a fundamental source of information when no previously reported human toxicity information exists. They include exhaustive medical information on a single patient or a set of patients with similar symptomatology which is gathered from health care facilities to identify etiology. - Epidemiologic studies (ESs) are focused on the possible association between the exposure to a substance and the potential adverse reactions observed in a given human population. ESs are classified as occupational (individuals are exposed in the workplace), or environmental (individuals are exposed through daily living). In the pharmaceutical context though, intentional human exposure to drug candidates is a necessary step in the development of medications. During the clinical trial stage, human exposure to substances is required to characterize efficacy and safety. This process consists of several phases which are exhaustively controlled and subjected to strict regulations and ethical review. Adverse-event monitoring and reporting is a key issue in the assessment of the risk-benefit balance associated with the medication which is established from the clinical trials data. After the medication is released to the market it is subjected to an exhaustive pharmacovigilance process focused on the identification of safety concerns. Serious and non-serious adverse effects reporting from several sources are collected during a period and medication risk-benefit balance is re-evaluated. ### Considerations Data from human assays are highly heterogeneous and integration with in vitro and animal data is a challenging task. There is a broad range of resources containing human data publicly available, but sometimes data access is limited. The nature of toxicological data has evolved in recent times and available resources and repositories comprise a variety of different types of data. On one hand, many data sources are nicely structured but, on the other hand, some others provide detailed information in an unstructured format. Data should be harmonized before integration. Disparate data sources are organized differently and also use different terminologies: - Resources providing access to occupational epidemiologic studies report health risks by using condition-centred vocabularies like (ICD9-CM and ICD10-CM) or just uncoded terms whereas databases reporting possible links between observed adverse reactions and medications are usually expressed according to the MedDRA ontology. - Different chemical identifiers are used depending on the toxic agent. - Similarly, medication identifiers are not always consistent among different sources. This is a challenging issue as many medicinal products have different denominations and available commercial presentations depending on the country/region where the product is commercialized. - Usually, structured resources present metadata explaining how the data is organized, thus enabling an easy data transformation process. Conversely, non-structured resources are not easy to harmonize as data organization is not consistent among the available documents. Databases containing clinical toxicological data of drugs can contain the results of clinical studies ([clinicaltrials.gov](https://clinicaltrials.gov/)), frequent adversities (Medline), or collect pharmacovigilance data ([FAERS](https://www.fda.gov/drugs/surveillance/questions-and-answers-fdas-adverse-event-reporting-system-faers)) depending on the data being incorporated, the interpretation is different. For example, in the case of spontaneous reporting systems, the frequency with which an adverse event is reported should be considered relative to the time the compound has been in the market and the frequency of these adverse events in the population treated. ### Solutions Examples of databases containing drug toxicological data: - [clinicaltrials.gov](https://clinicaltrials.gov/) is a resource depending on the National Library of medicine which makes available private and public-funded clinical trials. - The [FDA Adverse Event Reporting System (FAERS)](https://www.fda.gov/drugs/surveillance/questions-and-answers-fdas-adverse-event-reporting-system-faers) contains adverse event reports, medication error reports and product quality complaints submitted by healthcare professionals, consumers, and manufacturers. Harmonization of terminologies can be achieved by using different resources: - The [Unified Medical Language System (UMLS)](https://www.nlm.nih.gov/research/umls/index.html) provides mappings between different medical vocabularies. It includes common ontologies within the condition/diagnosis domain like SNOMED, ICD9CM, ICD10CM, and also the MedDRA ontology. - The [OHDSI](https://www.ohdsi.org/analytic-tools/athena-standardized-vocabularies/) initiative for health data harmonization is an alternative solution for the mapping of vocabularies needed for the harmonization of different resources. This initiative maintains the ATHENA set of vocabularies which is in constant evolution and covers relevant domains in the realm of health care. The OHDSI community is paying special attention to the mappings between medication identifiers coming from national regulatory agencies of the countries of provenance of the institutions involved in the initiative, and the RxNorm identifier which is the standard vocabulary used by OHDSI. - Resources in the context of environmental ([ITER](https://www.tera.org/iter/), [IRIS](https://www.epa.gov/iris)) or occupational ([Haz-Map](https://haz-map.com/)) toxicity using CAS Registry Number identifiers can be connected with those in the pharmaceutical field prone to use [ChEMBL](https://www.ebi.ac.uk/chembl/) identifiers via molecular identifiers available in both resources like the standard InChI or standard InChI Key representations. Services like EBI’s [UniChem](https://www.ebi.ac.uk/unichem/) can help to translate between different chemical identifiers. To import unstructured data sources into structured schemas is a really challenging task as it involves the application of natural language processing technologies. The development of these tools in the field of toxicology is still at the embrionary stage but several initiatives exist: - The [LimTox](http://limtox.bioinfo.cnio.es/) system is a text mining approach devoted to the extraction of associations between chemical agents and hepatotoxicity. - The [AOP4EUpest](http://www.biomedicale.parisdescartes.fr/aop4EUpest/home.php) webserver is a resource for the identification of annotated pesticides-biological events involved in Adverse Outcome Pathways (AOPs) via text mining approaches. ## Ecotoxicology data - Existing data ### Description Substances can also be characterized according to their potential to affect the environment. This data is collected by national and international regulatory agencies (e.g., ECHA in EU and, EPA in the USA) aiming to control the production, distribution, and use of potentially hazardous substances. Data collection is largely guided by legislation, which defines the test that should be carried out and the data that should be collected. ### Considerations When considering the effect of a substance on the environment, in addition to its hazard characterization, it is important to consider its environmental fate in terrestrial and aqueous environments, and its properties with respect to degradation by diverse routes (chemical, biodegradation, photodegradation). ### Solutions - The [ECOTOXicology Knowledgebase (ECOTOX)](https://cfpub.epa.gov/ecotox/) is a comprehensive, publicly available Knowledgebase providing single chemical environmental toxicity data on aquatic life, terrestrial plants, and wildlife. - The [CompTox Chemicals Dashboard](https://comptox.epa.gov/dashboard) provides toxicological information for over 800.000 chemical compounds, including experimental and predicted fate information. ",
      "url": "/pages/bedroesb/rdmkit/toxicology_data.html",
      "relUrl": "/toxicology_data.html"
    },"250": {
      "doc": "TransMed",
      "title": "TransMed",
      "content": "## What is the TransMed data and computing tool assembly? The TransMed data and computing tool assembly is an infrastructure provided by [ELIXIR Luxembourg](https://elixir-luxembourg.org) for clinical and translational projects. TransMed assembly provides the tools for managing ongoing projects that often require the management of cohort recruitment, and processing of samples, data and metadata. This entails GDPR-compliant and secure data collection, storage, curation, standardisation integration and analysis of clinical data and associated molecular, imaging and sensor/mobile data and metadata. TransMed tool assembly is also a blueprint showing how a collection of tools can be combined to support data lifecycle management in clinical and translational projects. ## Who can use the TransMed data and computing tool assembly? All researchers can use tools in the TransMed assembly individually or in combination depending on their project needs. Most of the tools in the TransMed assembly are open-source and can be re-used. ELIXIR Luxembourg provides know-how transfer and training on the tool assembly upon request from researchers and data steward organisations. To make a request please contact [info@elixir-luxembourg.org](mailto:info@elixir-luxembourg.org). Additionally, ELIXIR Luxembourg provides hosting of the TransMed assembly. Hosting of tools and data is free of charge for national users. For international users hosting of data (up to 10TB) is free on the basis that the data is shared with the wider research community with an appropriate access model such as controlled access. For international users, charges for the hosting tools and hosting of large datasets are evaluated on a case-by-case, please contact [info@elixir-luxembourg.org](mailto:info@elixir-luxembourg.org) for details. ## For what purpose can the TransMed assembly be used? {% include image.html file=\"TransMed_assembly.svg\" caption=\"Figure 1. TransMed data and computing tool assembly\" alt=\"TransMed tool assembly\" %} ### Data management planning Translational Biomedicine projects often deal with sensitive data from human subjects. Therefore, data management planning of this type of projects needs to take data protection and GDPR compliance into account . Typically a TransMed project involves multiple (clinical) study sites and can contain several cohorts. During the planning phase the dataflow for the project and data/metadata collected prospectively or retrospectively needs to be documented. Projects can use the [Data Information Sheet DISH](http://doi.org/10.5281/zenodo.5127940) to map the project dataflow and collect metadata necessary for GDPR-compliant processing. In addition, a data protection impact assessment needs to be performed taking into account partner roles, responsibilities and the data information collected via the DISH. For this purpose TransMed assembly uses the Data Information System - [DAISY](https://daisy-demo.elixir-luxembourg.org/), which indexes all information collected by DISH and provides a repository to accumulate GDPR-required project documentation such as ethics approvals and consent templates and subject information sheets and ultimately the project data management plan. TransMed assembly includes the risk management tool [MONARC](https://open-source-security-software.net/project/MONARC), which can be used to perform Data Protection Impact Assessments (DPIA). DPIAs are a requirement of the GDPR for projects dealing with sensitive human data. ### Data collection, transfer and storage For projects involving patient recruitment the TransMed assembly provides the Smart Scheduling System, [SMASCH](https://smasch.pages.uni.lu ), tracking availability of resources in clinics and manages patient visits. Pseudonymised clinical data and patient surveys are then collected by the state of the art electronic data capture (EDC) system [REDCap](https://projectredcap.org) through a battery of electronic case report forms (eCRFs). Imaging data from the clinics are deposited into a dedicated imaging platform [XNAT](https://www.xnat.org/). Omics data, both in raw and derived form can be deposited to the data provenance system [iRODS](https://irods.org/). The transfer of data files can be done via various encrypted communication options as outlined in the [Data transfer](data_transfer) section of the RDMkit. The TransMed assembly most typically utilises (S)FTP, Aspera FASP and ownCloud. Data is also encrypted at rest with hard-ware and also with file-level encryption using either open-source utilities such as gpg or commercial options such as Aspera FASP. ### Data curation and harmonisation To facilitate cross-cohort/cross-study interoperability of data, upon collection, the data needs to be curated and harmonised. For this purpose the TransMed assembly uses a variety of open standards and tools. For data quality and cleansing the assembly uses [OpenRefine](https://openrefine.org/), which provides an intuitive interface to generate facets of data that support the research to identify quality issues and outliner. It also enables traceable and yet easy data correction. For data Extraction, Transformation and Loading (ETL) the assembly uses [Talend Open Studio](https://www.talend.com/) (for complex and reusable ETLs) as well as R and Python (for ad-hoc and simple transformation). To evaluate and improve FAIRness of datasets, the assembly follows the recipes in the [FAIR Cookbook](https://fairplus.github.io/the-fair-cookbook/) developed by the FAIRplus consortium. Related to standard data models and ontologies the assembly follows the recommendations in the FAIR Cookbook recipe for selecting terminologies and ontologies. ### Data integration and analysis TransMed projects usually require different data types from different cohorts to be integrated into one data platform for the exploring, sub-setting and integrated analysis for hypothesis generation. The TransMed assembly consists of several such tools: [Ada](https://ada.parkinson.lu/documentation/intro) is a web-based tool to provide a performant and highly configurable system for secured integration, visualization, and collaborative analysis of heterogeneous data sets, primarily targeting clinical and experimental sources. The assembly also includes other tools for specific data types, such as [ATLAS](https://github.com/OHDSI/Atlas/wiki) that integrate features from various [OHDSI](https://ohdsi.org/) applications for Electronic Health Record data in [OMOP](https://ohdsi.github.io/CommonDataModel/) format into a single cohesive experience. Transmart is a tool that provides easy integration between phenotypic/clinical data and molecular data and a “drag-and-drop” fashion data exploration interface. ### Data stewardship To facilitate the findability of data the TransMed assembly provides a [Data/Sample Catalog tool](https://datacatalog.elixir-luxembourg.org/) that supports the indexing search and discovery of studies, data sets and samples accumulated in the context of projects from different sites and cohorts. The catalog implements a controlled-access model by integration with [AAI REMS](https://github.com/CSCfi/rems). Audit trailing of data access is achieved by integration of the [DAISY tool](https://daisy-demo.elixir-luxembourg.org/) in the access process. The catalog tool can be integrated with various identity management systems such as [Keycloak](https://www.keycloak.org/), [ELIXIR-AAI](https://elixir-europe.org/services/compute/aai) or [Free-IPA](https://www.freeipa.org/). ",
      "url": "/pages/bedroesb/rdmkit/transmed_assembly.html",
      "relUrl": "/transmed_assembly.html"
    },"251": {
      "doc": "TSD",
      "title": "TSD",
      "content": "## What is the Norwegian tools assembly for sensitive data - TSD data management tools assembly? The Norwegian ELIXIR tools assembly for sensitive data is centred around [TSD - literally for: services for sensitive data](https://www.uio.no/english/services/it/research/sensitive-data/) is an infrastructure provided by [the University of Oslo (UiO)](https://www.uio.no). Together with the other complementary tools provided by ELIXIR, TSD can be used for the management of [sensitive data](sensitive_data), including handling of [Human data](human_data). This assembly covers [Planning](planning), [Processing](processing), [Analysing](analysing) and [Sharing](sharing) Data Life Cycle stages and offer [Data Storage](storage) capacities and tools for [transfer](data_transfer) of sensitive data, following the requirements of the EU general data protection regulations (GDPR) and its Norwegian implementation. ## Who can use the TSD data management tools assembly? Resources on TSD itself are accessible for international users through [EOSC](https://marketplace.eosc-portal.eu/services/tds) and directly through the [University of Oslo](https://www.uio.no/english/services/it/research/sensitive-data/access/). Researchers in Norway can in addition obtain access through the [national e-infrastructure program Sigma2](https://www.sigma2.no/sensitive-data-services) and additional storage quotas (up to 10TB) for existing TSD projects through the (Norwegian) bioinformatics support desk [contact@bioinfo.no](mailto:contact@bioinfo.no). If you are affiliated to a Norwegian institution which has already stipulated a [data processing and/or service agreement with TSD](https://www.uio.no/tjenester/it/forskning/sensitiv/hjelp/start/kontrakter/index.html), this document can be used or amended to account for your project. Otherwise, you can establish a data agreement for your individual project by contacting [tsd-contact@usit.uio.no](mailto:tsd-contact@usit.uio.no). ## What can you use the TSD data management tools assembly for? {% include image.html file=\"TSD_tool_assembly.svg\" caption=\"Figure 1. Norwegian ELIXIR tools assembly for sensitive data - TSD\" alt=\"TSD tool assembly\" %} The Norwegian tools assembly for sensitive data offers support with [Data Management Planning](planning) through an [instance of the Data Stewardship Wizard](https://elixir-no.ds-wizard.org) following the guidelines of the major national and European funding bodys. Dedicated references guide you through national infrastructure, resources, laws and regulations and also include the [Tryggve ELSI Checklist](https://neic.no/tryggve/links/) for Ethical, Legal and Social Implications. Soon you will be able to submit storage request forms for [Data Storage](storage) in TSD with defined access permissions through the Data Stewardship Wizard. TSD offers [Data Storage](storage) services. Moreover, [Processing](processing) and [Analysing](analysing) of data is performed in a safe environment within TSD. As a national user, you can access TSD by identifying yourself using the Norwegian [ID-porten](https://eid.difi.no/en/id-porten) system. International users can get access by contacting [tsd-contact@usit.uio.no}(mailto:tsd-contact@usit.uio.no). Within TSD, you can access a Windows or Linux virtual machine (VM) and, upon request, [high performance computing (HPC) resources](https://www.uio.no/english/services/it/research/sensitive-data/use-tsd/hpc/resources.html) and [backup storage](https://www.uio.no/english/services/it/research/sensitive-data/use-tsd/directories-files/backup/index.html). You login using two factor authentication with [Google Authenticator](https://support.google.com/accounts/answer/1066447?co=GENIE.Platform%3DAndroid&hl=en) and a dedicated username and password. The [login in procedure](https://www.uio.no/english/services/it/research/sensitive-data/use-tsd/login/index.html) is different for Windows and Linux VMs on TSD. As the primary design goal of TSD is security, [transfer of data](data_transfer) by other means to and from [TSD is restricted and logged](https://www.uio.no/english/services/it/research/sensitive-data/use-tsd/import-export/index.html). ### Data management planning You can access the [ELIXIR-NO instance of the Data Stewardship Wizard](https://elixir-no.ds-wizard.org) using [ELIXIR AAI](https://elixir-europe.org/services/compute/aai), which can be coupled with the national solution for secure login and data sharing in the educational and research sector [FEIDE](https://www.feide.no/). ### Data Collection If you use one of the National Norwegian research infrastructures, such as the Norwegian sequencing infrastructure [NorSeq](https://www.norseq.org/) they can directly upload data to your TSD project for you, as described in this [page](https://elixir.no/Services-bak/data_produced_NorSeq) The sensitive data tools assembly provides [Nettskjema](https://nettskjema.no) as a solution for designing and managing data collections using online forms and surveys. This is a secure and GDPR-compliant service. It can be accessed through the UiO's web pages and it is used through a web browser. Submissions from a Nettskjema questionnaire can be delivered securely (fully encrypted) to your project area within TSD. TSD-users are granted access to Nettskjema through [IDporten or FEIDE](https://www.uio.no/tjenester/it/adm-app/nettskjema/mer-om/eksterne-brukere). When the Nettskjema form is complete, you can upload it on TSD following [these instructions](https://www.uio.no/tjenester/it/adm-app/nettskjema/hjelp/koble-skjema-til-tsd.html). After verification, the form can be used for collecting sensitive data. Note that further processing and analysis of the results should be conducted within TSD. If exporting data is necessary, the files should be properly [de-identified or anonymised](sensitive_data.html#how-can-you-de-identify-your-data). ### Data Processing and Analysis For [Processing](processing) and [Analysing](analysing) your data, you can use singularity containers and [individual tools on the HPC cluster](https://www.uio.no/english/services/it/research/sensitive-data/use-tsd/hpc/software/). The computing services provided through TSD include an Illumina DRAGEN (Dynamic Read Analysis for GENomics) node, which can be used to speed up genomic analysis of sequencing data. Dragen is a dedicated resource, if you want to run jobs on DRAGEN please send an email to [tsd-drift@usit.uio.no](mailto:tsd-drift@usit.uio.no). ### Data Sharing and Preservation One solution for permanent archiving and sharing of personally identifiable genetic and phenotypic datasets resulting from biomedical research data is to deposit them to the [European Genome-phenome Archive (EGA)](https://ega-archive.org/). The EGA applies a controlled access model. There can be limitations, e.g. given consents, for your datasets which prevents them from leaving your jurisdiction or being archived in general. This will be partly addressed in the future by federated EGA services with nodes operating from one country or institution under one specific jurisdiction. This model will enable discovery of publicly shareable metadata about studies/datasets archived at the federated EGA nodes through the Central EGA, while the remaining data is stored in a local solution. These nodes will offer the same APIs and interfaces as the Central EGA and provide independent data distribution to users. The Norwegian federated EGA (NFEGA) will be accessible through [ELIXIR AAI](https://elixir-europe.org/services/compute/aai), compatible with [FEIDE](https://www.feide.no/). ",
      "url": "/pages/bedroesb/rdmkit/tsd_assembly.html",
      "relUrl": "/tsd_assembly.html"
    },"252": {
      "doc": "United Kingdom",
      "title": "United Kingdom",
      "content": "## Introduction ## Funders * Biotechnology and Biological Sciences Research Council (BBSRC) [DM guidelines](https://www.ukri.org/councils/bbsrc/guidance-for-applicants/what-to-include-in-your-application/data-management-plan/) * Wellcome Trust [DM guidelines](https://wellcome.org/grant-funding/guidance/data-software-materials-management-and-sharing-policy) * Medical Research Council (MRC) [DM guidelines](https://www.open.ac.uk/library-research-support/research-data-management/mrc-data-management-plans) ## Regulations ## Domain-specific infrastructures/resources ",
      "url": "/pages/bedroesb/rdmkit/uk_resources.html",
      "relUrl": "/uk_resources.html"
    },"253": {
      "doc": "Website overview",
      "title": "Website overview",
      "content": "{% include site-overview.html %} ",
      "url": "/pages/bedroesb/rdmkit/website_overview.html",
      "relUrl": "/website_overview.html"
    },"254": {
      "doc": "Working with git",
      "title": "Working with git",
      "content": "## Forking - branching - changing - pushing - PR This is a general workflow in how to work on your own fork (copy) of the rdmkit repo and request changes through a pull request: NOTE: if you already did these steps in the past, start from the `git fetch upstream` command. - Make a fork of this repository, using the fork button. - Open a terminal and clone your fork using: ``` git clone git@github.com:USERNAME/rdmkit.git cd rdmkit ``` NOTE: Make sure you clone the fork and not the original elixir-europe/rdmkit one. - Keep your fork up to date (IMPORTANT!). ``` git remote add upstream https://github.com/elixir-europe/rdmkit.git git fetch upstream git checkout master (if you are not already on the master branch, check with `git branch`) git pull upstream master ``` - Create a new branch named after your feature/edit. ``` git checkout -b 'FEATURE_NAME' ``` - Make the changes you want to make using an editor of choice - Save. - Open terminal and stage your changes: ``` git add . ``` - Committing changes ``` git commit -m \"Changing the tool-resource file\" ``` - Pushing you changes to your fork ``` git push origin 'FEATURE_NAME' ``` - Go to [https://github.com/elixir-europe/rdmkit](https://github.com/elixir-europe/rdmkit) and click on *Compare & pull request* - Open the pull request an describe your changes. - Wait for review by other editors. Editors that are responsible for the sections you make changes to will be assigned as reviewer automatically. ## The advantage of working locally: previewing your changes through your web browser The website is build on GitHub using Jekyll, a simple, static site generator based on ruby. When you have a local copy cloned onto your computer, it is possible to generate the website based on this repo. This makes it possible to preview changes live, every time you save a file from within the GitHub rdmkit repo. Follow these steps to deploy the website based on your local clone (copy) of the rdmkit repo: Make sure you have cloned the rdmkit repo: git clone git@github.com:USERNAME/rdmkit.git cd rdmkit To run the website locally, you can either use [Docker](https://www.docker.com/) or use Jekyll directly after installing various dependencies. ### Run using Docker 1. If not already installed on your machine, install Docker. From the root of the ``rdmkit`` directory, run: ``` docker run -it --rm -p 4000:4000 -v $PWD:/srv/jekyll jekyll/jekyll:latest /bin/bash -c \"chmod a+w /srv/jekyll/Gemfile.lock && chmod 777 /srv/jekyll && jekyll serve -w\" ``` This will start the docker container and serve the website locally. ### Run using Jekyll directly 1. If not already present on your machine, install ruby. Note that incompatibility issues may arise with ruby 3.0.0 (released 25.12.20) or newer versions. 1. Install Jekyll If you have never installed or run a Jekyll site locally on your computer, follow these instructions to install Jekyll: * Install Jekyll on MacOS/Ubuntu/Other_Linux/Windows: [https://jekyllrb.com/docs/installation/](https://jekyllrb.com/docs/installation/) 1. Install Bundler and Jekyll ``` gem install jekyll bundler ``` 1. Install dependencies ``` bundle install ``` 1. deploy website ``` bundle exec jekyll serve ``` Additional information can be found at the following link: [https://docs.github.com/en/free-pro-team@latest/github/working-with-github-pages/testing-your-github-pages-site-locally-with-jekyll](https://docs.github.com/en/free-pro-team@latest/github/working-with-github-pages/testing-your-github-pages-site-locally-with-jekyll) ",
      "url": "/pages/bedroesb/rdmkit/working_with_git.html",
      "relUrl": "/working_with_git.html"
    },"255": {
      "doc": "XNAT-PIC",
      "title": "XNAT-PIC",
      "content": "## Summary Preclinical imaging centers deal with many challenges mostly related to the variety of imaging instrumentation yielding huge volumes of raw data. The current procedures to collect, share and reuse preclinical image data are insufficient, thus revealing an urgent need of standardization in terms of data storage and image processing. **XNAT for Preclinical Imaging Centers (XNAT-PIC)** has been developed to overcome this limitation by extending XNAT’s basic functionalities to meet the needs of preclinical imaging facilities. ## What is XNAT-PIC? **XNAT for Preclinical Imaging Centers (XNAT-PIC)** consists of a set of tools built in Python and MATLAB to [store](storage), [process](processing) and [share](sharing) preclinical imaging studies built on top of the [XNAT](https://www.xnat.org/) imaging informatics platform. ## Who is XNAT-PIC intended for? XNAT-PIC is inteded for scientists, researchers and data stewards working in the preclinical and biomedical imaging field to support image data management and processing. ## Which task can be solved with XNAT-PIC? XNAT-PIC is a set of tools to support preclinical imaging scientists in their data management and processing needs. The Extensible Neuroimaging Archive Toolkit [XNAT](https://www.xnat.org/) is an imaging informatics platform developed by the Neuroinformatics Research Group at the Washington University for the management, storage and analysis of biomedical image data. XNAT is an open-source project that can support a wide range of imaging modalities thanks to its extensibility. XNAT-PIC consists of: {% include image.html file=\"xnat-pic.png\" caption=\"Schematic overview of the XNAT-PIC tool assembly.\" alt=\"Schematic overview of the XNAT-PIC tool assembly.\" %} 1. **MRI2DICOM** [processes](processing) Magnetic Resonance (MR) images and convert them from ParaVision® (Bruker, Inc. Billerica, MA) file format to DICOM standard 2. **XNAT-PIC Uploader** to import and [store](storage) multimodal DICOM image datasets to XNAT 3. **XNAT-PIC Pipelines** for [analysing](analysing) single or multiple subjects within the same project in XNAT. ## Citation If you use **XNAT-PIC** please cite: * S. Zullino, A. Paglialonga, W. Dastrù, D. L. Longo, S. Aime. XNAT-PIC: Extending XNAT to Preclinical Imaging Centers, 2021. Pre-print: https://arxiv.org/abs/2103.02044 ",
      "url": "/pages/bedroesb/rdmkit/xnat_pic_assembly.html",
      "relUrl": "/xnat_pic_assembly.html"
    }
  }
  